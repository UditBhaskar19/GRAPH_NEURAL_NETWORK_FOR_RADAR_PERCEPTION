{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "module_rootdir = '.'\n",
    "dataset_rootdir = '.'\n",
    "label_rootdir = module_rootdir\n",
    "sys.path.append(module_rootdir)\n",
    "\n",
    "trained_proposal_weights_path = './model_weights/gnn/1715232829109/graph_based_detector.pt'\n",
    "proposal_config_file_path = './configuration_radarscenes_gnn.yml'\n",
    "classifier_config_file_path = './configuration_radarscenes_classifier.yml'\n",
    "\n",
    "from modules.set_configurations.common import read_yaml\n",
    "from modules.set_configurations.set_config_gnn import config as config_gnn\n",
    "from modules.set_configurations.set_config_classifier import config as config_classifier\n",
    "from modules.set_configurations.set_param_for_inference_gnn import set_parameters_for_inference\n",
    "from modules.neural_net.classifier.classifier import Model_Training\n",
    "from modules.inference.clustering import Simple_DBSCAN\n",
    "from modules.data_utils.read_data import get_sequence_data\n",
    "from modules.data_utils.labels import compute_new_labels_to_id_dict\n",
    "from modules.data_utils.labels import compute_old_to_new_label_id_map\n",
    "\n",
    "from modules.data_generator.datagen_gnn import compute_node_idx_for_each_cluster\n",
    "from modules.data_utils.read_data import extract_frame\n",
    "from modules.compute_groundtruth.compute_node_labels import compute_ground_truth as compute_ground_truth_node\n",
    "from modules.compute_features.graph_features import select_moving_data\n",
    "from modules.compute_features.graph_features import compute_adjacency_information\n",
    "from modules.compute_features.graph_features import compute_edge_features\n",
    "from modules.compute_features.graph_features import compute_node_features\n",
    "from modules.compute_groundtruth.compute_offsets import unnormalize_gt_offsets\n",
    "from modules.data_generator.datagen_classifier import extract_proposals\n",
    "from modules.data_generator.datagen_classifier import extract_and_compute_features_and_labels\n",
    "from modules.data_generator.datagen_classifier import remove_low_quality_proposals\n",
    "from modules.data_generator.datagen_classifier import compute_graph\n",
    "\n",
    "from modules.compute_groundtruth.compute_node_labels import compute_gt_clusters\n",
    "from modules.inference.ellipse import compute_cov_ellipse\n",
    "from modules.plot_utils.plot_func import compare_pred_gt_clusters\n",
    "\n",
    "sequence_name = 'sequence_148' # 'sequence_108'  # 'sequence_148'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Good to go!\n"
     ]
    }
   ],
   "source": [
    "config_gnn_obj = config_gnn(proposal_config_file_path)\n",
    "config_classifier_obj = config_classifier(proposal_config_file_path, classifier_config_file_path)\n",
    "\n",
    "param_obj = set_parameters_for_inference(module_rootdir, config_gnn_obj, trained_proposal_weights_path)\n",
    "device = param_obj['device']\n",
    "grid = param_obj['grid']\n",
    "prop_extractor = param_obj['detector']\n",
    "\n",
    "detector_train = Model_Training(config_classifier_obj)\n",
    "detector_train = detector_train.to(device)\n",
    "\n",
    "params = [p for p in detector_train.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, momentum=0.9, lr=config_classifier_obj.learning_rate, weight_decay=config_classifier_obj.weight_decay)\n",
    "\n",
    "clustering_eps = config_classifier_obj.clustering_eps # 1.4\n",
    "valid_cluster_num_meas_thr = config_classifier_obj.valid_cluster_num_meas_thr  # 2\n",
    "meas_noise_cov = config_classifier_obj.meas_noise_cov\n",
    "clustering_obj = Simple_DBSCAN(clustering_eps)\n",
    "\n",
    "scene_metadata, radar_mount_data, radar_data_all_scenes, odometry_data_all_scenes \\\n",
    "    = get_sequence_data(dataset_rootdir, config_classifier_obj.dataset_path, sequence_name, config_classifier_obj.window_size)\n",
    "\n",
    "labels_to_id_dict = compute_new_labels_to_id_dict()\n",
    "old_to_new_label_id_map = compute_old_to_new_label_id_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "frame number: 40\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_frame(idx):\n",
    "\n",
    "    print('-' * 100)\n",
    "    print(f'frame number: {idx}')\n",
    "\n",
    "    data_dict = extract_frame(\n",
    "        idx = idx, \n",
    "        windowed_data_list = scene_metadata,\n",
    "        radar_mount_data = radar_mount_data,\n",
    "        radar_data_all_scenes = radar_data_all_scenes,\n",
    "        odometry_data_all_scenes = odometry_data_all_scenes,\n",
    "        reject_outlier = False)\n",
    "    \n",
    "    gt_dict_node = compute_ground_truth_node(data_dict, labels_to_id_dict, old_to_new_label_id_map)\n",
    "    data_dict, gt_dict_node = grid.select_meas_within_the_grid(data_dict, gt_dict_node)\n",
    "    data_dict_dyn, node_labels_dict_dyn = select_moving_data(data_dict, gt_dict_node, labels_to_id_dict)\n",
    "    adj_dict_dyn = compute_adjacency_information(\n",
    "        data_dict_dyn, config_classifier_obj.ball_query_eps_square, config_classifier_obj.k_number_nearest_points)\n",
    "\n",
    "    other_features_dyn = np.stack((\n",
    "            data_dict_dyn['meas_px'], data_dict_dyn['meas_py'], \n",
    "            data_dict_dyn['meas_vx'], data_dict_dyn['meas_vy']), axis=-1)\n",
    "    edge_features_dyn = compute_edge_features(data_dict_dyn, adj_dict_dyn['adj_list'])\n",
    "    node_features_dyn = compute_node_features(\n",
    "            data_dict_dyn, adj_dict_dyn['degree'], \n",
    "            include_region_confidence = config_classifier_obj.include_region_confidence, \n",
    "            min_range = config_classifier_obj.grid_min_r, max_range = config_classifier_obj.grid_max_r, \n",
    "            min_azimuth = config_classifier_obj.grid_min_th, max_azimuth = config_classifier_obj.grid_max_th)\n",
    "    \n",
    "    graph_features = {}\n",
    "    graph_features['other_features_dyn'] = torch.from_numpy(other_features_dyn).to(device).to(torch.float32)\n",
    "    graph_features['edge_features_dyn'] = torch.from_numpy(edge_features_dyn).to(device).to(torch.float32)\n",
    "    graph_features['node_features_dyn'] = torch.from_numpy(node_features_dyn).to(device).to(torch.float32)\n",
    "    graph_features['edge_index_dyn'] = torch.from_numpy(adj_dict_dyn['adj_list'] ).to(device).to(torch.int64)\n",
    "    graph_features['adj_matrix_dyn'] = torch.from_numpy(adj_dict_dyn['adj_matrix'] ).to(device).to(torch.bool)\n",
    "\n",
    "    gt_labels_dyn = node_labels_dict_dyn['class_labels']\n",
    "    cluster_node_idx, cluster_labels = compute_node_idx_for_each_cluster(data_dict_dyn['meas_trackid'], gt_labels_dyn, device)\n",
    "\n",
    "    # cluster offsets predictions\n",
    "    _, node_offsets_predictions, _, _ = prop_extractor(\n",
    "            node_features = graph_features['node_features_dyn'],\n",
    "            edge_features = graph_features['edge_features_dyn'],\n",
    "            edge_index = graph_features['edge_index_dyn'],\n",
    "            adj_matrix = graph_features['adj_matrix_dyn'],\n",
    "            cluster_node_idx = cluster_node_idx )\n",
    "    \n",
    "    reg_deltas = unnormalize_gt_offsets(node_offsets_predictions, config_classifier_obj.reg_mu, config_classifier_obj.reg_sigma)\n",
    "    pred_cluster_centers_xy = graph_features['other_features_dyn'][:, :2] + reg_deltas\n",
    "\n",
    "    # extract proposals\n",
    "    pred_cluster_centers_xy = pred_cluster_centers_xy.detach().cpu().numpy()\n",
    "    proposal_list = extract_proposals(pred_cluster_centers_xy, clustering_obj)\n",
    "\n",
    "    # compute aggregate features and compute labels for each proposal\n",
    "    px = graph_features['other_features_dyn'][:, 0].detach().cpu().numpy()\n",
    "    py = graph_features['other_features_dyn'][:, 1].detach().cpu().numpy()\n",
    "    rcs = graph_features['node_features_dyn'][:, 1].detach().cpu().numpy()\n",
    "\n",
    "    object_features_list, object_class_list, \\\n",
    "    object_mu_list, object_sigma_list, object_num_meas_list \\\n",
    "        = extract_and_compute_features_and_labels(\n",
    "                proposal_list, px, py, rcs, \n",
    "                node_labels_dict_dyn['class_labels'], \n",
    "                meas_noise_cov)\n",
    "            \n",
    "    # remove bad quality proposals\n",
    "    # a proposal is considered bad quality of num_meas of each clustered proposal < self.num_meas_thr\n",
    "    object_features_list, object_class_list, \\\n",
    "    pred_mu_list, pred_sigma_list, object_num_meas_list \\\n",
    "        = remove_low_quality_proposals(\n",
    "            valid_cluster_num_meas_thr, object_features_list, object_class_list, \n",
    "            object_mu_list, object_sigma_list, object_num_meas_list)\n",
    "    \n",
    "    # ======================================================================================\n",
    "    # compare prediction vs ground-truth clusters\n",
    "    gt_mu_list, gt_sigma_list = compute_gt_clusters(px, py, data_dict_dyn['meas_trackid'], meas_noise_cov)\n",
    "    gt_cluster_boundary_points_list = []\n",
    "    for mu, cov in zip(gt_mu_list, gt_sigma_list):\n",
    "        boundary_points, _ = compute_cov_ellipse(mu, cov, chi_sq = 2, n_points=50)\n",
    "        gt_cluster_boundary_points_list.append(boundary_points)\n",
    "\n",
    "    pred_cluster_boundary_points_list = []\n",
    "    for mu, cov in zip(pred_mu_list, pred_sigma_list):\n",
    "        boundary_points, _ = compute_cov_ellipse(mu, cov, chi_sq = 2, n_points=50)\n",
    "        pred_cluster_boundary_points_list.append(boundary_points)\n",
    "    \n",
    "    compare_pred_gt_clusters(\n",
    "        px, py, \n",
    "        pred_mu_list, pred_cluster_boundary_points_list,\n",
    "        gt_mu_list, gt_cluster_boundary_points_list,\n",
    "        boundary_marker_size=2,\n",
    "        figsize=(16,8))\n",
    "    # ======================================================================================\n",
    "    \n",
    "    # create training data: node_features, graph_structure, cluster_sizes\n",
    "    object_features, object_class, object_num_meas, edge_idx = \\\n",
    "        compute_graph(object_features_list, object_class_list, object_num_meas_list, device)\n",
    "    \n",
    "    training_data = {}\n",
    "    training_data['object_features'] = [object_features]\n",
    "    training_data['object_class'] = [object_class]\n",
    "    training_data['object_num_meas'] = [object_num_meas]\n",
    "    training_data['edge_idx'] = [edge_idx]\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "batch_data = process_frame(idx=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0][Class loss: 4.838792324066162]\n",
      "[Iter 1][Class loss: 4.548478603363037]\n",
      "[Iter 2][Class loss: 4.187589645385742]\n",
      "[Iter 3][Class loss: 3.685678958892822]\n",
      "[Iter 4][Class loss: 2.160894155502319]\n",
      "[Iter 5][Class loss: 9.397743225097656]\n",
      "[Iter 6][Class loss: 3.458829879760742]\n",
      "[Iter 7][Class loss: 3.974530220031738]\n",
      "[Iter 8][Class loss: 4.022130489349365]\n",
      "[Iter 9][Class loss: 3.938043117523193]\n",
      "[Iter 10][Class loss: 3.687208414077759]\n",
      "[Iter 11][Class loss: 3.001206159591675]\n",
      "[Iter 12][Class loss: 1.690577983856201]\n",
      "[Iter 13][Class loss: 2.044149160385132]\n",
      "[Iter 14][Class loss: 3.588730335235596]\n",
      "[Iter 15][Class loss: 4.076015472412109]\n",
      "[Iter 16][Class loss: 4.161353111267090]\n",
      "[Iter 17][Class loss: 4.151959419250488]\n",
      "[Iter 18][Class loss: 4.098620414733887]\n",
      "[Iter 19][Class loss: 4.002560615539551]\n",
      "[Iter 20][Class loss: 3.814420223236084]\n",
      "[Iter 21][Class loss: 3.412658929824829]\n",
      "[Iter 22][Class loss: 2.407515525817871]\n",
      "[Iter 23][Class loss: 3.802061557769775]\n",
      "[Iter 24][Class loss: 2.112534999847412]\n",
      "[Iter 25][Class loss: 2.672251701354980]\n",
      "[Iter 26][Class loss: 2.404353857040405]\n",
      "[Iter 27][Class loss: 1.516917347908020]\n",
      "[Iter 28][Class loss: 3.347100257873535]\n",
      "[Iter 29][Class loss: 2.228884696960449]\n",
      "[Iter 30][Class loss: 2.973224163055420]\n",
      "[Iter 31][Class loss: 3.036471366882324]\n",
      "[Iter 32][Class loss: 2.698590278625488]\n",
      "[Iter 33][Class loss: 1.772660374641418]\n",
      "[Iter 34][Class loss: 1.812941789627075]\n",
      "[Iter 35][Class loss: 1.309095978736877]\n",
      "[Iter 36][Class loss: 1.493720054626465]\n",
      "[Iter 37][Class loss: 1.792059898376465]\n",
      "[Iter 38][Class loss: 1.292112588882446]\n",
      "[Iter 39][Class loss: 1.900744557380676]\n",
      "[Iter 40][Class loss: 1.282920718193054]\n",
      "[Iter 41][Class loss: 1.571552157402039]\n",
      "[Iter 42][Class loss: 1.213973045349121]\n",
      "[Iter 43][Class loss: 1.334274768829346]\n",
      "[Iter 44][Class loss: 1.106935501098633]\n",
      "[Iter 45][Class loss: 1.130443215370178]\n",
      "[Iter 46][Class loss: 1.160644054412842]\n",
      "[Iter 47][Class loss: 0.978478074073792]\n",
      "[Iter 48][Class loss: 1.235212326049805]\n",
      "[Iter 49][Class loss: 1.021826744079590]\n",
      "[Iter 50][Class loss: 1.161088228225708]\n",
      "[Iter 51][Class loss: 0.951210856437683]\n",
      "[Iter 52][Class loss: 1.176254510879517]\n",
      "[Iter 53][Class loss: 0.951933383941650]\n",
      "[Iter 54][Class loss: 1.079758882522583]\n",
      "[Iter 55][Class loss: 0.935314416885376]\n",
      "[Iter 56][Class loss: 1.097193360328674]\n",
      "[Iter 57][Class loss: 0.935037314891815]\n",
      "[Iter 58][Class loss: 1.029664397239685]\n",
      "[Iter 59][Class loss: 0.931246280670166]\n",
      "[Iter 60][Class loss: 1.024930834770203]\n",
      "[Iter 61][Class loss: 0.919427275657654]\n",
      "[Iter 62][Class loss: 0.977979302406311]\n",
      "[Iter 63][Class loss: 0.929869532585144]\n",
      "[Iter 64][Class loss: 0.956424832344055]\n",
      "[Iter 65][Class loss: 0.926933050155640]\n",
      "[Iter 66][Class loss: 0.938364863395691]\n",
      "[Iter 67][Class loss: 0.939801812171936]\n",
      "[Iter 68][Class loss: 0.919083237648010]\n",
      "[Iter 69][Class loss: 0.942048251628876]\n",
      "[Iter 70][Class loss: 0.914996147155762]\n",
      "[Iter 71][Class loss: 0.935501813888550]\n",
      "[Iter 72][Class loss: 0.913072824478149]\n",
      "[Iter 73][Class loss: 0.927106082439423]\n",
      "[Iter 74][Class loss: 0.912522375583649]\n",
      "[Iter 75][Class loss: 0.917268276214600]\n",
      "[Iter 76][Class loss: 0.916210830211639]\n",
      "[Iter 77][Class loss: 0.909242749214172]\n",
      "[Iter 78][Class loss: 0.916856467723846]\n",
      "[Iter 79][Class loss: 0.906564474105835]\n",
      "[Iter 80][Class loss: 0.914470553398132]\n",
      "[Iter 81][Class loss: 0.905240535736084]\n",
      "[Iter 82][Class loss: 0.910406827926636]\n",
      "[Iter 83][Class loss: 0.904560267925262]\n",
      "[Iter 84][Class loss: 0.905388593673706]\n",
      "[Iter 85][Class loss: 0.904687881469727]\n",
      "[Iter 86][Class loss: 0.901011824607849]\n",
      "[Iter 87][Class loss: 0.903660178184509]\n",
      "[Iter 88][Class loss: 0.898891627788544]\n",
      "[Iter 89][Class loss: 0.901357591152191]\n",
      "[Iter 90][Class loss: 0.897736907005310]\n",
      "[Iter 91][Class loss: 0.898138046264648]\n",
      "[Iter 92][Class loss: 0.896387457847595]\n",
      "[Iter 93][Class loss: 0.894644498825073]\n",
      "[Iter 94][Class loss: 0.894469797611237]\n",
      "[Iter 95][Class loss: 0.891547143459320]\n",
      "[Iter 96][Class loss: 0.892018735408783]\n",
      "[Iter 97][Class loss: 0.889460682868958]\n",
      "[Iter 98][Class loss: 0.889214396476746]\n",
      "[Iter 99][Class loss: 0.887509524822235]\n",
      "[Iter 100][Class loss: 0.886099457740784]\n",
      "[Iter 101][Class loss: 0.885159015655518]\n",
      "[Iter 102][Class loss: 0.883088350296021]\n",
      "[Iter 103][Class loss: 0.882266998291016]\n",
      "[Iter 104][Class loss: 0.880124926567078]\n",
      "[Iter 105][Class loss: 0.879010200500488]\n",
      "[Iter 106][Class loss: 0.877140402793884]\n",
      "[Iter 107][Class loss: 0.875430405139923]\n",
      "[Iter 108][Class loss: 0.873750805854797]\n",
      "[Iter 109][Class loss: 0.871498823165894]\n",
      "[Iter 110][Class loss: 0.869694948196411]\n",
      "[Iter 111][Class loss: 0.867175698280334]\n",
      "[Iter 112][Class loss: 0.865041613578796]\n",
      "[Iter 113][Class loss: 0.862412214279175]\n",
      "[Iter 114][Class loss: 0.859763026237488]\n",
      "[Iter 115][Class loss: 0.856956243515015]\n",
      "[Iter 116][Class loss: 0.853868007659912]\n",
      "[Iter 117][Class loss: 0.850751638412476]\n",
      "[Iter 118][Class loss: 0.847074568271637]\n",
      "[Iter 119][Class loss: 0.843376040458679]\n",
      "[Iter 120][Class loss: 0.839357256889343]\n",
      "[Iter 121][Class loss: 0.835368990898132]\n",
      "[Iter 122][Class loss: 0.830971956253052]\n",
      "[Iter 123][Class loss: 0.826104879379272]\n",
      "[Iter 124][Class loss: 0.820920884609222]\n",
      "[Iter 125][Class loss: 0.815391778945923]\n",
      "[Iter 126][Class loss: 0.809598445892334]\n",
      "[Iter 127][Class loss: 0.803272485733032]\n",
      "[Iter 128][Class loss: 0.797200739383698]\n",
      "[Iter 129][Class loss: 0.790916562080383]\n",
      "[Iter 130][Class loss: 0.784303188323975]\n",
      "[Iter 131][Class loss: 0.777348101139069]\n",
      "[Iter 132][Class loss: 0.769967019557953]\n",
      "[Iter 133][Class loss: 0.762190103530884]\n",
      "[Iter 134][Class loss: 0.754497587680817]\n",
      "[Iter 135][Class loss: 0.747003257274628]\n",
      "[Iter 136][Class loss: 0.739588081836700]\n",
      "[Iter 137][Class loss: 0.732151627540588]\n",
      "[Iter 138][Class loss: 0.724837660789490]\n",
      "[Iter 139][Class loss: 0.717582821846008]\n",
      "[Iter 140][Class loss: 0.710398674011230]\n",
      "[Iter 141][Class loss: 0.703439116477966]\n",
      "[Iter 142][Class loss: 0.696698248386383]\n",
      "[Iter 143][Class loss: 0.690320432186127]\n",
      "[Iter 144][Class loss: 0.684414803981781]\n",
      "[Iter 145][Class loss: 0.678960800170898]\n",
      "[Iter 146][Class loss: 0.673923909664154]\n",
      "[Iter 147][Class loss: 0.669345617294312]\n",
      "[Iter 148][Class loss: 0.665299654006958]\n",
      "[Iter 149][Class loss: 0.661783933639526]\n",
      "[Iter 150][Class loss: 0.658601760864258]\n",
      "[Iter 151][Class loss: 0.655803322792053]\n",
      "[Iter 152][Class loss: 0.653297841548920]\n",
      "[Iter 153][Class loss: 0.650989651679993]\n",
      "[Iter 154][Class loss: 0.648903131484985]\n",
      "[Iter 155][Class loss: 0.646961688995361]\n",
      "[Iter 156][Class loss: 0.645220875740051]\n",
      "[Iter 157][Class loss: 0.643639087677002]\n",
      "[Iter 158][Class loss: 0.642154455184937]\n",
      "[Iter 159][Class loss: 0.640765488147736]\n",
      "[Iter 160][Class loss: 0.639461874961853]\n",
      "[Iter 161][Class loss: 0.638239204883575]\n",
      "[Iter 162][Class loss: 0.637040436267853]\n",
      "[Iter 163][Class loss: 0.635896921157837]\n",
      "[Iter 164][Class loss: 0.634790360927582]\n",
      "[Iter 165][Class loss: 0.633712887763977]\n",
      "[Iter 166][Class loss: 0.632842779159546]\n",
      "[Iter 167][Class loss: 0.631826758384705]\n",
      "[Iter 168][Class loss: 0.630811691284180]\n",
      "[Iter 169][Class loss: 0.629805803298950]\n",
      "[Iter 170][Class loss: 0.628808915615082]\n",
      "[Iter 171][Class loss: 0.627817630767822]\n",
      "[Iter 172][Class loss: 0.626824498176575]\n",
      "[Iter 173][Class loss: 0.625818967819214]\n",
      "[Iter 174][Class loss: 0.624709486961365]\n",
      "[Iter 175][Class loss: 0.623603701591492]\n",
      "[Iter 176][Class loss: 0.622680425643921]\n",
      "[Iter 177][Class loss: 0.621696650981903]\n",
      "[Iter 178][Class loss: 0.620675921440125]\n",
      "[Iter 179][Class loss: 0.619644165039062]\n",
      "[Iter 180][Class loss: 0.618645846843719]\n",
      "[Iter 181][Class loss: 0.617646932601929]\n",
      "[Iter 182][Class loss: 0.616636514663696]\n",
      "[Iter 183][Class loss: 0.615597128868103]\n",
      "[Iter 184][Class loss: 0.614548087120056]\n",
      "[Iter 185][Class loss: 0.613513052463531]\n",
      "[Iter 186][Class loss: 0.612448215484619]\n",
      "[Iter 187][Class loss: 0.611380338668823]\n",
      "[Iter 188][Class loss: 0.610306084156036]\n",
      "[Iter 189][Class loss: 0.609261631965637]\n",
      "[Iter 190][Class loss: 0.608191013336182]\n",
      "[Iter 191][Class loss: 0.607069492340088]\n",
      "[Iter 192][Class loss: 0.605906069278717]\n",
      "[Iter 193][Class loss: 0.604746282100677]\n",
      "[Iter 194][Class loss: 0.603576719760895]\n",
      "[Iter 195][Class loss: 0.602370381355286]\n",
      "[Iter 196][Class loss: 0.601127505302429]\n",
      "[Iter 197][Class loss: 0.599852502346039]\n",
      "[Iter 198][Class loss: 0.598548173904419]\n",
      "[Iter 199][Class loss: 0.597254753112793]\n",
      "[Iter 200][Class loss: 0.595929086208344]\n",
      "[Iter 201][Class loss: 0.594550609588623]\n",
      "[Iter 202][Class loss: 0.593181967735291]\n",
      "[Iter 203][Class loss: 0.591769456863403]\n",
      "[Iter 204][Class loss: 0.590304374694824]\n",
      "[Iter 205][Class loss: 0.588805794715881]\n",
      "[Iter 206][Class loss: 0.587296366691589]\n",
      "[Iter 207][Class loss: 0.585764825344086]\n",
      "[Iter 208][Class loss: 0.584192991256714]\n",
      "[Iter 209][Class loss: 0.582591116428375]\n",
      "[Iter 210][Class loss: 0.580950140953064]\n",
      "[Iter 211][Class loss: 0.579261958599091]\n",
      "[Iter 212][Class loss: 0.577528417110443]\n",
      "[Iter 213][Class loss: 0.575739741325378]\n",
      "[Iter 214][Class loss: 0.573879420757294]\n",
      "[Iter 215][Class loss: 0.571982383728027]\n",
      "[Iter 216][Class loss: 0.570025205612183]\n",
      "[Iter 217][Class loss: 0.568022787570953]\n",
      "[Iter 218][Class loss: 0.565971016883850]\n",
      "[Iter 219][Class loss: 0.563872873783112]\n",
      "[Iter 220][Class loss: 0.561684310436249]\n",
      "[Iter 221][Class loss: 0.559417605400085]\n",
      "[Iter 222][Class loss: 0.557121396064758]\n",
      "[Iter 223][Class loss: 0.554712295532227]\n",
      "[Iter 224][Class loss: 0.552229404449463]\n",
      "[Iter 225][Class loss: 0.549617648124695]\n",
      "[Iter 226][Class loss: 0.546854913234711]\n",
      "[Iter 227][Class loss: 0.544029593467712]\n",
      "[Iter 228][Class loss: 0.541051805019379]\n",
      "[Iter 229][Class loss: 0.537911415100098]\n",
      "[Iter 230][Class loss: 0.534628927707672]\n",
      "[Iter 231][Class loss: 0.531227350234985]\n",
      "[Iter 232][Class loss: 0.527583956718445]\n",
      "[Iter 233][Class loss: 0.523808002471924]\n",
      "[Iter 234][Class loss: 0.519929647445679]\n",
      "[Iter 235][Class loss: 0.515890300273895]\n",
      "[Iter 236][Class loss: 0.511662244796753]\n",
      "[Iter 237][Class loss: 0.507190227508545]\n",
      "[Iter 238][Class loss: 0.502541422843933]\n",
      "[Iter 239][Class loss: 0.497657418251038]\n",
      "[Iter 240][Class loss: 0.492534816265106]\n",
      "[Iter 241][Class loss: 0.487102657556534]\n",
      "[Iter 242][Class loss: 0.481442064046860]\n",
      "[Iter 243][Class loss: 0.475422501564026]\n",
      "[Iter 244][Class loss: 0.469063162803650]\n",
      "[Iter 245][Class loss: 0.462461084127426]\n",
      "[Iter 246][Class loss: 0.455362945795059]\n",
      "[Iter 247][Class loss: 0.447935789823532]\n",
      "[Iter 248][Class loss: 0.439987838268280]\n",
      "[Iter 249][Class loss: 0.431536972522736]\n",
      "[Iter 250][Class loss: 0.422584533691406]\n",
      "[Iter 251][Class loss: 0.413086265325546]\n",
      "[Iter 252][Class loss: 0.402934670448303]\n",
      "[Iter 253][Class loss: 0.392179220914841]\n",
      "[Iter 254][Class loss: 0.380691766738892]\n",
      "[Iter 255][Class loss: 0.368479311466217]\n",
      "[Iter 256][Class loss: 0.355504214763641]\n",
      "[Iter 257][Class loss: 0.341731607913971]\n",
      "[Iter 258][Class loss: 0.327159643173218]\n",
      "[Iter 259][Class loss: 0.311779081821442]\n",
      "[Iter 260][Class loss: 0.295712500810623]\n",
      "[Iter 261][Class loss: 0.279136925935745]\n",
      "[Iter 262][Class loss: 0.262187957763672]\n",
      "[Iter 263][Class loss: 0.245259970426559]\n",
      "[Iter 264][Class loss: 0.228807657957077]\n",
      "[Iter 265][Class loss: 0.213277071714401]\n",
      "[Iter 266][Class loss: 0.199064090847969]\n",
      "[Iter 267][Class loss: 0.186464115977287]\n",
      "[Iter 268][Class loss: 0.175912737846375]\n",
      "[Iter 269][Class loss: 0.167399674654007]\n",
      "[Iter 270][Class loss: 0.160928294062614]\n",
      "[Iter 271][Class loss: 0.156191155314445]\n",
      "[Iter 272][Class loss: 0.152795419096947]\n",
      "[Iter 273][Class loss: 0.150506630539894]\n",
      "[Iter 274][Class loss: 0.148881927132607]\n",
      "[Iter 275][Class loss: 0.147714078426361]\n",
      "[Iter 276][Class loss: 0.146864250302315]\n",
      "[Iter 277][Class loss: 0.146276086568832]\n",
      "[Iter 278][Class loss: 0.145864844322205]\n",
      "[Iter 279][Class loss: 0.145558059215546]\n",
      "[Iter 280][Class loss: 0.145288079977036]\n",
      "[Iter 281][Class loss: 0.145062312483788]\n",
      "[Iter 282][Class loss: 0.144887998700142]\n",
      "[Iter 283][Class loss: 0.144682452082634]\n",
      "[Iter 284][Class loss: 0.144533231854439]\n",
      "[Iter 285][Class loss: 0.144401103258133]\n",
      "[Iter 286][Class loss: 0.144264027476311]\n",
      "[Iter 287][Class loss: 0.144154474139214]\n",
      "[Iter 288][Class loss: 0.144054979085922]\n",
      "[Iter 289][Class loss: 0.143980145454407]\n",
      "[Iter 290][Class loss: 0.143924385309219]\n",
      "[Iter 291][Class loss: 0.143850311636925]\n",
      "[Iter 292][Class loss: 0.143775522708893]\n",
      "[Iter 293][Class loss: 0.143707007169724]\n",
      "[Iter 294][Class loss: 0.143646016716957]\n",
      "[Iter 295][Class loss: 0.143589660525322]\n",
      "[Iter 296][Class loss: 0.143529281020164]\n",
      "[Iter 297][Class loss: 0.143476337194443]\n",
      "[Iter 298][Class loss: 0.143422767519951]\n",
      "[Iter 299][Class loss: 0.143365949392319]\n",
      "[Iter 300][Class loss: 0.143312677741051]\n",
      "[Iter 301][Class loss: 0.143265143036842]\n",
      "[Iter 302][Class loss: 0.143231034278870]\n",
      "[Iter 303][Class loss: 0.143185958266258]\n",
      "[Iter 304][Class loss: 0.143135204911232]\n",
      "[Iter 305][Class loss: 0.143090084195137]\n",
      "[Iter 306][Class loss: 0.143043413758278]\n",
      "[Iter 307][Class loss: 0.142995223402977]\n",
      "[Iter 308][Class loss: 0.142962947487831]\n",
      "[Iter 309][Class loss: 0.142931953072548]\n",
      "[Iter 310][Class loss: 0.142891719937325]\n",
      "[Iter 311][Class loss: 0.142846152186394]\n",
      "[Iter 312][Class loss: 0.142798393964767]\n",
      "[Iter 313][Class loss: 0.142774179577827]\n",
      "[Iter 314][Class loss: 0.142744958400726]\n",
      "[Iter 315][Class loss: 0.142706707119942]\n",
      "[Iter 316][Class loss: 0.142663657665253]\n",
      "[Iter 317][Class loss: 0.142619267106056]\n",
      "[Iter 318][Class loss: 0.142511352896690]\n",
      "[Iter 319][Class loss: 0.142483353614807]\n",
      "[Iter 320][Class loss: 0.142453640699387]\n",
      "[Iter 321][Class loss: 0.142422392964363]\n",
      "[Iter 322][Class loss: 0.142391651868820]\n",
      "[Iter 323][Class loss: 0.142373815178871]\n",
      "[Iter 324][Class loss: 0.142322331666946]\n",
      "[Iter 325][Class loss: 0.142284989356995]\n",
      "[Iter 326][Class loss: 0.142248809337616]\n",
      "[Iter 327][Class loss: 0.142213061451912]\n",
      "[Iter 328][Class loss: 0.142163485288620]\n",
      "[Iter 329][Class loss: 0.142109349370003]\n",
      "[Iter 330][Class loss: 0.142054229974747]\n",
      "[Iter 331][Class loss: 0.142004892230034]\n",
      "[Iter 332][Class loss: 0.141958564519882]\n",
      "[Iter 333][Class loss: 0.141910165548325]\n",
      "[Iter 334][Class loss: 0.141880199313164]\n",
      "[Iter 335][Class loss: 0.141813218593597]\n",
      "[Iter 336][Class loss: 0.141759470105171]\n",
      "[Iter 337][Class loss: 0.141712442040443]\n",
      "[Iter 338][Class loss: 0.141665816307068]\n",
      "[Iter 339][Class loss: 0.141614660620689]\n",
      "[Iter 340][Class loss: 0.141523063182831]\n",
      "[Iter 341][Class loss: 0.141465947031975]\n",
      "[Iter 342][Class loss: 0.141405656933784]\n",
      "[Iter 343][Class loss: 0.141346871852875]\n",
      "[Iter 344][Class loss: 0.141292989253998]\n",
      "[Iter 345][Class loss: 0.141234591603279]\n",
      "[Iter 346][Class loss: 0.141178503632545]\n",
      "[Iter 347][Class loss: 0.141119435429573]\n",
      "[Iter 348][Class loss: 0.141057476401329]\n",
      "[Iter 349][Class loss: 0.140993416309357]\n",
      "[Iter 350][Class loss: 0.140934571623802]\n",
      "[Iter 351][Class loss: 0.140887707471848]\n",
      "[Iter 352][Class loss: 0.140833526849747]\n",
      "[Iter 353][Class loss: 0.140774577856064]\n",
      "[Iter 354][Class loss: 0.140714198350906]\n",
      "[Iter 355][Class loss: 0.140654593706131]\n",
      "[Iter 356][Class loss: 0.140594005584717]\n",
      "[Iter 357][Class loss: 0.140528172254562]\n",
      "[Iter 358][Class loss: 0.140468716621399]\n",
      "[Iter 359][Class loss: 0.140396937727928]\n",
      "[Iter 360][Class loss: 0.140331193804741]\n",
      "[Iter 361][Class loss: 0.140265271067619]\n",
      "[Iter 362][Class loss: 0.140193745493889]\n",
      "[Iter 363][Class loss: 0.140134111046791]\n",
      "[Iter 364][Class loss: 0.140066325664520]\n",
      "[Iter 365][Class loss: 0.140003696084023]\n",
      "[Iter 366][Class loss: 0.139935925602913]\n",
      "[Iter 367][Class loss: 0.139868646860123]\n",
      "[Iter 368][Class loss: 0.139799177646637]\n",
      "[Iter 369][Class loss: 0.139734223484993]\n",
      "[Iter 370][Class loss: 0.139664396643639]\n",
      "[Iter 371][Class loss: 0.139596387743950]\n",
      "[Iter 372][Class loss: 0.139532223343849]\n",
      "[Iter 373][Class loss: 0.139468014240265]\n",
      "[Iter 374][Class loss: 0.139409914612770]\n",
      "[Iter 375][Class loss: 0.139344289898872]\n",
      "[Iter 376][Class loss: 0.139283716678619]\n",
      "[Iter 377][Class loss: 0.139222204685211]\n",
      "[Iter 378][Class loss: 0.139156520366669]\n",
      "[Iter 379][Class loss: 0.139094650745392]\n",
      "[Iter 380][Class loss: 0.139019086956978]\n",
      "[Iter 381][Class loss: 0.138953134417534]\n",
      "[Iter 382][Class loss: 0.138886153697968]\n",
      "[Iter 383][Class loss: 0.138810545206070]\n",
      "[Iter 384][Class loss: 0.138747468590736]\n",
      "[Iter 385][Class loss: 0.138678550720215]\n",
      "[Iter 386][Class loss: 0.138614147901535]\n",
      "[Iter 387][Class loss: 0.138532891869545]\n",
      "[Iter 388][Class loss: 0.138462752103806]\n",
      "[Iter 389][Class loss: 0.138398706912994]\n",
      "[Iter 390][Class loss: 0.138319268822670]\n",
      "[Iter 391][Class loss: 0.138254225254059]\n",
      "[Iter 392][Class loss: 0.138171046972275]\n",
      "[Iter 393][Class loss: 0.138091981410980]\n",
      "[Iter 394][Class loss: 0.138027295470238]\n",
      "[Iter 395][Class loss: 0.137931853532791]\n",
      "[Iter 396][Class loss: 0.137844249606133]\n",
      "[Iter 397][Class loss: 0.137746259570122]\n",
      "[Iter 398][Class loss: 0.137651979923248]\n",
      "[Iter 399][Class loss: 0.137556567788124]\n",
      "[Iter 400][Class loss: 0.137473806738853]\n",
      "[Iter 401][Class loss: 0.137398853898048]\n",
      "[Iter 402][Class loss: 0.137317121028900]\n",
      "[Iter 403][Class loss: 0.137241378426552]\n",
      "[Iter 404][Class loss: 0.137162730097771]\n",
      "[Iter 405][Class loss: 0.137089401483536]\n",
      "[Iter 406][Class loss: 0.136997535824776]\n",
      "[Iter 407][Class loss: 0.136911705136299]\n",
      "[Iter 408][Class loss: 0.136827126145363]\n",
      "[Iter 409][Class loss: 0.136757299304008]\n",
      "[Iter 410][Class loss: 0.136669278144836]\n",
      "[Iter 411][Class loss: 0.136585935950279]\n",
      "[Iter 412][Class loss: 0.136474430561066]\n",
      "[Iter 413][Class loss: 0.136375248432159]\n",
      "[Iter 414][Class loss: 0.136253491044044]\n",
      "[Iter 415][Class loss: 0.136150255799294]\n",
      "[Iter 416][Class loss: 0.136045366525650]\n",
      "[Iter 417][Class loss: 0.135950937867165]\n",
      "[Iter 418][Class loss: 0.135850220918655]\n",
      "[Iter 419][Class loss: 0.135747209191322]\n",
      "[Iter 420][Class loss: 0.135643243789673]\n",
      "[Iter 421][Class loss: 0.135535880923271]\n",
      "[Iter 422][Class loss: 0.135429531335831]\n",
      "[Iter 423][Class loss: 0.135309964418411]\n",
      "[Iter 424][Class loss: 0.135188743472099]\n",
      "[Iter 425][Class loss: 0.135076791048050]\n",
      "[Iter 426][Class loss: 0.134961947798729]\n",
      "[Iter 427][Class loss: 0.134859949350357]\n",
      "[Iter 428][Class loss: 0.134736806154251]\n",
      "[Iter 429][Class loss: 0.134660437703133]\n",
      "[Iter 430][Class loss: 0.134531795978546]\n",
      "[Iter 431][Class loss: 0.134423285722733]\n",
      "[Iter 432][Class loss: 0.134323641657829]\n",
      "[Iter 433][Class loss: 0.134195849299431]\n",
      "[Iter 434][Class loss: 0.134064018726349]\n",
      "[Iter 435][Class loss: 0.133952453732491]\n",
      "[Iter 436][Class loss: 0.133824691176414]\n",
      "[Iter 437][Class loss: 0.133696779608727]\n",
      "[Iter 438][Class loss: 0.133567646145821]\n",
      "[Iter 439][Class loss: 0.133447423577309]\n",
      "[Iter 440][Class loss: 0.133313253521919]\n",
      "[Iter 441][Class loss: 0.133177325129509]\n",
      "[Iter 442][Class loss: 0.133039951324463]\n",
      "[Iter 443][Class loss: 0.132934272289276]\n",
      "[Iter 444][Class loss: 0.132796794176102]\n",
      "[Iter 445][Class loss: 0.132643088698387]\n",
      "[Iter 446][Class loss: 0.132554963231087]\n",
      "[Iter 447][Class loss: 0.132375285029411]\n",
      "[Iter 448][Class loss: 0.132224380970001]\n",
      "[Iter 449][Class loss: 0.132072210311890]\n",
      "[Iter 450][Class loss: 0.131931617856026]\n",
      "[Iter 451][Class loss: 0.131750240921974]\n",
      "[Iter 452][Class loss: 0.131567999720573]\n",
      "[Iter 453][Class loss: 0.131398305296898]\n",
      "[Iter 454][Class loss: 0.131241858005524]\n",
      "[Iter 455][Class loss: 0.131042420864105]\n",
      "[Iter 456][Class loss: 0.131046190857887]\n",
      "[Iter 457][Class loss: 0.130953237414360]\n",
      "[Iter 458][Class loss: 0.130850777029991]\n",
      "[Iter 459][Class loss: 0.130810633301735]\n",
      "[Iter 460][Class loss: 0.130643337965012]\n",
      "[Iter 461][Class loss: 0.130440562963486]\n",
      "[Iter 462][Class loss: 0.130294352769852]\n",
      "[Iter 463][Class loss: 0.130099326372147]\n",
      "[Iter 464][Class loss: 0.129811421036720]\n",
      "[Iter 465][Class loss: 0.129700809717178]\n",
      "[Iter 466][Class loss: 0.129464298486710]\n",
      "[Iter 467][Class loss: 0.129197195172310]\n",
      "[Iter 468][Class loss: 0.129060462117195]\n",
      "[Iter 469][Class loss: 0.128925949335098]\n",
      "[Iter 470][Class loss: 0.128659218549728]\n",
      "[Iter 471][Class loss: 0.128505751490593]\n",
      "[Iter 472][Class loss: 0.128364250063896]\n",
      "[Iter 473][Class loss: 0.128212764859200]\n",
      "[Iter 474][Class loss: 0.128061562776566]\n",
      "[Iter 475][Class loss: 0.127879634499550]\n",
      "[Iter 476][Class loss: 0.127834528684616]\n",
      "[Iter 477][Class loss: 0.127687677741051]\n",
      "[Iter 478][Class loss: 0.127439901232719]\n",
      "[Iter 479][Class loss: 0.127479523420334]\n",
      "[Iter 480][Class loss: 0.127145498991013]\n",
      "[Iter 481][Class loss: 0.127160370349884]\n",
      "[Iter 482][Class loss: 0.126854524016380]\n",
      "[Iter 483][Class loss: 0.126761153340340]\n",
      "[Iter 484][Class loss: 0.126580238342285]\n",
      "[Iter 485][Class loss: 0.126427963376045]\n",
      "[Iter 486][Class loss: 0.126321479678154]\n",
      "[Iter 487][Class loss: 0.126115396618843]\n",
      "[Iter 488][Class loss: 0.126008421182632]\n",
      "[Iter 489][Class loss: 0.125867038965225]\n",
      "[Iter 490][Class loss: 0.125680938363075]\n",
      "[Iter 491][Class loss: 0.125568777322769]\n",
      "[Iter 492][Class loss: 0.125409916043282]\n",
      "[Iter 493][Class loss: 0.125315129756927]\n",
      "[Iter 494][Class loss: 0.125164225697517]\n",
      "[Iter 495][Class loss: 0.124996051192284]\n",
      "[Iter 496][Class loss: 0.125142678618431]\n",
      "[Iter 497][Class loss: 0.126366794109344]\n",
      "[Iter 498][Class loss: 0.124871432781219]\n",
      "[Iter 499][Class loss: 0.125595226883888]\n",
      "[Iter 500][Class loss: 0.125208392739296]\n",
      "[Iter 501][Class loss: 0.124769151210785]\n",
      "[Iter 502][Class loss: 0.125114321708679]\n",
      "[Iter 503][Class loss: 0.124429509043694]\n",
      "[Iter 504][Class loss: 0.124800086021423]\n",
      "[Iter 505][Class loss: 0.124140687286854]\n",
      "[Iter 506][Class loss: 0.124305404722691]\n",
      "[Iter 507][Class loss: 0.123981535434723]\n",
      "[Iter 508][Class loss: 0.123811550438404]\n",
      "[Iter 509][Class loss: 0.123848058283329]\n",
      "[Iter 510][Class loss: 0.123368777334690]\n",
      "[Iter 511][Class loss: 0.123530857264996]\n",
      "[Iter 512][Class loss: 0.123117059469223]\n",
      "[Iter 513][Class loss: 0.123155772686005]\n",
      "[Iter 514][Class loss: 0.122873589396477]\n",
      "[Iter 515][Class loss: 0.122708991169930]\n",
      "[Iter 516][Class loss: 0.122537985444069]\n",
      "[Iter 517][Class loss: 0.122478023171425]\n",
      "[Iter 518][Class loss: 0.122161738574505]\n",
      "[Iter 519][Class loss: 0.122142091393471]\n",
      "[Iter 520][Class loss: 0.121932417154312]\n",
      "[Iter 521][Class loss: 0.121878668665886]\n",
      "[Iter 522][Class loss: 0.121713690459728]\n",
      "[Iter 523][Class loss: 0.121527753770351]\n",
      "[Iter 524][Class loss: 0.121398285031319]\n",
      "[Iter 525][Class loss: 0.121177263557911]\n",
      "[Iter 526][Class loss: 0.121076181530952]\n",
      "[Iter 527][Class loss: 0.120909199118614]\n",
      "[Iter 528][Class loss: 0.120829336345196]\n",
      "[Iter 529][Class loss: 0.120710663497448]\n",
      "[Iter 530][Class loss: 0.120538391172886]\n",
      "[Iter 531][Class loss: 0.120403058826923]\n",
      "[Iter 532][Class loss: 0.120251238346100]\n",
      "[Iter 533][Class loss: 0.120123393833637]\n",
      "[Iter 534][Class loss: 0.119927860796452]\n",
      "[Iter 535][Class loss: 0.119870543479919]\n",
      "[Iter 536][Class loss: 0.119654148817062]\n",
      "[Iter 537][Class loss: 0.119580067694187]\n",
      "[Iter 538][Class loss: 0.119571901857853]\n",
      "[Iter 539][Class loss: 0.119318880140781]\n",
      "[Iter 540][Class loss: 0.119155526161194]\n",
      "[Iter 541][Class loss: 0.119067229330540]\n",
      "[Iter 542][Class loss: 0.118893973529339]\n",
      "[Iter 543][Class loss: 0.118806697428226]\n",
      "[Iter 544][Class loss: 0.118660971522331]\n",
      "[Iter 545][Class loss: 0.118551358580589]\n",
      "[Iter 546][Class loss: 0.118384249508381]\n",
      "[Iter 547][Class loss: 0.118305265903473]\n",
      "[Iter 548][Class loss: 0.118148401379585]\n",
      "[Iter 549][Class loss: 0.117988862097263]\n",
      "[Iter 550][Class loss: 0.117860138416290]\n",
      "[Iter 551][Class loss: 0.117710508406162]\n",
      "[Iter 552][Class loss: 0.117584809660912]\n",
      "[Iter 553][Class loss: 0.117393717169762]\n",
      "[Iter 554][Class loss: 0.117246091365814]\n",
      "[Iter 555][Class loss: 0.117098793387413]\n",
      "[Iter 556][Class loss: 0.116940096020699]\n",
      "[Iter 557][Class loss: 0.116768121719360]\n",
      "[Iter 558][Class loss: 0.116585910320282]\n",
      "[Iter 559][Class loss: 0.116447679698467]\n",
      "[Iter 560][Class loss: 0.116244897246361]\n",
      "[Iter 561][Class loss: 0.116071157157421]\n",
      "[Iter 562][Class loss: 0.115838520228863]\n",
      "[Iter 563][Class loss: 0.116148591041565]\n",
      "[Iter 564][Class loss: 0.115685820579529]\n",
      "[Iter 565][Class loss: 0.115577980875969]\n",
      "[Iter 566][Class loss: 0.115766882896423]\n",
      "[Iter 567][Class loss: 0.115469947457314]\n",
      "[Iter 568][Class loss: 0.114897422492504]\n",
      "[Iter 569][Class loss: 0.115190431475639]\n",
      "[Iter 570][Class loss: 0.114751383662224]\n",
      "[Iter 571][Class loss: 0.114394456148148]\n",
      "[Iter 572][Class loss: 0.114374332129955]\n",
      "[Iter 573][Class loss: 0.114126279950142]\n",
      "[Iter 574][Class loss: 0.113682642579079]\n",
      "[Iter 575][Class loss: 0.113533601164818]\n",
      "[Iter 576][Class loss: 0.113474227488041]\n",
      "[Iter 577][Class loss: 0.113005004823208]\n",
      "[Iter 578][Class loss: 0.112833991646767]\n",
      "[Iter 579][Class loss: 0.112527094781399]\n",
      "[Iter 580][Class loss: 0.111905671656132]\n",
      "[Iter 581][Class loss: 0.111788555979729]\n",
      "[Iter 582][Class loss: 0.111212588846684]\n",
      "[Iter 583][Class loss: 0.110757559537888]\n",
      "[Iter 584][Class loss: 0.110484443604946]\n",
      "[Iter 585][Class loss: 0.110183805227280]\n",
      "[Iter 586][Class loss: 0.109716087579727]\n",
      "[Iter 587][Class loss: 0.109374433755875]\n",
      "[Iter 588][Class loss: 0.109006725251675]\n",
      "[Iter 589][Class loss: 0.108559362590313]\n",
      "[Iter 590][Class loss: 0.108187913894653]\n",
      "[Iter 591][Class loss: 0.107831820845604]\n",
      "[Iter 592][Class loss: 0.107307247817516]\n",
      "[Iter 593][Class loss: 0.107073038816452]\n",
      "[Iter 594][Class loss: 0.106319651007652]\n",
      "[Iter 595][Class loss: 0.105827942490578]\n",
      "[Iter 596][Class loss: 0.105279795825481]\n",
      "[Iter 597][Class loss: 0.104710511863232]\n",
      "[Iter 598][Class loss: 0.104103170335293]\n",
      "[Iter 599][Class loss: 0.103527523577213]\n",
      "[Iter 600][Class loss: 0.102976165711880]\n",
      "[Iter 601][Class loss: 0.102739065885544]\n",
      "[Iter 602][Class loss: 0.101488679647446]\n",
      "[Iter 603][Class loss: 0.100311614573002]\n",
      "[Iter 604][Class loss: 0.099701121449471]\n",
      "[Iter 605][Class loss: 0.101218789815903]\n",
      "[Iter 606][Class loss: 0.110813610255718]\n",
      "[Iter 607][Class loss: 0.151598051190376]\n",
      "[Iter 608][Class loss: 0.228608652949333]\n",
      "[Iter 609][Class loss: 0.156732186675072]\n",
      "[Iter 610][Class loss: 0.106448821723461]\n",
      "[Iter 611][Class loss: 0.118771463632584]\n",
      "[Iter 612][Class loss: 0.111056938767433]\n",
      "[Iter 613][Class loss: 0.120375163853168]\n",
      "[Iter 614][Class loss: 0.109113007783890]\n",
      "[Iter 615][Class loss: 0.118057504296303]\n",
      "[Iter 616][Class loss: 0.107054203748703]\n",
      "[Iter 617][Class loss: 0.112313039600849]\n",
      "[Iter 618][Class loss: 0.106164015829563]\n",
      "[Iter 619][Class loss: 0.106301151216030]\n",
      "[Iter 620][Class loss: 0.104527845978737]\n",
      "[Iter 621][Class loss: 0.102580837905407]\n",
      "[Iter 622][Class loss: 0.102352887392044]\n",
      "[Iter 623][Class loss: 0.098561093211174]\n",
      "[Iter 624][Class loss: 0.097740389406681]\n",
      "[Iter 625][Class loss: 0.094160489737988]\n",
      "[Iter 626][Class loss: 0.094177372753620]\n",
      "[Iter 627][Class loss: 0.090528279542923]\n",
      "[Iter 628][Class loss: 0.089431844651699]\n",
      "[Iter 629][Class loss: 0.087015464901924]\n",
      "[Iter 630][Class loss: 0.085020631551743]\n",
      "[Iter 631][Class loss: 0.082090415060520]\n",
      "[Iter 632][Class loss: 0.079050309956074]\n",
      "[Iter 633][Class loss: 0.075985737144947]\n",
      "[Iter 634][Class loss: 0.074078671634197]\n",
      "[Iter 635][Class loss: 0.072134397923946]\n",
      "[Iter 636][Class loss: 0.069739803671837]\n",
      "[Iter 637][Class loss: 0.066842399537563]\n",
      "[Iter 638][Class loss: 0.063686780631542]\n",
      "[Iter 639][Class loss: 0.060096457600594]\n",
      "[Iter 640][Class loss: 0.057162713259459]\n",
      "[Iter 641][Class loss: 0.054922346025705]\n",
      "[Iter 642][Class loss: 0.051149610430002]\n",
      "[Iter 643][Class loss: 0.048937134444714]\n",
      "[Iter 644][Class loss: 0.045678485184908]\n",
      "[Iter 645][Class loss: 0.042638022452593]\n",
      "[Iter 646][Class loss: 0.039463542401791]\n",
      "[Iter 647][Class loss: 0.036468315869570]\n",
      "[Iter 648][Class loss: 0.033889770507812]\n",
      "[Iter 649][Class loss: 0.030337164178491]\n",
      "[Iter 650][Class loss: 0.028109055012465]\n",
      "[Iter 651][Class loss: 0.025593921542168]\n",
      "[Iter 652][Class loss: 0.023063218221068]\n",
      "[Iter 653][Class loss: 0.020697044208646]\n",
      "[Iter 654][Class loss: 0.019074384123087]\n",
      "[Iter 655][Class loss: 0.017100872471929]\n",
      "[Iter 656][Class loss: 0.015065194107592]\n",
      "[Iter 657][Class loss: 0.013502521440387]\n",
      "[Iter 658][Class loss: 0.012196329422295]\n",
      "[Iter 659][Class loss: 0.010698386467993]\n",
      "[Iter 660][Class loss: 0.009387172758579]\n",
      "[Iter 661][Class loss: 0.008505033329129]\n",
      "[Iter 662][Class loss: 0.007769503165036]\n",
      "[Iter 663][Class loss: 0.006887740455568]\n",
      "[Iter 664][Class loss: 0.006080529652536]\n",
      "[Iter 665][Class loss: 0.005514014977962]\n",
      "[Iter 666][Class loss: 0.004971286281943]\n",
      "[Iter 667][Class loss: 0.004485394340008]\n",
      "[Iter 668][Class loss: 0.004030484240502]\n",
      "[Iter 669][Class loss: 0.003614743007347]\n",
      "[Iter 670][Class loss: 0.003315088571981]\n",
      "[Iter 671][Class loss: 0.003068114630878]\n",
      "[Iter 672][Class loss: 0.002822300884873]\n",
      "[Iter 673][Class loss: 0.002570223063231]\n",
      "[Iter 674][Class loss: 0.002368973568082]\n",
      "[Iter 675][Class loss: 0.002207734622061]\n",
      "[Iter 676][Class loss: 0.002066612942144]\n",
      "[Iter 677][Class loss: 0.001906359568238]\n",
      "[Iter 678][Class loss: 0.001758074155077]\n",
      "[Iter 679][Class loss: 0.001655097235925]\n",
      "[Iter 680][Class loss: 0.001566902035847]\n",
      "[Iter 681][Class loss: 0.001472980482504]\n",
      "[Iter 682][Class loss: 0.001384222647175]\n",
      "[Iter 683][Class loss: 0.001299176830798]\n",
      "[Iter 684][Class loss: 0.001230618450791]\n",
      "[Iter 685][Class loss: 0.001178106525913]\n",
      "[Iter 686][Class loss: 0.001127003575675]\n",
      "[Iter 687][Class loss: 0.001076309476048]\n",
      "[Iter 688][Class loss: 0.001027096295729]\n",
      "[Iter 689][Class loss: 0.000979709438980]\n",
      "[Iter 690][Class loss: 0.000936490774620]\n",
      "[Iter 691][Class loss: 0.000899261503946]\n",
      "[Iter 692][Class loss: 0.000873641518410]\n",
      "[Iter 693][Class loss: 0.000849190808367]\n",
      "[Iter 694][Class loss: 0.000817220483441]\n",
      "[Iter 695][Class loss: 0.000786806922406]\n",
      "[Iter 696][Class loss: 0.000759726739489]\n",
      "[Iter 697][Class loss: 0.000739246024750]\n",
      "[Iter 698][Class loss: 0.000719767762348]\n",
      "[Iter 699][Class loss: 0.000700658652931]\n",
      "[Iter 700][Class loss: 0.000681754841935]\n",
      "[Iter 701][Class loss: 0.000663211743813]\n",
      "[Iter 702][Class loss: 0.000645651598461]\n",
      "[Iter 703][Class loss: 0.000629449728876]\n",
      "[Iter 704][Class loss: 0.000612214440480]\n",
      "[Iter 705][Class loss: 0.000598470098339]\n",
      "[Iter 706][Class loss: 0.000586197944358]\n",
      "[Iter 707][Class loss: 0.000575027486775]\n",
      "[Iter 708][Class loss: 0.000563961453736]\n",
      "[Iter 709][Class loss: 0.000550305529032]\n",
      "[Iter 710][Class loss: 0.000539049156941]\n",
      "[Iter 711][Class loss: 0.000527744239662]\n",
      "[Iter 712][Class loss: 0.000520937144756]\n",
      "[Iter 713][Class loss: 0.000511398946401]\n",
      "[Iter 714][Class loss: 0.000503658084199]\n",
      "[Iter 715][Class loss: 0.000496323278639]\n",
      "[Iter 716][Class loss: 0.000488131481688]\n",
      "[Iter 717][Class loss: 0.000479427108075]\n",
      "[Iter 718][Class loss: 0.000471364008263]\n",
      "[Iter 719][Class loss: 0.000465647550300]\n",
      "[Iter 720][Class loss: 0.000458207214251]\n",
      "[Iter 721][Class loss: 0.000452908949228]\n",
      "[Iter 722][Class loss: 0.000446634978289]\n",
      "[Iter 723][Class loss: 0.000439156952780]\n",
      "[Iter 724][Class loss: 0.000434625486378]\n",
      "[Iter 725][Class loss: 0.000427781400504]\n",
      "[Iter 726][Class loss: 0.000423267483711]\n",
      "[Iter 727][Class loss: 0.000418198615080]\n",
      "[Iter 728][Class loss: 0.000412489636801]\n",
      "[Iter 729][Class loss: 0.000407448271289]\n",
      "[Iter 730][Class loss: 0.000402765668696]\n",
      "[Iter 731][Class loss: 0.000397560506826]\n",
      "[Iter 732][Class loss: 0.000392748304876]\n",
      "[Iter 733][Class loss: 0.000388717802707]\n",
      "[Iter 734][Class loss: 0.000383765931474]\n",
      "[Iter 735][Class loss: 0.000379386998247]\n",
      "[Iter 736][Class loss: 0.000375780044124]\n",
      "[Iter 737][Class loss: 0.000371934671421]\n",
      "[Iter 738][Class loss: 0.000367580272723]\n",
      "[Iter 739][Class loss: 0.000363053841284]\n",
      "[Iter 740][Class loss: 0.000360094418284]\n",
      "[Iter 741][Class loss: 0.000356008007657]\n",
      "[Iter 742][Class loss: 0.000353010225808]\n",
      "[Iter 743][Class loss: 0.000349420413841]\n",
      "[Iter 744][Class loss: 0.000346298562363]\n",
      "[Iter 745][Class loss: 0.000342526822351]\n",
      "[Iter 746][Class loss: 0.000339438876836]\n",
      "[Iter 747][Class loss: 0.000336251454428]\n",
      "[Iter 748][Class loss: 0.000332581403200]\n",
      "[Iter 749][Class loss: 0.000329719128786]\n",
      "[Iter 750][Class loss: 0.000326143461280]\n",
      "[Iter 751][Class loss: 0.000323292362737]\n",
      "[Iter 752][Class loss: 0.000320270366501]\n",
      "[Iter 753][Class loss: 0.000317252706736]\n",
      "[Iter 754][Class loss: 0.000314593984513]\n",
      "[Iter 755][Class loss: 0.000311711395625]\n",
      "[Iter 756][Class loss: 0.000309593306156]\n",
      "[Iter 757][Class loss: 0.000306780071696]\n",
      "[Iter 758][Class loss: 0.000303801323753]\n",
      "[Iter 759][Class loss: 0.000301789666992]\n",
      "[Iter 760][Class loss: 0.000298944651149]\n",
      "[Iter 761][Class loss: 0.000296083715511]\n",
      "[Iter 762][Class loss: 0.000294074387057]\n",
      "[Iter 763][Class loss: 0.000291431933874]\n",
      "[Iter 764][Class loss: 0.000288642651867]\n",
      "[Iter 765][Class loss: 0.000286720576696]\n",
      "[Iter 766][Class loss: 0.000283909786958]\n",
      "[Iter 767][Class loss: 0.000281741027720]\n",
      "[Iter 768][Class loss: 0.000279424624750]\n",
      "[Iter 769][Class loss: 0.000277127255686]\n",
      "[Iter 770][Class loss: 0.000274987658486]\n",
      "[Iter 771][Class loss: 0.000273152603768]\n",
      "[Iter 772][Class loss: 0.000270755990641]\n",
      "[Iter 773][Class loss: 0.000269328535069]\n",
      "[Iter 774][Class loss: 0.000267212482868]\n",
      "[Iter 775][Class loss: 0.000264945236268]\n",
      "[Iter 776][Class loss: 0.000263231399003]\n",
      "[Iter 777][Class loss: 0.000261011824477]\n",
      "[Iter 778][Class loss: 0.000259368884144]\n",
      "[Iter 779][Class loss: 0.000257126259385]\n",
      "[Iter 780][Class loss: 0.000255259976257]\n",
      "[Iter 781][Class loss: 0.000253423466347]\n",
      "[Iter 782][Class loss: 0.000251956284046]\n",
      "[Iter 783][Class loss: 0.000249890756095]\n",
      "[Iter 784][Class loss: 0.000248408410698]\n",
      "[Iter 785][Class loss: 0.000246633251663]\n",
      "[Iter 786][Class loss: 0.000244834693149]\n",
      "[Iter 787][Class loss: 0.000243371556280]\n",
      "[Iter 788][Class loss: 0.000241507106693]\n",
      "[Iter 789][Class loss: 0.000239838467678]\n",
      "[Iter 790][Class loss: 0.000238211796386]\n",
      "[Iter 791][Class loss: 0.000236467953073]\n",
      "[Iter 792][Class loss: 0.000235032406636]\n",
      "[Iter 793][Class loss: 0.000233465339988]\n",
      "[Iter 794][Class loss: 0.000232059857808]\n",
      "[Iter 795][Class loss: 0.000230425590416]\n",
      "[Iter 796][Class loss: 0.000228807490203]\n",
      "[Iter 797][Class loss: 0.000227407188504]\n",
      "[Iter 798][Class loss: 0.000225995099754]\n",
      "[Iter 799][Class loss: 0.000224536895985]\n",
      "[Iter 800][Class loss: 0.000223316485062]\n",
      "[Iter 801][Class loss: 0.000221664507990]\n",
      "[Iter 802][Class loss: 0.000220575282583]\n",
      "[Iter 803][Class loss: 0.000218980043428]\n",
      "[Iter 804][Class loss: 0.000217749329749]\n",
      "[Iter 805][Class loss: 0.000216255473788]\n",
      "[Iter 806][Class loss: 0.000214899337152]\n",
      "[Iter 807][Class loss: 0.000213670398807]\n",
      "[Iter 808][Class loss: 0.000212354658288]\n",
      "[Iter 809][Class loss: 0.000211110454984]\n",
      "[Iter 810][Class loss: 0.000209939273191]\n",
      "[Iter 811][Class loss: 0.000208877478144]\n",
      "[Iter 812][Class loss: 0.000207588615012]\n",
      "[Iter 813][Class loss: 0.000206350756343]\n",
      "[Iter 814][Class loss: 0.000205497955903]\n",
      "[Iter 815][Class loss: 0.000204004187253]\n",
      "[Iter 816][Class loss: 0.000202885072213]\n",
      "[Iter 817][Class loss: 0.000201836388442]\n",
      "[Iter 818][Class loss: 0.000200686306925]\n",
      "[Iter 819][Class loss: 0.000199597881874]\n",
      "[Iter 820][Class loss: 0.000198492634809]\n",
      "[Iter 821][Class loss: 0.000197579473024]\n",
      "[Iter 822][Class loss: 0.000196285778657]\n",
      "[Iter 823][Class loss: 0.000195224682102]\n",
      "[Iter 824][Class loss: 0.000194133855985]\n",
      "[Iter 825][Class loss: 0.000193263913388]\n",
      "[Iter 826][Class loss: 0.000192281237105]\n",
      "[Iter 827][Class loss: 0.000191238214029]\n",
      "[Iter 828][Class loss: 0.000190346312593]\n",
      "[Iter 829][Class loss: 0.000189147453057]\n",
      "[Iter 830][Class loss: 0.000188289312064]\n",
      "[Iter 831][Class loss: 0.000187193334568]\n",
      "[Iter 832][Class loss: 0.000186255754670]\n",
      "[Iter 833][Class loss: 0.000185310593224]\n",
      "[Iter 834][Class loss: 0.000184446951607]\n",
      "[Iter 835][Class loss: 0.000183534895768]\n",
      "[Iter 836][Class loss: 0.000182585950824]\n",
      "[Iter 837][Class loss: 0.000181713738129]\n",
      "[Iter 838][Class loss: 0.000180639035534]\n",
      "[Iter 839][Class loss: 0.000179955648491]\n",
      "[Iter 840][Class loss: 0.000179016686161]\n",
      "[Iter 841][Class loss: 0.000178103189683]\n",
      "[Iter 842][Class loss: 0.000177212976268]\n",
      "[Iter 843][Class loss: 0.000176409288542]\n",
      "[Iter 844][Class loss: 0.000175500055775]\n",
      "[Iter 845][Class loss: 0.000174579123268]\n",
      "[Iter 846][Class loss: 0.000173860811628]\n",
      "[Iter 847][Class loss: 0.000173030042788]\n",
      "[Iter 848][Class loss: 0.000172169224243]\n",
      "[Iter 849][Class loss: 0.000171273975866]\n",
      "[Iter 850][Class loss: 0.000170461978996]\n",
      "[Iter 851][Class loss: 0.000169671897311]\n",
      "[Iter 852][Class loss: 0.000168923812453]\n",
      "[Iter 853][Class loss: 0.000168121419847]\n",
      "[Iter 854][Class loss: 0.000167321122717]\n",
      "[Iter 855][Class loss: 0.000166674726643]\n",
      "[Iter 856][Class loss: 0.000165798061062]\n",
      "[Iter 857][Class loss: 0.000165071862284]\n",
      "[Iter 858][Class loss: 0.000164493860211]\n",
      "[Iter 859][Class loss: 0.000163766759215]\n",
      "[Iter 860][Class loss: 0.000163035074365]\n",
      "[Iter 861][Class loss: 0.000162289608852]\n",
      "[Iter 862][Class loss: 0.000161515868967]\n",
      "[Iter 863][Class loss: 0.000160977462656]\n",
      "[Iter 864][Class loss: 0.000160402531037]\n",
      "[Iter 865][Class loss: 0.000159525923664]\n",
      "[Iter 866][Class loss: 0.000158853275934]\n",
      "[Iter 867][Class loss: 0.000158165406901]\n",
      "[Iter 868][Class loss: 0.000157301808940]\n",
      "[Iter 869][Class loss: 0.000156757872901]\n",
      "[Iter 870][Class loss: 0.000156031557708]\n",
      "[Iter 871][Class loss: 0.000155273533892]\n",
      "[Iter 872][Class loss: 0.000154675683007]\n",
      "[Iter 873][Class loss: 0.000153938948642]\n",
      "[Iter 874][Class loss: 0.000153470798978]\n",
      "[Iter 875][Class loss: 0.000152746739332]\n",
      "[Iter 876][Class loss: 0.000152071865159]\n",
      "[Iter 877][Class loss: 0.000151438231114]\n",
      "[Iter 878][Class loss: 0.000150799402036]\n",
      "[Iter 879][Class loss: 0.000150129475514]\n",
      "[Iter 880][Class loss: 0.000149537867401]\n",
      "[Iter 881][Class loss: 0.000148910563439]\n",
      "[Iter 882][Class loss: 0.000148256862303]\n",
      "[Iter 883][Class loss: 0.000147687882418]\n",
      "[Iter 884][Class loss: 0.000147120939801]\n",
      "[Iter 885][Class loss: 0.000146515885717]\n",
      "[Iter 886][Class loss: 0.000145924408571]\n",
      "[Iter 887][Class loss: 0.000145377038280]\n",
      "[Iter 888][Class loss: 0.000144767967868]\n",
      "[Iter 889][Class loss: 0.000144158853800]\n",
      "[Iter 890][Class loss: 0.000143734738231]\n",
      "[Iter 891][Class loss: 0.000143089695484]\n",
      "[Iter 892][Class loss: 0.000142593838973]\n",
      "[Iter 893][Class loss: 0.000142045377288]\n",
      "[Iter 894][Class loss: 0.000141423486639]\n",
      "[Iter 895][Class loss: 0.000140899530379]\n",
      "[Iter 896][Class loss: 0.000140272371937]\n",
      "[Iter 897][Class loss: 0.000139804062201]\n",
      "[Iter 898][Class loss: 0.000139326410135]\n",
      "[Iter 899][Class loss: 0.000138737232191]\n",
      "[Iter 900][Class loss: 0.000138151837746]\n",
      "[Iter 901][Class loss: 0.000137633032864]\n",
      "[Iter 902][Class loss: 0.000137153023388]\n",
      "[Iter 903][Class loss: 0.000136643924634]\n",
      "[Iter 904][Class loss: 0.000136085553095]\n",
      "[Iter 905][Class loss: 0.000135585563839]\n",
      "[Iter 906][Class loss: 0.000135101610795]\n",
      "[Iter 907][Class loss: 0.000134640387842]\n",
      "[Iter 908][Class loss: 0.000134092144435]\n",
      "[Iter 909][Class loss: 0.000133683308377]\n",
      "[Iter 910][Class loss: 0.000133107940201]\n",
      "[Iter 911][Class loss: 0.000132629633299]\n",
      "[Iter 912][Class loss: 0.000132135945023]\n",
      "[Iter 913][Class loss: 0.000131639244501]\n",
      "[Iter 914][Class loss: 0.000131163396873]\n",
      "[Iter 915][Class loss: 0.000130735439598]\n",
      "[Iter 916][Class loss: 0.000130195956444]\n",
      "[Iter 917][Class loss: 0.000129746913444]\n",
      "[Iter 918][Class loss: 0.000129284046125]\n",
      "[Iter 919][Class loss: 0.000128803032567]\n",
      "[Iter 920][Class loss: 0.000128362298710]\n",
      "[Iter 921][Class loss: 0.000127968261950]\n",
      "[Iter 922][Class loss: 0.000127517007058]\n",
      "[Iter 923][Class loss: 0.000127076375065]\n",
      "[Iter 924][Class loss: 0.000126577142510]\n",
      "[Iter 925][Class loss: 0.000126246653963]\n",
      "[Iter 926][Class loss: 0.000125689883134]\n",
      "[Iter 927][Class loss: 0.000125263439259]\n",
      "[Iter 928][Class loss: 0.000124898404465]\n",
      "[Iter 929][Class loss: 0.000124434125610]\n",
      "[Iter 930][Class loss: 0.000124061756651]\n",
      "[Iter 931][Class loss: 0.000123637379147]\n",
      "[Iter 932][Class loss: 0.000123167323181]\n",
      "[Iter 933][Class loss: 0.000122806901345]\n",
      "[Iter 934][Class loss: 0.000122312470921]\n",
      "[Iter 935][Class loss: 0.000121926692373]\n",
      "[Iter 936][Class loss: 0.000121573495562]\n",
      "[Iter 937][Class loss: 0.000121112941997]\n",
      "[Iter 938][Class loss: 0.000120742188301]\n",
      "[Iter 939][Class loss: 0.000120236203657]\n",
      "[Iter 940][Class loss: 0.000120003533084]\n",
      "[Iter 941][Class loss: 0.000119495351100]\n",
      "[Iter 942][Class loss: 0.000119114192785]\n",
      "[Iter 943][Class loss: 0.000118704992929]\n",
      "[Iter 944][Class loss: 0.000118303782074]\n",
      "[Iter 945][Class loss: 0.000117919029435]\n",
      "[Iter 946][Class loss: 0.000117539922940]\n",
      "[Iter 947][Class loss: 0.000117161063827]\n",
      "[Iter 948][Class loss: 0.000116760667879]\n",
      "[Iter 949][Class loss: 0.000116385286674]\n",
      "[Iter 950][Class loss: 0.000116050847282]\n",
      "[Iter 951][Class loss: 0.000115694252599]\n",
      "[Iter 952][Class loss: 0.000115282367915]\n",
      "[Iter 953][Class loss: 0.000115038084914]\n",
      "[Iter 954][Class loss: 0.000114548165584]\n",
      "[Iter 955][Class loss: 0.000114339927677]\n",
      "[Iter 956][Class loss: 0.000114042100904]\n",
      "[Iter 957][Class loss: 0.000113592483103]\n",
      "[Iter 958][Class loss: 0.000113131398393]\n",
      "[Iter 959][Class loss: 0.000113031528599]\n",
      "[Iter 960][Class loss: 0.000112575085950]\n",
      "[Iter 961][Class loss: 0.000112156361865]\n",
      "[Iter 962][Class loss: 0.000111919478513]\n",
      "[Iter 963][Class loss: 0.000111518384074]\n",
      "[Iter 964][Class loss: 0.000111087312689]\n",
      "[Iter 965][Class loss: 0.000110784669232]\n",
      "[Iter 966][Class loss: 0.000110419801786]\n",
      "[Iter 967][Class loss: 0.000110094188130]\n",
      "[Iter 968][Class loss: 0.000109779779450]\n",
      "[Iter 969][Class loss: 0.000109421511297]\n",
      "[Iter 970][Class loss: 0.000109057007649]\n",
      "[Iter 971][Class loss: 0.000108737134724]\n",
      "[Iter 972][Class loss: 0.000108445354272]\n",
      "[Iter 973][Class loss: 0.000108148204163]\n",
      "[Iter 974][Class loss: 0.000107743107947]\n",
      "[Iter 975][Class loss: 0.000107432140794]\n",
      "[Iter 976][Class loss: 0.000107148072857]\n",
      "[Iter 977][Class loss: 0.000106821076770]\n",
      "[Iter 978][Class loss: 0.000106481653347]\n",
      "[Iter 979][Class loss: 0.000106147825136]\n",
      "[Iter 980][Class loss: 0.000105830207758]\n",
      "[Iter 981][Class loss: 0.000105536397314]\n",
      "[Iter 982][Class loss: 0.000105281913420]\n",
      "[Iter 983][Class loss: 0.000104968297819]\n",
      "[Iter 984][Class loss: 0.000104660539364]\n",
      "[Iter 985][Class loss: 0.000104365091829]\n",
      "[Iter 986][Class loss: 0.000104090562672]\n",
      "[Iter 987][Class loss: 0.000103824597318]\n",
      "[Iter 988][Class loss: 0.000103449616290]\n",
      "[Iter 989][Class loss: 0.000103262733319]\n",
      "[Iter 990][Class loss: 0.000102920843347]\n",
      "[Iter 991][Class loss: 0.000102629681351]\n",
      "[Iter 992][Class loss: 0.000102390302345]\n",
      "[Iter 993][Class loss: 0.000102061327198]\n",
      "[Iter 994][Class loss: 0.000101753073977]\n",
      "[Iter 995][Class loss: 0.000101512618130]\n",
      "[Iter 996][Class loss: 0.000101175370219]\n",
      "[Iter 997][Class loss: 0.000100890632893]\n",
      "[Iter 998][Class loss: 0.000100626479252]\n",
      "[Iter 999][Class loss: 0.000100355850009]\n",
      "[Iter 1000][Class loss: 0.000100061050034]\n",
      "[Iter 1001][Class loss: 0.000099769240478]\n",
      "[Iter 1002][Class loss: 0.000099494696769]\n",
      "[Iter 1003][Class loss: 0.000099237673567]\n",
      "[Iter 1004][Class loss: 0.000098989592516]\n",
      "[Iter 1005][Class loss: 0.000098737211374]\n",
      "[Iter 1006][Class loss: 0.000098430755315]\n",
      "[Iter 1007][Class loss: 0.000098242162494]\n",
      "[Iter 1008][Class loss: 0.000097888027085]\n",
      "[Iter 1009][Class loss: 0.000097685187939]\n",
      "[Iter 1010][Class loss: 0.000097406613349]\n",
      "[Iter 1011][Class loss: 0.000097137395642]\n",
      "[Iter 1012][Class loss: 0.000096856019809]\n",
      "[Iter 1013][Class loss: 0.000096627511084]\n",
      "[Iter 1014][Class loss: 0.000096354859124]\n",
      "[Iter 1015][Class loss: 0.000096090472653]\n",
      "[Iter 1016][Class loss: 0.000095844021416]\n",
      "[Iter 1017][Class loss: 0.000095611103461]\n",
      "[Iter 1018][Class loss: 0.000095343923022]\n",
      "[Iter 1019][Class loss: 0.000095082548796]\n",
      "[Iter 1020][Class loss: 0.000094837945653]\n",
      "[Iter 1021][Class loss: 0.000094605893537]\n",
      "[Iter 1022][Class loss: 0.000094353716122]\n",
      "[Iter 1023][Class loss: 0.000094151619123]\n",
      "[Iter 1024][Class loss: 0.000093872353318]\n",
      "[Iter 1025][Class loss: 0.000093696602562]\n",
      "[Iter 1026][Class loss: 0.000093474838650]\n",
      "[Iter 1027][Class loss: 0.000093217247922]\n",
      "[Iter 1028][Class loss: 0.000092969188699]\n",
      "[Iter 1029][Class loss: 0.000092706373835]\n",
      "[Iter 1030][Class loss: 0.000092486239737]\n",
      "[Iter 1031][Class loss: 0.000092254500487]\n",
      "[Iter 1032][Class loss: 0.000092059402959]\n",
      "[Iter 1033][Class loss: 0.000091780151706]\n",
      "[Iter 1034][Class loss: 0.000091646026704]\n",
      "[Iter 1035][Class loss: 0.000091358393547]\n",
      "[Iter 1036][Class loss: 0.000091196241556]\n",
      "[Iter 1037][Class loss: 0.000091034497018]\n",
      "[Iter 1038][Class loss: 0.000090773159172]\n",
      "[Iter 1039][Class loss: 0.000090487214038]\n",
      "[Iter 1040][Class loss: 0.000090282555902]\n",
      "[Iter 1041][Class loss: 0.000090027213446]\n",
      "[Iter 1042][Class loss: 0.000089829918579]\n",
      "[Iter 1043][Class loss: 0.000089613109594]\n",
      "[Iter 1044][Class loss: 0.000089368666522]\n",
      "[Iter 1045][Class loss: 0.000089137771283]\n",
      "[Iter 1046][Class loss: 0.000088916938694]\n",
      "[Iter 1047][Class loss: 0.000088695989689]\n",
      "[Iter 1048][Class loss: 0.000088504821179]\n",
      "[Iter 1049][Class loss: 0.000088273081928]\n",
      "[Iter 1050][Class loss: 0.000088074899395]\n",
      "[Iter 1051][Class loss: 0.000087882799562]\n",
      "[Iter 1052][Class loss: 0.000087630010967]\n",
      "[Iter 1053][Class loss: 0.000087510306912]\n",
      "[Iter 1054][Class loss: 0.000087235428509]\n",
      "[Iter 1055][Class loss: 0.000087110354798]\n",
      "[Iter 1056][Class loss: 0.000086956424639]\n",
      "[Iter 1057][Class loss: 0.000086711545009]\n",
      "[Iter 1058][Class loss: 0.000086434694822]\n",
      "[Iter 1059][Class loss: 0.000086359395937]\n",
      "[Iter 1060][Class loss: 0.000086149797426]\n",
      "[Iter 1061][Class loss: 0.000085804640548]\n",
      "[Iter 1062][Class loss: 0.000085676503659]\n",
      "[Iter 1063][Class loss: 0.000085494270024]\n",
      "[Iter 1064][Class loss: 0.000085230327386]\n",
      "[Iter 1065][Class loss: 0.000085070030764]\n",
      "[Iter 1066][Class loss: 0.000084841056378]\n",
      "[Iter 1067][Class loss: 0.000084685969341]\n",
      "[Iter 1068][Class loss: 0.000084515137132]\n",
      "[Iter 1069][Class loss: 0.000084296174464]\n",
      "[Iter 1070][Class loss: 0.000084088962467]\n",
      "[Iter 1071][Class loss: 0.000083904902567]\n",
      "[Iter 1072][Class loss: 0.000083699131210]\n",
      "[Iter 1073][Class loss: 0.000083539518528]\n",
      "[Iter 1074][Class loss: 0.000083371742221]\n",
      "[Iter 1075][Class loss: 0.000083157123299]\n",
      "[Iter 1076][Class loss: 0.000082980965090]\n",
      "[Iter 1077][Class loss: 0.000082759273937]\n",
      "[Iter 1078][Class loss: 0.000082598533481]\n",
      "[Iter 1079][Class loss: 0.000082430953626]\n",
      "[Iter 1080][Class loss: 0.000082254220615]\n",
      "[Iter 1081][Class loss: 0.000082053906226]\n",
      "[Iter 1082][Class loss: 0.000081861129729]\n",
      "[Iter 1083][Class loss: 0.000081694197434]\n",
      "[Iter 1084][Class loss: 0.000081497477368]\n",
      "[Iter 1085][Class loss: 0.000081320860772]\n",
      "[Iter 1086][Class loss: 0.000081147314631]\n",
      "[Iter 1087][Class loss: 0.000080975216406]\n",
      "[Iter 1088][Class loss: 0.000080804391473]\n",
      "[Iter 1089][Class loss: 0.000080609141150]\n",
      "[Iter 1090][Class loss: 0.000080439313024]\n",
      "[Iter 1091][Class loss: 0.000080274883658]\n",
      "[Iter 1092][Class loss: 0.000080111756688]\n",
      "[Iter 1093][Class loss: 0.000079962032032]\n",
      "[Iter 1094][Class loss: 0.000079780584201]\n",
      "[Iter 1095][Class loss: 0.000079633318819]\n",
      "[Iter 1096][Class loss: 0.000079451914644]\n",
      "[Iter 1097][Class loss: 0.000079273719166]\n",
      "[Iter 1098][Class loss: 0.000079109755461]\n",
      "[Iter 1099][Class loss: 0.000078934986959]\n",
      "[Iter 1100][Class loss: 0.000078776152804]\n",
      "[Iter 1101][Class loss: 0.000078624027083]\n",
      "[Iter 1102][Class loss: 0.000078471537563]\n",
      "[Iter 1103][Class loss: 0.000078272874816]\n",
      "[Iter 1104][Class loss: 0.000078130600741]\n",
      "[Iter 1105][Class loss: 0.000077952856373]\n",
      "[Iter 1106][Class loss: 0.000077791650256]\n",
      "[Iter 1107][Class loss: 0.000077622724348]\n",
      "[Iter 1108][Class loss: 0.000077462158515]\n",
      "[Iter 1109][Class loss: 0.000077330696513]\n",
      "[Iter 1110][Class loss: 0.000077155724284]\n",
      "[Iter 1111][Class loss: 0.000077021308243]\n",
      "[Iter 1112][Class loss: 0.000076850752521]\n",
      "[Iter 1113][Class loss: 0.000076706804975]\n",
      "[Iter 1114][Class loss: 0.000076536700362]\n",
      "[Iter 1115][Class loss: 0.000076367752627]\n",
      "[Iter 1116][Class loss: 0.000076231583080]\n",
      "[Iter 1117][Class loss: 0.000076067721238]\n",
      "[Iter 1118][Class loss: 0.000075900243246]\n",
      "[Iter 1119][Class loss: 0.000075780364568]\n",
      "[Iter 1120][Class loss: 0.000075612791989]\n",
      "[Iter 1121][Class loss: 0.000075462121458]\n",
      "[Iter 1122][Class loss: 0.000075303454651]\n",
      "[Iter 1123][Class loss: 0.000075146585004]\n",
      "[Iter 1124][Class loss: 0.000075049676525]\n",
      "[Iter 1125][Class loss: 0.000074886665971]\n",
      "[Iter 1126][Class loss: 0.000074712770584]\n",
      "[Iter 1127][Class loss: 0.000074579336797]\n",
      "[Iter 1128][Class loss: 0.000074425290222]\n",
      "[Iter 1129][Class loss: 0.000074277937529]\n",
      "[Iter 1130][Class loss: 0.000074137737101]\n",
      "[Iter 1131][Class loss: 0.000073986710049]\n",
      "[Iter 1132][Class loss: 0.000073837480159]\n",
      "[Iter 1133][Class loss: 0.000073695395258]\n",
      "[Iter 1134][Class loss: 0.000073537237768]\n",
      "[Iter 1135][Class loss: 0.000073435243394]\n",
      "[Iter 1136][Class loss: 0.000073255228926]\n",
      "[Iter 1137][Class loss: 0.000073117342254]\n",
      "[Iter 1138][Class loss: 0.000072989045293]\n",
      "[Iter 1139][Class loss: 0.000072835202445]\n",
      "[Iter 1140][Class loss: 0.000072733426350]\n",
      "[Iter 1141][Class loss: 0.000072587448813]\n",
      "[Iter 1142][Class loss: 0.000072443333920]\n",
      "[Iter 1143][Class loss: 0.000072305687354]\n",
      "[Iter 1144][Class loss: 0.000072151633503]\n",
      "[Iter 1145][Class loss: 0.000072034810728]\n",
      "[Iter 1146][Class loss: 0.000071891299740]\n",
      "[Iter 1147][Class loss: 0.000071792441304]\n",
      "[Iter 1148][Class loss: 0.000071631955507]\n",
      "[Iter 1149][Class loss: 0.000071487585956]\n",
      "[Iter 1150][Class loss: 0.000071368769568]\n",
      "[Iter 1151][Class loss: 0.000071240472607]\n",
      "[Iter 1152][Class loss: 0.000071091955760]\n",
      "[Iter 1153][Class loss: 0.000070948910434]\n",
      "[Iter 1154][Class loss: 0.000070818292443]\n",
      "[Iter 1155][Class loss: 0.000070688081905]\n",
      "[Iter 1156][Class loss: 0.000070555448474]\n",
      "[Iter 1157][Class loss: 0.000070422509452]\n",
      "[Iter 1158][Class loss: 0.000070281021181]\n",
      "[Iter 1159][Class loss: 0.000070149733801]\n",
      "[Iter 1160][Class loss: 0.000070019486884]\n",
      "[Iter 1161][Class loss: 0.000069884525146]\n",
      "[Iter 1162][Class loss: 0.000069761052146]\n",
      "[Iter 1163][Class loss: 0.000069634748797]\n",
      "[Iter 1164][Class loss: 0.000069537622039]\n",
      "[Iter 1165][Class loss: 0.000069389163400]\n",
      "[Iter 1166][Class loss: 0.000069263689511]\n",
      "[Iter 1167][Class loss: 0.000069145076850]\n",
      "[Iter 1168][Class loss: 0.000069010668085]\n",
      "[Iter 1169][Class loss: 0.000068885616201]\n",
      "[Iter 1170][Class loss: 0.000068760142312]\n",
      "[Iter 1171][Class loss: 0.000068643530540]\n",
      "[Iter 1172][Class loss: 0.000068513734732]\n",
      "[Iter 1173][Class loss: 0.000068398338044]\n",
      "[Iter 1174][Class loss: 0.000068277804530]\n",
      "[Iter 1175][Class loss: 0.000068163732067]\n",
      "[Iter 1176][Class loss: 0.000068037086749]\n",
      "[Iter 1177][Class loss: 0.000067908011260]\n",
      "[Iter 1178][Class loss: 0.000067798362579]\n",
      "[Iter 1179][Class loss: 0.000067697175837]\n",
      "[Iter 1180][Class loss: 0.000067557019065]\n",
      "[Iter 1181][Class loss: 0.000067429340561]\n",
      "[Iter 1182][Class loss: 0.000067322049290]\n",
      "[Iter 1183][Class loss: 0.000067210479756]\n",
      "[Iter 1184][Class loss: 0.000067087872594]\n",
      "[Iter 1185][Class loss: 0.000066959655669]\n",
      "[Iter 1186][Class loss: 0.000066850436269]\n",
      "[Iter 1187][Class loss: 0.000066744556534]\n",
      "[Iter 1188][Class loss: 0.000066620443249]\n",
      "[Iter 1189][Class loss: 0.000066508771852]\n",
      "[Iter 1190][Class loss: 0.000066378583142]\n",
      "[Iter 1191][Class loss: 0.000066270215029]\n",
      "[Iter 1192][Class loss: 0.000066153690568]\n",
      "[Iter 1193][Class loss: 0.000066039188823]\n",
      "[Iter 1194][Class loss: 0.000065938031184]\n",
      "[Iter 1195][Class loss: 0.000065827298386]\n",
      "[Iter 1196][Class loss: 0.000065715168603]\n",
      "[Iter 1197][Class loss: 0.000065602449467]\n",
      "[Iter 1198][Class loss: 0.000065481523052]\n",
      "[Iter 1199][Class loss: 0.000065379834268]\n",
      "[Iter 1200][Class loss: 0.000065269698098]\n",
      "[Iter 1201][Class loss: 0.000065162777901]\n",
      "[Iter 1202][Class loss: 0.000065044099756]\n",
      "[Iter 1203][Class loss: 0.000064938503783]\n",
      "[Iter 1204][Class loss: 0.000064822685090]\n",
      "[Iter 1205][Class loss: 0.000064710395236]\n",
      "[Iter 1206][Class loss: 0.000064602863858]\n",
      "[Iter 1207][Class loss: 0.000064517924329]\n",
      "[Iter 1208][Class loss: 0.000064389751060]\n",
      "[Iter 1209][Class loss: 0.000064319254307]\n",
      "[Iter 1210][Class loss: 0.000064208448748]\n",
      "[Iter 1211][Class loss: 0.000064108891820]\n",
      "[Iter 1212][Class loss: 0.000064002917497]\n",
      "[Iter 1213][Class loss: 0.000063873434556]\n",
      "[Iter 1214][Class loss: 0.000063776649768]\n",
      "[Iter 1215][Class loss: 0.000063699844759]\n",
      "[Iter 1216][Class loss: 0.000063591789512]\n",
      "[Iter 1217][Class loss: 0.000063489787863]\n",
      "[Iter 1218][Class loss: 0.000063357198087]\n",
      "[Iter 1219][Class loss: 0.000063239524025]\n",
      "[Iter 1220][Class loss: 0.000063158629928]\n",
      "[Iter 1221][Class loss: 0.000063064129790]\n",
      "[Iter 1222][Class loss: 0.000062942650402]\n",
      "[Iter 1223][Class loss: 0.000062821083702]\n",
      "[Iter 1224][Class loss: 0.000062737461121]\n",
      "[Iter 1225][Class loss: 0.000062627543230]\n",
      "[Iter 1226][Class loss: 0.000062533348682]\n",
      "[Iter 1227][Class loss: 0.000062415740103]\n",
      "[Iter 1228][Class loss: 0.000062318962591]\n",
      "[Iter 1229][Class loss: 0.000062218408857]\n",
      "[Iter 1230][Class loss: 0.000062112834712]\n",
      "[Iter 1231][Class loss: 0.000062006365624]\n",
      "[Iter 1232][Class loss: 0.000061914681282]\n",
      "[Iter 1233][Class loss: 0.000061823477154]\n",
      "[Iter 1234][Class loss: 0.000061715632910]\n",
      "[Iter 1235][Class loss: 0.000061632716097]\n",
      "[Iter 1236][Class loss: 0.000061533850385]\n",
      "[Iter 1237][Class loss: 0.000061419923441]\n",
      "[Iter 1238][Class loss: 0.000061322942202]\n",
      "[Iter 1239][Class loss: 0.000061234692112]\n",
      "[Iter 1240][Class loss: 0.000061134400312]\n",
      "[Iter 1241][Class loss: 0.000061040525907]\n",
      "[Iter 1242][Class loss: 0.000060941678385]\n",
      "[Iter 1243][Class loss: 0.000060845221014]\n",
      "[Iter 1244][Class loss: 0.000060742491769]\n",
      "[Iter 1245][Class loss: 0.000060657101130]\n",
      "[Iter 1246][Class loss: 0.000060571263020]\n",
      "[Iter 1247][Class loss: 0.000060463753471]\n",
      "[Iter 1248][Class loss: 0.000060386548284]\n",
      "[Iter 1249][Class loss: 0.000060295482399]\n",
      "[Iter 1250][Class loss: 0.000060196063714]\n",
      "[Iter 1251][Class loss: 0.000060100741393]\n",
      "[Iter 1252][Class loss: 0.000060002184910]\n",
      "[Iter 1253][Class loss: 0.000059904206864]\n",
      "[Iter 1254][Class loss: 0.000059844354837]\n",
      "[Iter 1255][Class loss: 0.000059728514316]\n",
      "[Iter 1256][Class loss: 0.000059656369558]\n",
      "[Iter 1257][Class loss: 0.000059569105360]\n",
      "[Iter 1258][Class loss: 0.000059469559346]\n",
      "[Iter 1259][Class loss: 0.000059373018303]\n",
      "[Iter 1260][Class loss: 0.000059286481701]\n",
      "[Iter 1261][Class loss: 0.000059195677750]\n",
      "[Iter 1262][Class loss: 0.000059114856413]\n",
      "[Iter 1263][Class loss: 0.000059007372329]\n",
      "[Iter 1264][Class loss: 0.000058952187828]\n",
      "[Iter 1265][Class loss: 0.000058838799305]\n",
      "[Iter 1266][Class loss: 0.000058761463151]\n",
      "[Iter 1267][Class loss: 0.000058691497543]\n",
      "[Iter 1268][Class loss: 0.000058603243815]\n",
      "[Iter 1269][Class loss: 0.000058505043853]\n",
      "[Iter 1270][Class loss: 0.000058396813984]\n",
      "[Iter 1271][Class loss: 0.000058296373027]\n",
      "[Iter 1272][Class loss: 0.000058240035287]\n",
      "[Iter 1273][Class loss: 0.000058144942159]\n",
      "[Iter 1274][Class loss: 0.000058062512835]\n",
      "[Iter 1275][Class loss: 0.000057984470914]\n",
      "[Iter 1276][Class loss: 0.000057876281062]\n",
      "[Iter 1277][Class loss: 0.000057792680309]\n",
      "[Iter 1278][Class loss: 0.000057709090470]\n",
      "[Iter 1279][Class loss: 0.000057622430177]\n",
      "[Iter 1280][Class loss: 0.000057545141317]\n",
      "[Iter 1281][Class loss: 0.000057441509853]\n",
      "[Iter 1282][Class loss: 0.000057361194195]\n",
      "[Iter 1283][Class loss: 0.000057279565226]\n",
      "[Iter 1284][Class loss: 0.000057193581597]\n",
      "[Iter 1285][Class loss: 0.000057128279877]\n",
      "[Iter 1286][Class loss: 0.000057024833950]\n",
      "[Iter 1287][Class loss: 0.000056945733377]\n",
      "[Iter 1288][Class loss: 0.000056864846556]\n",
      "[Iter 1289][Class loss: 0.000056776305428]\n",
      "[Iter 1290][Class loss: 0.000056699944253]\n",
      "[Iter 1291][Class loss: 0.000056619443058]\n",
      "[Iter 1292][Class loss: 0.000056544638937]\n",
      "[Iter 1293][Class loss: 0.000056464021327]\n",
      "[Iter 1294][Class loss: 0.000056372511608]\n",
      "[Iter 1295][Class loss: 0.000056307995692]\n",
      "[Iter 1296][Class loss: 0.000056206983572]\n",
      "[Iter 1297][Class loss: 0.000056133099861]\n",
      "[Iter 1298][Class loss: 0.000056044333178]\n",
      "[Iter 1299][Class loss: 0.000055979784520]\n",
      "[Iter 1300][Class loss: 0.000055890333897]\n",
      "[Iter 1301][Class loss: 0.000055809141486]\n",
      "[Iter 1302][Class loss: 0.000055741664255]\n",
      "[Iter 1303][Class loss: 0.000055654250900]\n",
      "[Iter 1304][Class loss: 0.000055587126553]\n",
      "[Iter 1305][Class loss: 0.000055492950196]\n",
      "[Iter 1306][Class loss: 0.000055431482906]\n",
      "[Iter 1307][Class loss: 0.000055345426517]\n",
      "[Iter 1308][Class loss: 0.000055272772443]\n",
      "[Iter 1309][Class loss: 0.000055195865571]\n",
      "[Iter 1310][Class loss: 0.000055114946008]\n",
      "[Iter 1311][Class loss: 0.000055045416957]\n",
      "[Iter 1312][Class loss: 0.000054958076362]\n",
      "[Iter 1313][Class loss: 0.000054902484408]\n",
      "[Iter 1314][Class loss: 0.000054812837334]\n",
      "[Iter 1315][Class loss: 0.000054741700296]\n",
      "[Iter 1316][Class loss: 0.000054660311434]\n",
      "[Iter 1317][Class loss: 0.000054590826039]\n",
      "[Iter 1318][Class loss: 0.000054505046137]\n",
      "[Iter 1319][Class loss: 0.000054428710428]\n",
      "[Iter 1320][Class loss: 0.000054364849348]\n",
      "[Iter 1321][Class loss: 0.000054288575484]\n",
      "[Iter 1322][Class loss: 0.000054214666307]\n",
      "[Iter 1323][Class loss: 0.000054146919865]\n",
      "[Iter 1324][Class loss: 0.000054058949900]\n",
      "[Iter 1325][Class loss: 0.000053988318541]\n",
      "[Iter 1326][Class loss: 0.000053916952311]\n",
      "[Iter 1327][Class loss: 0.000053839779866]\n",
      "[Iter 1328][Class loss: 0.000053770017985]\n",
      "[Iter 1329][Class loss: 0.000053693074733]\n",
      "[Iter 1330][Class loss: 0.000053629370086]\n",
      "[Iter 1331][Class loss: 0.000053553198086]\n",
      "[Iter 1332][Class loss: 0.000053494517488]\n",
      "[Iter 1333][Class loss: 0.000053406234656]\n",
      "[Iter 1334][Class loss: 0.000053335650591]\n",
      "[Iter 1335][Class loss: 0.000053261261201]\n",
      "[Iter 1336][Class loss: 0.000053188115999]\n",
      "[Iter 1337][Class loss: 0.000053122272220]\n",
      "[Iter 1338][Class loss: 0.000053059058700]\n",
      "[Iter 1339][Class loss: 0.000052985211369]\n",
      "[Iter 1340][Class loss: 0.000052921142924]\n",
      "[Iter 1341][Class loss: 0.000052841689467]\n",
      "[Iter 1342][Class loss: 0.000052789877373]\n",
      "[Iter 1343][Class loss: 0.000052719849919]\n",
      "[Iter 1344][Class loss: 0.000052637566114]\n",
      "[Iter 1345][Class loss: 0.000052566072554]\n",
      "[Iter 1346][Class loss: 0.000052501160098]\n",
      "[Iter 1347][Class loss: 0.000052428858908]\n",
      "[Iter 1348][Class loss: 0.000052362942370]\n",
      "[Iter 1349][Class loss: 0.000052294766647]\n",
      "[Iter 1350][Class loss: 0.000052231400332]\n",
      "[Iter 1351][Class loss: 0.000052159513871]\n",
      "[Iter 1352][Class loss: 0.000052096278523]\n",
      "[Iter 1353][Class loss: 0.000052025374316]\n",
      "[Iter 1354][Class loss: 0.000051956179959]\n",
      "[Iter 1355][Class loss: 0.000051887105656]\n",
      "[Iter 1356][Class loss: 0.000051820439694]\n",
      "[Iter 1357][Class loss: 0.000051750204875]\n",
      "[Iter 1358][Class loss: 0.000051687115047]\n",
      "[Iter 1359][Class loss: 0.000051629409427]\n",
      "[Iter 1360][Class loss: 0.000051563434681]\n",
      "[Iter 1361][Class loss: 0.000051487037126]\n",
      "[Iter 1362][Class loss: 0.000051425380661]\n",
      "[Iter 1363][Class loss: 0.000051360675570]\n",
      "[Iter 1364][Class loss: 0.000051289200201]\n",
      "[Iter 1365][Class loss: 0.000051226725191]\n",
      "[Iter 1366][Class loss: 0.000051172639360]\n",
      "[Iter 1367][Class loss: 0.000051098471886]\n",
      "[Iter 1368][Class loss: 0.000051054761570]\n",
      "[Iter 1369][Class loss: 0.000050962891692]\n",
      "[Iter 1370][Class loss: 0.000050902621297]\n",
      "[Iter 1371][Class loss: 0.000050845927035]\n",
      "[Iter 1372][Class loss: 0.000050772796385]\n",
      "[Iter 1373][Class loss: 0.000050719743740]\n",
      "[Iter 1374][Class loss: 0.000050657068641]\n",
      "[Iter 1375][Class loss: 0.000050586419093]\n",
      "[Iter 1376][Class loss: 0.000050527578423]\n",
      "[Iter 1377][Class loss: 0.000050463466323]\n",
      "[Iter 1378][Class loss: 0.000050401988119]\n",
      "[Iter 1379][Class loss: 0.000050332895626]\n",
      "[Iter 1380][Class loss: 0.000050279486459]\n",
      "[Iter 1381][Class loss: 0.000050215061492]\n",
      "[Iter 1382][Class loss: 0.000050150047173]\n",
      "[Iter 1383][Class loss: 0.000050099162763]\n",
      "[Iter 1384][Class loss: 0.000050033111620]\n",
      "[Iter 1385][Class loss: 0.000049976915761]\n",
      "[Iter 1386][Class loss: 0.000049915121053]\n",
      "[Iter 1387][Class loss: 0.000049841837608]\n",
      "[Iter 1388][Class loss: 0.000049779810070]\n",
      "[Iter 1389][Class loss: 0.000049729125749]\n",
      "[Iter 1390][Class loss: 0.000049666494306]\n",
      "[Iter 1391][Class loss: 0.000049601781939]\n",
      "[Iter 1392][Class loss: 0.000049536702136]\n",
      "[Iter 1393][Class loss: 0.000049480186135]\n",
      "[Iter 1394][Class loss: 0.000049423131713]\n",
      "[Iter 1395][Class loss: 0.000049356549425]\n",
      "[Iter 1396][Class loss: 0.000049292353651]\n",
      "[Iter 1397][Class loss: 0.000049254002079]\n",
      "[Iter 1398][Class loss: 0.000049174585001]\n",
      "[Iter 1399][Class loss: 0.000049141010095]\n",
      "[Iter 1400][Class loss: 0.000049090071116]\n",
      "[Iter 1401][Class loss: 0.000049021829909]\n",
      "[Iter 1402][Class loss: 0.000048966732720]\n",
      "[Iter 1403][Class loss: 0.000048901190894]\n",
      "[Iter 1404][Class loss: 0.000048826357670]\n",
      "[Iter 1405][Class loss: 0.000048769452405]\n",
      "[Iter 1406][Class loss: 0.000048714107834]\n",
      "[Iter 1407][Class loss: 0.000048653077101]\n",
      "[Iter 1408][Class loss: 0.000048595567932]\n",
      "[Iter 1409][Class loss: 0.000048529425840]\n",
      "[Iter 1410][Class loss: 0.000048494039220]\n",
      "[Iter 1411][Class loss: 0.000048420857638]\n",
      "[Iter 1412][Class loss: 0.000048365574912]\n",
      "[Iter 1413][Class loss: 0.000048306377721]\n",
      "[Iter 1414][Class loss: 0.000048259415053]\n",
      "[Iter 1415][Class loss: 0.000048195317504]\n",
      "[Iter 1416][Class loss: 0.000048137953854]\n",
      "[Iter 1417][Class loss: 0.000048083551519]\n",
      "[Iter 1418][Class loss: 0.000048024427088]\n",
      "[Iter 1419][Class loss: 0.000047968809668]\n",
      "[Iter 1420][Class loss: 0.000047907604312]\n",
      "[Iter 1421][Class loss: 0.000047858513426]\n",
      "[Iter 1422][Class loss: 0.000047797308071]\n",
      "[Iter 1423][Class loss: 0.000047746019845]\n",
      "[Iter 1424][Class loss: 0.000047689216444]\n",
      "[Iter 1425][Class loss: 0.000047639026889]\n",
      "[Iter 1426][Class loss: 0.000047586036089]\n",
      "[Iter 1427][Class loss: 0.000047520170483]\n",
      "[Iter 1428][Class loss: 0.000047484114475]\n",
      "[Iter 1429][Class loss: 0.000047413737775]\n",
      "[Iter 1430][Class loss: 0.000047367509978]\n",
      "[Iter 1431][Class loss: 0.000047319874284]\n",
      "[Iter 1432][Class loss: 0.000047258865379]\n",
      "[Iter 1433][Class loss: 0.000047204008297]\n",
      "[Iter 1434][Class loss: 0.000047146670113]\n",
      "[Iter 1435][Class loss: 0.000047085588449]\n",
      "[Iter 1436][Class loss: 0.000047039320634]\n",
      "[Iter 1437][Class loss: 0.000046979108447]\n",
      "[Iter 1438][Class loss: 0.000046937308070]\n",
      "[Iter 1439][Class loss: 0.000046872046369]\n",
      "[Iter 1440][Class loss: 0.000046825181926]\n",
      "[Iter 1441][Class loss: 0.000046778608521]\n",
      "[Iter 1442][Class loss: 0.000046724613640]\n",
      "[Iter 1443][Class loss: 0.000046663411922]\n",
      "[Iter 1444][Class loss: 0.000046605404350]\n",
      "[Iter 1445][Class loss: 0.000046553526772]\n",
      "[Iter 1446][Class loss: 0.000046509107051]\n",
      "[Iter 1447][Class loss: 0.000046452056267]\n",
      "[Iter 1448][Class loss: 0.000046397708502]\n",
      "[Iter 1449][Class loss: 0.000046345718147]\n",
      "[Iter 1450][Class loss: 0.000046299755923]\n",
      "[Iter 1451][Class loss: 0.000046249064326]\n",
      "[Iter 1452][Class loss: 0.000046191111323]\n",
      "[Iter 1453][Class loss: 0.000046135246521]\n",
      "[Iter 1454][Class loss: 0.000046086584916]\n",
      "[Iter 1455][Class loss: 0.000046029286750]\n",
      "[Iter 1456][Class loss: 0.000045975964895]\n",
      "[Iter 1457][Class loss: 0.000045924178266]\n",
      "[Iter 1458][Class loss: 0.000045873362978]\n",
      "[Iter 1459][Class loss: 0.000045836277422]\n",
      "[Iter 1460][Class loss: 0.000045780434448]\n",
      "[Iter 1461][Class loss: 0.000045728433179]\n",
      "[Iter 1462][Class loss: 0.000045674980356]\n",
      "[Iter 1463][Class loss: 0.000045623430196]\n",
      "[Iter 1464][Class loss: 0.000045575048716]\n",
      "[Iter 1465][Class loss: 0.000045522665459]\n",
      "[Iter 1466][Class loss: 0.000045473698265]\n",
      "[Iter 1467][Class loss: 0.000045426364522]\n",
      "[Iter 1468][Class loss: 0.000045383483666]\n",
      "[Iter 1469][Class loss: 0.000045336353651]\n",
      "[Iter 1470][Class loss: 0.000045274187869]\n",
      "[Iter 1471][Class loss: 0.000045252607379]\n",
      "[Iter 1472][Class loss: 0.000045189968660]\n",
      "[Iter 1473][Class loss: 0.000045153290557]\n",
      "[Iter 1474][Class loss: 0.000045113192755]\n",
      "[Iter 1475][Class loss: 0.000045051252528]\n",
      "[Iter 1476][Class loss: 0.000044992386393]\n",
      "[Iter 1477][Class loss: 0.000044962805987]\n",
      "[Iter 1478][Class loss: 0.000044898391934]\n",
      "[Iter 1479][Class loss: 0.000044859461923]\n",
      "[Iter 1480][Class loss: 0.000044823795179]\n",
      "[Iter 1481][Class loss: 0.000044764961785]\n",
      "[Iter 1482][Class loss: 0.000044704815082]\n",
      "[Iter 1483][Class loss: 0.000044655280362]\n",
      "[Iter 1484][Class loss: 0.000044607637392]\n",
      "[Iter 1485][Class loss: 0.000044566477300]\n",
      "[Iter 1486][Class loss: 0.000044517939386]\n",
      "[Iter 1487][Class loss: 0.000044463613449]\n",
      "[Iter 1488][Class loss: 0.000044410764531]\n",
      "[Iter 1489][Class loss: 0.000044370935939]\n",
      "[Iter 1490][Class loss: 0.000044320295274]\n",
      "[Iter 1491][Class loss: 0.000044272775995]\n",
      "[Iter 1492][Class loss: 0.000044230811909]\n",
      "[Iter 1493][Class loss: 0.000044176664233]\n",
      "[Iter 1494][Class loss: 0.000044129978050]\n",
      "[Iter 1495][Class loss: 0.000044082455133]\n",
      "[Iter 1496][Class loss: 0.000044037828047]\n",
      "[Iter 1497][Class loss: 0.000043989704864]\n",
      "[Iter 1498][Class loss: 0.000043941210606]\n",
      "[Iter 1499][Class loss: 0.000043896372517]\n",
      "[Iter 1500][Class loss: 0.000043852494855]\n",
      "[Iter 1501][Class loss: 0.000043807580369]\n",
      "[Iter 1502][Class loss: 0.000043761036068]\n",
      "[Iter 1503][Class loss: 0.000043713949708]\n",
      "[Iter 1504][Class loss: 0.000043669162551]\n",
      "[Iter 1505][Class loss: 0.000043627784180]\n",
      "[Iter 1506][Class loss: 0.000043582047510]\n",
      "[Iter 1507][Class loss: 0.000043534178985]\n",
      "[Iter 1508][Class loss: 0.000043490530516]\n",
      "[Iter 1509][Class loss: 0.000043447296775]\n",
      "[Iter 1510][Class loss: 0.000043399773858]\n",
      "[Iter 1511][Class loss: 0.000043356259994]\n",
      "[Iter 1512][Class loss: 0.000043310046749]\n",
      "[Iter 1513][Class loss: 0.000043262458348]\n",
      "[Iter 1514][Class loss: 0.000043221185479]\n",
      "[Iter 1515][Class loss: 0.000043175288738]\n",
      "[Iter 1516][Class loss: 0.000043129206460]\n",
      "[Iter 1517][Class loss: 0.000043092797569]\n",
      "[Iter 1518][Class loss: 0.000043046627979]\n",
      "[Iter 1519][Class loss: 0.000043010608351]\n",
      "[Iter 1520][Class loss: 0.000042955041863]\n",
      "[Iter 1521][Class loss: 0.000042923536967]\n",
      "[Iter 1522][Class loss: 0.000042879328248]\n",
      "[Iter 1523][Class loss: 0.000042826810386]\n",
      "[Iter 1524][Class loss: 0.000042784959078]\n",
      "[Iter 1525][Class loss: 0.000042740313802]\n",
      "[Iter 1526][Class loss: 0.000042695661250]\n",
      "[Iter 1527][Class loss: 0.000042655068683]\n",
      "[Iter 1528][Class loss: 0.000042612413381]\n",
      "[Iter 1529][Class loss: 0.000042564119212]\n",
      "[Iter 1530][Class loss: 0.000042536503315]\n",
      "[Iter 1531][Class loss: 0.000042483487050]\n",
      "[Iter 1532][Class loss: 0.000042443447455]\n",
      "[Iter 1533][Class loss: 0.000042401457904]\n",
      "[Iter 1534][Class loss: 0.000042356732592]\n",
      "[Iter 1535][Class loss: 0.000042313149606]\n",
      "[Iter 1536][Class loss: 0.000042271312850]\n",
      "[Iter 1537][Class loss: 0.000042228388338]\n",
      "[Iter 1538][Class loss: 0.000042191022658]\n",
      "[Iter 1539][Class loss: 0.000042148731154]\n",
      "[Iter 1540][Class loss: 0.000042099869461]\n",
      "[Iter 1541][Class loss: 0.000042060797568]\n",
      "[Iter 1542][Class loss: 0.000042025567382]\n",
      "[Iter 1543][Class loss: 0.000041981820686]\n",
      "[Iter 1544][Class loss: 0.000041943902033]\n",
      "[Iter 1545][Class loss: 0.000041897223127]\n",
      "[Iter 1546][Class loss: 0.000041860846977]\n",
      "[Iter 1547][Class loss: 0.000041815710574]\n",
      "[Iter 1548][Class loss: 0.000041779414460]\n",
      "[Iter 1549][Class loss: 0.000041734689148]\n",
      "[Iter 1550][Class loss: 0.000041698349378]\n",
      "[Iter 1551][Class loss: 0.000041654060624]\n",
      "[Iter 1552][Class loss: 0.000041613042413]\n",
      "[Iter 1553][Class loss: 0.000041569473979]\n",
      "[Iter 1554][Class loss: 0.000041530060116]\n",
      "[Iter 1555][Class loss: 0.000041491122829]\n",
      "[Iter 1556][Class loss: 0.000041447514377]\n",
      "[Iter 1557][Class loss: 0.000041410865379]\n",
      "[Iter 1558][Class loss: 0.000041368075472]\n",
      "[Iter 1559][Class loss: 0.000041331040848]\n",
      "[Iter 1560][Class loss: 0.000041286497435]\n",
      "[Iter 1561][Class loss: 0.000041247971239]\n",
      "[Iter 1562][Class loss: 0.000041212857468]\n",
      "[Iter 1563][Class loss: 0.000041176492232]\n",
      "[Iter 1564][Class loss: 0.000041139497625]\n",
      "[Iter 1565][Class loss: 0.000041091498133]\n",
      "[Iter 1566][Class loss: 0.000041050021537]\n",
      "[Iter 1567][Class loss: 0.000041020113713]\n",
      "[Iter 1568][Class loss: 0.000040979699406]\n",
      "[Iter 1569][Class loss: 0.000040943807107]\n",
      "[Iter 1570][Class loss: 0.000040899220039]\n",
      "[Iter 1571][Class loss: 0.000040859544242]\n",
      "[Iter 1572][Class loss: 0.000040826082113]\n",
      "[Iter 1573][Class loss: 0.000040782972064]\n",
      "[Iter 1574][Class loss: 0.000040752580389]\n",
      "[Iter 1575][Class loss: 0.000040709885070]\n",
      "[Iter 1576][Class loss: 0.000040667819121]\n",
      "[Iter 1577][Class loss: 0.000040629136492]\n",
      "[Iter 1578][Class loss: 0.000040585651732]\n",
      "[Iter 1579][Class loss: 0.000040553488361]\n",
      "[Iter 1580][Class loss: 0.000040513856220]\n",
      "[Iter 1581][Class loss: 0.000040472732508]\n",
      "[Iter 1582][Class loss: 0.000040438397264]\n",
      "[Iter 1583][Class loss: 0.000040396033000]\n",
      "[Iter 1584][Class loss: 0.000040366881876]\n",
      "[Iter 1585][Class loss: 0.000040327264287]\n",
      "[Iter 1586][Class loss: 0.000040281178372]\n",
      "[Iter 1587][Class loss: 0.000040245209675]\n",
      "[Iter 1588][Class loss: 0.000040214268665]\n",
      "[Iter 1589][Class loss: 0.000040175807953]\n",
      "[Iter 1590][Class loss: 0.000040138049371]\n",
      "[Iter 1591][Class loss: 0.000040094015276]\n",
      "[Iter 1592][Class loss: 0.000040064209315]\n",
      "[Iter 1593][Class loss: 0.000040027978685]\n",
      "[Iter 1594][Class loss: 0.000039984835894]\n",
      "[Iter 1595][Class loss: 0.000039951395593]\n",
      "[Iter 1596][Class loss: 0.000039907718019]\n",
      "[Iter 1597][Class loss: 0.000039890423068]\n",
      "[Iter 1598][Class loss: 0.000039848015149]\n",
      "[Iter 1599][Class loss: 0.000039803795516]\n",
      "[Iter 1600][Class loss: 0.000039774371544]\n",
      "[Iter 1601][Class loss: 0.000039734208258]\n",
      "[Iter 1602][Class loss: 0.000039688533434]\n",
      "[Iter 1603][Class loss: 0.000039652310079]\n",
      "[Iter 1604][Class loss: 0.000039620299503]\n",
      "[Iter 1605][Class loss: 0.000039582329919]\n",
      "[Iter 1606][Class loss: 0.000039546837797]\n",
      "[Iter 1607][Class loss: 0.000039506623580]\n",
      "[Iter 1608][Class loss: 0.000039473976358]\n",
      "[Iter 1609][Class loss: 0.000039439335524]\n",
      "[Iter 1610][Class loss: 0.000039399892557]\n",
      "[Iter 1611][Class loss: 0.000039364997065]\n",
      "[Iter 1612][Class loss: 0.000039331964217]\n",
      "[Iter 1613][Class loss: 0.000039293434384]\n",
      "[Iter 1614][Class loss: 0.000039259222831]\n",
      "[Iter 1615][Class loss: 0.000039225000364]\n",
      "[Iter 1616][Class loss: 0.000039190093958]\n",
      "[Iter 1617][Class loss: 0.000039153230318]\n",
      "[Iter 1618][Class loss: 0.000039117388951]\n",
      "[Iter 1619][Class loss: 0.000039087568439]\n",
      "[Iter 1620][Class loss: 0.000039054175431]\n",
      "[Iter 1621][Class loss: 0.000039015299990]\n",
      "[Iter 1622][Class loss: 0.000038978861994]\n",
      "[Iter 1623][Class loss: 0.000038944926928]\n",
      "[Iter 1624][Class loss: 0.000038911392039]\n",
      "[Iter 1625][Class loss: 0.000038875361497]\n",
      "[Iter 1626][Class loss: 0.000038839203626]\n",
      "[Iter 1627][Class loss: 0.000038803613279]\n",
      "[Iter 1628][Class loss: 0.000038771740947]\n",
      "[Iter 1629][Class loss: 0.000038739206502]\n",
      "[Iter 1630][Class loss: 0.000038706744817]\n",
      "[Iter 1631][Class loss: 0.000038667618355]\n",
      "[Iter 1632][Class loss: 0.000038635815145]\n",
      "[Iter 1633][Class loss: 0.000038600046537]\n",
      "[Iter 1634][Class loss: 0.000038565674913]\n",
      "[Iter 1635][Class loss: 0.000038531390601]\n",
      "[Iter 1636][Class loss: 0.000038498856156]\n",
      "[Iter 1637][Class loss: 0.000038464699173]\n",
      "[Iter 1638][Class loss: 0.000038435799070]\n",
      "[Iter 1639][Class loss: 0.000038402700739]\n",
      "[Iter 1640][Class loss: 0.000038364360080]\n",
      "[Iter 1641][Class loss: 0.000038334754208]\n",
      "[Iter 1642][Class loss: 0.000038294143451]\n",
      "[Iter 1643][Class loss: 0.000038271966332]\n",
      "[Iter 1644][Class loss: 0.000038240861613]\n",
      "[Iter 1645][Class loss: 0.000038197558752]\n",
      "[Iter 1646][Class loss: 0.000038176200178]\n",
      "[Iter 1647][Class loss: 0.000038133002818]\n",
      "[Iter 1648][Class loss: 0.000038101057726]\n",
      "[Iter 1649][Class loss: 0.000038072321331]\n",
      "[Iter 1650][Class loss: 0.000038040907384]\n",
      "[Iter 1651][Class loss: 0.000038004334783]\n",
      "[Iter 1652][Class loss: 0.000037974805309]\n",
      "[Iter 1653][Class loss: 0.000037939316826]\n",
      "[Iter 1654][Class loss: 0.000037906720536]\n",
      "[Iter 1655][Class loss: 0.000037878438889]\n",
      "[Iter 1656][Class loss: 0.000037837835407]\n",
      "[Iter 1657][Class loss: 0.000037802612496]\n",
      "[Iter 1658][Class loss: 0.000037772126234]\n",
      "[Iter 1659][Class loss: 0.000037740574044]\n",
      "[Iter 1660][Class loss: 0.000037707519368]\n",
      "[Iter 1661][Class loss: 0.000037674595660]\n",
      "[Iter 1662][Class loss: 0.000037642217649]\n",
      "[Iter 1663][Class loss: 0.000037614139728]\n",
      "[Iter 1664][Class loss: 0.000037580430217]\n",
      "[Iter 1665][Class loss: 0.000037544225052]\n",
      "[Iter 1666][Class loss: 0.000037521473132]\n",
      "[Iter 1667][Class loss: 0.000037483972847]\n",
      "[Iter 1668][Class loss: 0.000037453799450]\n",
      "[Iter 1669][Class loss: 0.000037422789319]\n",
      "[Iter 1670][Class loss: 0.000037391328078]\n",
      "[Iter 1671][Class loss: 0.000037355235690]\n",
      "[Iter 1672][Class loss: 0.000037328762119]\n",
      "[Iter 1673][Class loss: 0.000037293983041]\n",
      "[Iter 1674][Class loss: 0.000037280329707]\n",
      "[Iter 1675][Class loss: 0.000037243284169]\n",
      "[Iter 1676][Class loss: 0.000037201869418]\n",
      "[Iter 1677][Class loss: 0.000037176956539]\n",
      "[Iter 1678][Class loss: 0.000037145229726]\n",
      "[Iter 1679][Class loss: 0.000037113473809]\n",
      "[Iter 1680][Class loss: 0.000037075169530]\n",
      "[Iter 1681][Class loss: 0.000037047124351]\n",
      "[Iter 1682][Class loss: 0.000037019122828]\n",
      "[Iter 1683][Class loss: 0.000036991244997]\n",
      "[Iter 1684][Class loss: 0.000036957004340]\n",
      "[Iter 1685][Class loss: 0.000036924291635]\n",
      "[Iter 1686][Class loss: 0.000036890211049]\n",
      "[Iter 1687][Class loss: 0.000036868565076]\n",
      "[Iter 1688][Class loss: 0.000036832014302]\n",
      "[Iter 1689][Class loss: 0.000036805897253]\n",
      "[Iter 1690][Class loss: 0.000036769954022]\n",
      "[Iter 1691][Class loss: 0.000036747118429]\n",
      "[Iter 1692][Class loss: 0.000036709934648]\n",
      "[Iter 1693][Class loss: 0.000036687775719]\n",
      "[Iter 1694][Class loss: 0.000036660931073]\n",
      "[Iter 1695][Class loss: 0.000036623350752]\n",
      "[Iter 1696][Class loss: 0.000036596586142]\n",
      "[Iter 1697][Class loss: 0.000036563666072]\n",
      "[Iter 1698][Class loss: 0.000036535260733]\n",
      "[Iter 1699][Class loss: 0.000036507517507]\n",
      "[Iter 1700][Class loss: 0.000036477656977]\n",
      "[Iter 1701][Class loss: 0.000036443285353]\n",
      "[Iter 1702][Class loss: 0.000036417222873]\n",
      "[Iter 1703][Class loss: 0.000036387449654]\n",
      "[Iter 1704][Class loss: 0.000036363704567]\n",
      "[Iter 1705][Class loss: 0.000036330689909]\n",
      "[Iter 1706][Class loss: 0.000036294328311]\n",
      "[Iter 1707][Class loss: 0.000036265286326]\n",
      "[Iter 1708][Class loss: 0.000036241028283]\n",
      "[Iter 1709][Class loss: 0.000036209814425]\n",
      "[Iter 1710][Class loss: 0.000036177490983]\n",
      "[Iter 1711][Class loss: 0.000036152785469]\n",
      "[Iter 1712][Class loss: 0.000036122306483]\n",
      "[Iter 1713][Class loss: 0.000036093188100]\n",
      "[Iter 1714][Class loss: 0.000036062425352]\n",
      "[Iter 1715][Class loss: 0.000036027988244]\n",
      "[Iter 1716][Class loss: 0.000036007280869]\n",
      "[Iter 1717][Class loss: 0.000035971810576]\n",
      "[Iter 1718][Class loss: 0.000035943303374]\n",
      "[Iter 1719][Class loss: 0.000035916855268]\n",
      "[Iter 1720][Class loss: 0.000035885321267]\n",
      "[Iter 1721][Class loss: 0.000035858440242]\n",
      "[Iter 1722][Class loss: 0.000035827131796]\n",
      "[Iter 1723][Class loss: 0.000035798293538]\n",
      "[Iter 1724][Class loss: 0.000035769870010]\n",
      "[Iter 1725][Class loss: 0.000035741431930]\n",
      "[Iter 1726][Class loss: 0.000035713157558]\n",
      "[Iter 1727][Class loss: 0.000035685625335]\n",
      "[Iter 1728][Class loss: 0.000035658289562]\n",
      "[Iter 1729][Class loss: 0.000035627570469]\n",
      "[Iter 1730][Class loss: 0.000035600634874]\n",
      "[Iter 1731][Class loss: 0.000035576886148]\n",
      "[Iter 1732][Class loss: 0.000035547156585]\n",
      "[Iter 1733][Class loss: 0.000035522360122]\n",
      "[Iter 1734][Class loss: 0.000035491291783]\n",
      "[Iter 1735][Class loss: 0.000035465211113]\n",
      "[Iter 1736][Class loss: 0.000035435641621]\n",
      "[Iter 1737][Class loss: 0.000035412671423]\n",
      "[Iter 1738][Class loss: 0.000035377073800]\n",
      "[Iter 1739][Class loss: 0.000035355576983]\n",
      "[Iter 1740][Class loss: 0.000035328601371]\n",
      "[Iter 1741][Class loss: 0.000035299450246]\n",
      "[Iter 1742][Class loss: 0.000035266242776]\n",
      "[Iter 1743][Class loss: 0.000035246623156]\n",
      "[Iter 1744][Class loss: 0.000035221790313]\n",
      "[Iter 1745][Class loss: 0.000035194083466]\n",
      "[Iter 1746][Class loss: 0.000035173168726]\n",
      "[Iter 1747][Class loss: 0.000035138749809]\n",
      "[Iter 1748][Class loss: 0.000035113476770]\n",
      "[Iter 1749][Class loss: 0.000035083241528]\n",
      "[Iter 1750][Class loss: 0.000035058670619]\n",
      "[Iter 1751][Class loss: 0.000035033444874]\n",
      "[Iter 1752][Class loss: 0.000035001299693]\n",
      "[Iter 1753][Class loss: 0.000034976874304]\n",
      "[Iter 1754][Class loss: 0.000034949393012]\n",
      "[Iter 1755][Class loss: 0.000034926866647]\n",
      "[Iter 1756][Class loss: 0.000034890923416]\n",
      "[Iter 1757][Class loss: 0.000034868644434]\n",
      "[Iter 1758][Class loss: 0.000034843465983]\n",
      "[Iter 1759][Class loss: 0.000034818673157]\n",
      "[Iter 1760][Class loss: 0.000034788892663]\n",
      "[Iter 1761][Class loss: 0.000034758217225]\n",
      "[Iter 1762][Class loss: 0.000034733260691]\n",
      "[Iter 1763][Class loss: 0.000034710858017]\n",
      "[Iter 1764][Class loss: 0.000034688360756]\n",
      "[Iter 1765][Class loss: 0.000034657725337]\n",
      "[Iter 1766][Class loss: 0.000034635231714]\n",
      "[Iter 1767][Class loss: 0.000034605938708]\n",
      "[Iter 1768][Class loss: 0.000034574135498]\n",
      "[Iter 1769][Class loss: 0.000034554101148]\n",
      "[Iter 1770][Class loss: 0.000034524186049]\n",
      "[Iter 1771][Class loss: 0.000034499578760]\n",
      "[Iter 1772][Class loss: 0.000034474411223]\n",
      "[Iter 1773][Class loss: 0.000034443550248]\n",
      "[Iter 1774][Class loss: 0.000034420747397]\n",
      "[Iter 1775][Class loss: 0.000034394994145]\n",
      "[Iter 1776][Class loss: 0.000034367843909]\n",
      "[Iter 1777][Class loss: 0.000034339253034]\n",
      "[Iter 1778][Class loss: 0.000034317912650]\n",
      "[Iter 1779][Class loss: 0.000034287088056]\n",
      "[Iter 1780][Class loss: 0.000034263939597]\n",
      "[Iter 1781][Class loss: 0.000034237029467]\n",
      "[Iter 1782][Class loss: 0.000034210042941]\n",
      "[Iter 1783][Class loss: 0.000034183292883]\n",
      "[Iter 1784][Class loss: 0.000034159456845]\n",
      "[Iter 1785][Class loss: 0.000034133463487]\n",
      "[Iter 1786][Class loss: 0.000034108517866]\n",
      "[Iter 1787][Class loss: 0.000034081444028]\n",
      "[Iter 1788][Class loss: 0.000034062079067]\n",
      "[Iter 1789][Class loss: 0.000034033189877]\n",
      "[Iter 1790][Class loss: 0.000034007312934]\n",
      "[Iter 1791][Class loss: 0.000033986674680]\n",
      "[Iter 1792][Class loss: 0.000033954289393]\n",
      "[Iter 1793][Class loss: 0.000033936921682]\n",
      "[Iter 1794][Class loss: 0.000033912823710]\n",
      "[Iter 1795][Class loss: 0.000033880369301]\n",
      "[Iter 1796][Class loss: 0.000033867036109]\n",
      "[Iter 1797][Class loss: 0.000033834308852]\n",
      "[Iter 1798][Class loss: 0.000033818454540]\n",
      "[Iter 1799][Class loss: 0.000033800421079]\n",
      "[Iter 1800][Class loss: 0.000033770447772]\n",
      "[Iter 1801][Class loss: 0.000033734042518]\n",
      "[Iter 1802][Class loss: 0.000033730055293]\n",
      "[Iter 1803][Class loss: 0.000033703174267]\n",
      "[Iter 1804][Class loss: 0.000033662581700]\n",
      "[Iter 1805][Class loss: 0.000033644086216]\n",
      "[Iter 1806][Class loss: 0.000033616001019]\n",
      "[Iter 1807][Class loss: 0.000033589421946]\n",
      "[Iter 1808][Class loss: 0.000033566539059]\n",
      "[Iter 1809][Class loss: 0.000033541615267]\n",
      "[Iter 1810][Class loss: 0.000033518310374]\n",
      "[Iter 1811][Class loss: 0.000033492106013]\n",
      "[Iter 1812][Class loss: 0.000033466993045]\n",
      "[Iter 1813][Class loss: 0.000033439842809]\n",
      "[Iter 1814][Class loss: 0.000033415806683]\n",
      "[Iter 1815][Class loss: 0.000033395997889]\n",
      "[Iter 1816][Class loss: 0.000033371190511]\n",
      "[Iter 1817][Class loss: 0.000033349191654]\n",
      "[Iter 1818][Class loss: 0.000033320375223]\n",
      "[Iter 1819][Class loss: 0.000033298307244]\n",
      "[Iter 1820][Class loss: 0.000033274453017]\n",
      "[Iter 1821][Class loss: 0.000033250409615]\n",
      "[Iter 1822][Class loss: 0.000033227639506]\n",
      "[Iter 1823][Class loss: 0.000033199707104]\n",
      "[Iter 1824][Class loss: 0.000033177318983]\n",
      "[Iter 1825][Class loss: 0.000033154214179]\n",
      "[Iter 1826][Class loss: 0.000033128435462]\n",
      "[Iter 1827][Class loss: 0.000033107957279]\n",
      "[Iter 1828][Class loss: 0.000033086056646]\n",
      "[Iter 1829][Class loss: 0.000033061289287]\n",
      "[Iter 1830][Class loss: 0.000033036543755]\n",
      "[Iter 1831][Class loss: 0.000033012584026]\n",
      "[Iter 1832][Class loss: 0.000032987540180]\n",
      "[Iter 1833][Class loss: 0.000032970710890]\n",
      "[Iter 1834][Class loss: 0.000032946139982]\n",
      "[Iter 1835][Class loss: 0.000032916825148]\n",
      "[Iter 1836][Class loss: 0.000032906653360]\n",
      "[Iter 1837][Class loss: 0.000032875774195]\n",
      "[Iter 1838][Class loss: 0.000032856831240]\n",
      "[Iter 1839][Class loss: 0.000032838062907]\n",
      "[Iter 1840][Class loss: 0.000032807976822]\n",
      "[Iter 1841][Class loss: 0.000032782489143]\n",
      "[Iter 1842][Class loss: 0.000032763200579]\n",
      "[Iter 1843][Class loss: 0.000032738083974]\n",
      "[Iter 1844][Class loss: 0.000032718457078]\n",
      "[Iter 1845][Class loss: 0.000032689174986]\n",
      "[Iter 1846][Class loss: 0.000032677973650]\n",
      "[Iter 1847][Class loss: 0.000032649433706]\n",
      "[Iter 1848][Class loss: 0.000032625994209]\n",
      "[Iter 1849][Class loss: 0.000032609743357]\n",
      "[Iter 1850][Class loss: 0.000032585696317]\n",
      "[Iter 1851][Class loss: 0.000032559124520]\n",
      "[Iter 1852][Class loss: 0.000032532567275]\n",
      "[Iter 1853][Class loss: 0.000032510237361]\n",
      "[Iter 1854][Class loss: 0.000032488351280]\n",
      "[Iter 1855][Class loss: 0.000032469055441]\n",
      "[Iter 1856][Class loss: 0.000032443662349]\n",
      "[Iter 1857][Class loss: 0.000032424974052]\n",
      "[Iter 1858][Class loss: 0.000032395459129]\n",
      "[Iter 1859][Class loss: 0.000032372783608]\n",
      "[Iter 1860][Class loss: 0.000032353840652]\n",
      "[Iter 1861][Class loss: 0.000032331205148]\n",
      "[Iter 1862][Class loss: 0.000032305211789]\n",
      "[Iter 1863][Class loss: 0.000032285184716]\n",
      "[Iter 1864][Class loss: 0.000032261799788]\n",
      "[Iter 1865][Class loss: 0.000032239426218]\n",
      "[Iter 1866][Class loss: 0.000032214979001]\n",
      "[Iter 1867][Class loss: 0.000032197989640]\n",
      "[Iter 1868][Class loss: 0.000032171712519]\n",
      "[Iter 1869][Class loss: 0.000032151492633]\n",
      "[Iter 1870][Class loss: 0.000032127594750]\n",
      "[Iter 1871][Class loss: 0.000032104348065]\n",
      "[Iter 1872][Class loss: 0.000032083611586]\n",
      "[Iter 1873][Class loss: 0.000032060808735]\n",
      "[Iter 1874][Class loss: 0.000032039712096]\n",
      "[Iter 1875][Class loss: 0.000032018215279]\n",
      "[Iter 1876][Class loss: 0.000031998315535]\n",
      "[Iter 1877][Class loss: 0.000031975068850]\n",
      "[Iter 1878][Class loss: 0.000031951523852]\n",
      "[Iter 1879][Class loss: 0.000031931347621]\n",
      "[Iter 1880][Class loss: 0.000031911902624]\n",
      "[Iter 1881][Class loss: 0.000031889616366]\n",
      "[Iter 1882][Class loss: 0.000031869010854]\n",
      "[Iter 1883][Class loss: 0.000031844207115]\n",
      "[Iter 1884][Class loss: 0.000031827978091]\n",
      "[Iter 1885][Class loss: 0.000031807438063]\n",
      "[Iter 1886][Class loss: 0.000031779687561]\n",
      "[Iter 1887][Class loss: 0.000031770330679]\n",
      "[Iter 1888][Class loss: 0.000031744239095]\n",
      "[Iter 1889][Class loss: 0.000031721061532]\n",
      "[Iter 1890][Class loss: 0.000031705836591]\n",
      "[Iter 1891][Class loss: 0.000031682222470]\n",
      "[Iter 1892][Class loss: 0.000031654013583]\n",
      "[Iter 1893][Class loss: 0.000031638552173]\n",
      "[Iter 1894][Class loss: 0.000031614668842]\n",
      "[Iter 1895][Class loss: 0.000031592440791]\n",
      "[Iter 1896][Class loss: 0.000031574669265]\n",
      "[Iter 1897][Class loss: 0.000031548439438]\n",
      "[Iter 1898][Class loss: 0.000031530755223]\n",
      "[Iter 1899][Class loss: 0.000031506202504]\n",
      "[Iter 1900][Class loss: 0.000031484567444]\n",
      "[Iter 1901][Class loss: 0.000031464922358]\n",
      "[Iter 1902][Class loss: 0.000031444564229]\n",
      "[Iter 1903][Class loss: 0.000031421335734]\n",
      "[Iter 1904][Class loss: 0.000031400581065]\n",
      "[Iter 1905][Class loss: 0.000031379757274]\n",
      "[Iter 1906][Class loss: 0.000031360912544]\n",
      "[Iter 1907][Class loss: 0.000031339051930]\n",
      "[Iter 1908][Class loss: 0.000031317445973]\n",
      "[Iter 1909][Class loss: 0.000031297033274]\n",
      "[Iter 1910][Class loss: 0.000031276100344]\n",
      "[Iter 1911][Class loss: 0.000031255825888]\n",
      "[Iter 1912][Class loss: 0.000031235191273]\n",
      "[Iter 1913][Class loss: 0.000031213654438]\n",
      "[Iter 1914][Class loss: 0.000031197978387]\n",
      "[Iter 1915][Class loss: 0.000031175910408]\n",
      "[Iter 1916][Class loss: 0.000031160154322]\n",
      "[Iter 1917][Class loss: 0.000031134579331]\n",
      "[Iter 1918][Class loss: 0.000031117531762]\n",
      "[Iter 1919][Class loss: 0.000031097340980]\n",
      "[Iter 1920][Class loss: 0.000031076324376]\n",
      "[Iter 1921][Class loss: 0.000031053408748]\n",
      "[Iter 1922][Class loss: 0.000031033541745]\n",
      "[Iter 1923][Class loss: 0.000031013187254]\n",
      "[Iter 1924][Class loss: 0.000031004638004]\n",
      "[Iter 1925][Class loss: 0.000030980343581]\n",
      "[Iter 1926][Class loss: 0.000030956987757]\n",
      "[Iter 1927][Class loss: 0.000030937058909]\n",
      "[Iter 1928][Class loss: 0.000030914703530]\n",
      "[Iter 1929][Class loss: 0.000030896291719]\n",
      "[Iter 1930][Class loss: 0.000030873023206]\n",
      "[Iter 1931][Class loss: 0.000030852868804]\n",
      "[Iter 1932][Class loss: 0.000030833038181]\n",
      "[Iter 1933][Class loss: 0.000030817089282]\n",
      "[Iter 1934][Class loss: 0.000030793496990]\n",
      "[Iter 1935][Class loss: 0.000030779570807]\n",
      "[Iter 1936][Class loss: 0.000030759885703]\n",
      "[Iter 1937][Class loss: 0.000030741230148]\n",
      "[Iter 1938][Class loss: 0.000030720944778]\n",
      "[Iter 1939][Class loss: 0.000030693430745]\n",
      "[Iter 1940][Class loss: 0.000030674134905]\n",
      "[Iter 1941][Class loss: 0.000030657745810]\n",
      "[Iter 1942][Class loss: 0.000030643848731]\n",
      "[Iter 1943][Class loss: 0.000030618772143]\n",
      "[Iter 1944][Class loss: 0.000030605719076]\n",
      "[Iter 1945][Class loss: 0.000030586234061]\n",
      "[Iter 1946][Class loss: 0.000030560986488]\n",
      "[Iter 1947][Class loss: 0.000030550429074]\n",
      "[Iter 1948][Class loss: 0.000030533134122]\n",
      "[Iter 1949][Class loss: 0.000030502527807]\n",
      "[Iter 1950][Class loss: 0.000030480436180]\n",
      "[Iter 1951][Class loss: 0.000030464314477]\n",
      "[Iter 1952][Class loss: 0.000030449278711]\n",
      "[Iter 1953][Class loss: 0.000030432613130]\n",
      "[Iter 1954][Class loss: 0.000030409675674]\n",
      "[Iter 1955][Class loss: 0.000030391011023]\n",
      "[Iter 1956][Class loss: 0.000030367653380]\n",
      "[Iter 1957][Class loss: 0.000030348726796]\n",
      "[Iter 1958][Class loss: 0.000030332623282]\n",
      "[Iter 1959][Class loss: 0.000030315910408]\n",
      "[Iter 1960][Class loss: 0.000030295725082]\n",
      "[Iter 1961][Class loss: 0.000030270914067]\n",
      "[Iter 1962][Class loss: 0.000030251088901]\n",
      "[Iter 1963][Class loss: 0.000030233251891]\n",
      "[Iter 1964][Class loss: 0.000030218372558]\n",
      "[Iter 1965][Class loss: 0.000030197514207]\n",
      "[Iter 1966][Class loss: 0.000030176308428]\n",
      "[Iter 1967][Class loss: 0.000030158227673]\n",
      "[Iter 1968][Class loss: 0.000030137927752]\n",
      "[Iter 1969][Class loss: 0.000030119634175]\n",
      "[Iter 1970][Class loss: 0.000030102357414]\n",
      "[Iter 1971][Class loss: 0.000030081377190]\n",
      "[Iter 1972][Class loss: 0.000030060544304]\n",
      "[Iter 1973][Class loss: 0.000030043524021]\n",
      "[Iter 1974][Class loss: 0.000030023813451]\n",
      "[Iter 1975][Class loss: 0.000030009152397]\n",
      "[Iter 1976][Class loss: 0.000029989134418]\n",
      "[Iter 1977][Class loss: 0.000029969818570]\n",
      "[Iter 1978][Class loss: 0.000029949551390]\n",
      "[Iter 1979][Class loss: 0.000029934828490]\n",
      "[Iter 1980][Class loss: 0.000029918834116]\n",
      "[Iter 1981][Class loss: 0.000029896898923]\n",
      "[Iter 1982][Class loss: 0.000029875403925]\n",
      "[Iter 1983][Class loss: 0.000029861796065]\n",
      "[Iter 1984][Class loss: 0.000029838412956]\n",
      "[Iter 1985][Class loss: 0.000029822502256]\n",
      "[Iter 1986][Class loss: 0.000029801554774]\n",
      "[Iter 1987][Class loss: 0.000029789349355]\n",
      "[Iter 1988][Class loss: 0.000029766295484]\n",
      "[Iter 1989][Class loss: 0.000029754228308]\n",
      "[Iter 1990][Class loss: 0.000029740753234]\n",
      "[Iter 1991][Class loss: 0.000029718630685]\n",
      "[Iter 1992][Class loss: 0.000029694989280]\n",
      "[Iter 1993][Class loss: 0.000029685337722]\n",
      "[Iter 1994][Class loss: 0.000029663346140]\n",
      "[Iter 1995][Class loss: 0.000029640606954]\n",
      "[Iter 1996][Class loss: 0.000029627428376]\n",
      "[Iter 1997][Class loss: 0.000029607334000]\n",
      "[Iter 1998][Class loss: 0.000029588456528]\n",
      "[Iter 1999][Class loss: 0.000029570264815]\n"
     ]
    }
   ],
   "source": [
    "max_iters = 2000\n",
    "iter_start_offset = 0\n",
    "\n",
    "for iter_train_outer in range(iter_start_offset, max_iters):\n",
    "    detector_train.train()\n",
    "    loss = detector_train(\n",
    "        node_features = batch_data['object_features'],\n",
    "        edge_index = batch_data['edge_idx'],\n",
    "        object_size = batch_data['object_num_meas'],\n",
    "        groundtruths = batch_data['object_class'] )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_str = f\"[Iter {iter_train_outer}][Class loss: {loss.item():.15f}]\"\n",
    "    print(loss_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class scores: tensor([1.0000, 0.9981, 1.0000, 1.0000, 0.9974, 0.9987, 0.9988, 0.9988],\n",
      "       device='cuda:0')\n",
      "predicted class       : tensor([0, 0, 0, 0, 4, 6, 6, 6], device='cuda:0')\n",
      "ground-truth classses : tensor([0, 0, 0, 0, 4, 6, 6, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "@ torch.no_grad()\n",
    "def inference(classifier):\n",
    "    pred_class_logits = classifier(\n",
    "        node_features = batch_data['object_features'][0],\n",
    "        edge_index = batch_data['edge_idx'][0],\n",
    "        object_size = batch_data['object_num_meas'][0])\n",
    "    cls_prob = F.softmax(pred_class_logits, dim=-1)\n",
    "    cls_score, cls_idx = torch.max(cls_prob, dim=-1)\n",
    "    return cls_score, cls_idx\n",
    "\n",
    "classifier = detector_train.pred.eval()\n",
    "cls_score, cls_idx = inference(classifier)\n",
    "\n",
    "print(f\"predicted class scores: {cls_score}\")\n",
    "print(f\"predicted class       : {cls_idx}\")\n",
    "print(f\"ground-truth classses : {batch_data['object_class'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
