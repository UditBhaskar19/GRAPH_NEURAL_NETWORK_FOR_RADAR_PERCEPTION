{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "module_rootdir = '.'\n",
    "dataset_rootdir = '.'\n",
    "label_rootdir = module_rootdir\n",
    "sys.path.append(module_rootdir)\n",
    "\n",
    "trained_proposal_weights_path = './model_weights/gnn/1715232829109/graph_based_detector.pt'\n",
    "proposal_config_file_path = './configuration_radarscenes_gnn.yml'\n",
    "classifier_config_file_path = './configuration_radarscenes_classifier.yml'\n",
    "\n",
    "from modules.set_configurations.common import read_yaml\n",
    "from modules.set_configurations.set_config_gnn import config as config_gnn\n",
    "from modules.set_configurations.set_config_classifier import config as config_classifier\n",
    "from modules.set_configurations.set_param_for_inference_gnn import set_parameters_for_inference\n",
    "from modules.neural_net.classifier.classifier import Model_Training\n",
    "from modules.inference.clustering import Simple_DBSCAN\n",
    "from modules.data_utils.read_data import get_sequence_data\n",
    "from modules.data_utils.labels import compute_new_labels_to_id_dict\n",
    "from modules.data_utils.labels import compute_old_to_new_label_id_map\n",
    "\n",
    "from modules.data_generator.datagen_gnn import compute_node_idx_for_each_cluster\n",
    "from modules.data_utils.read_data import extract_frame\n",
    "from modules.compute_groundtruth.compute_node_labels import compute_ground_truth as compute_ground_truth_node\n",
    "from modules.compute_features.graph_features import select_moving_data\n",
    "from modules.compute_features.graph_features import compute_adjacency_information\n",
    "from modules.compute_features.graph_features import compute_edge_features\n",
    "from modules.compute_features.graph_features import compute_node_features\n",
    "from modules.compute_groundtruth.compute_offsets import unnormalize_gt_offsets\n",
    "from modules.data_generator.datagen_classifier import extract_proposals\n",
    "from modules.data_generator.datagen_classifier import extract_and_compute_features_and_labels\n",
    "from modules.data_generator.datagen_classifier import remove_low_quality_proposals\n",
    "from modules.data_generator.datagen_classifier import compute_graph\n",
    "\n",
    "from modules.compute_groundtruth.compute_node_labels import compute_gt_clusters\n",
    "from modules.inference.ellipse import compute_cov_ellipse\n",
    "from modules.plot_utils.plot_func import compare_pred_gt_clusters\n",
    "\n",
    "sequence_name = 'sequence_148' # 'sequence_108'  # 'sequence_148'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is available. Good to go!\n"
     ]
    }
   ],
   "source": [
    "config_gnn_obj = config_gnn(proposal_config_file_path)\n",
    "config_classifier_obj = config_classifier(proposal_config_file_path, classifier_config_file_path)\n",
    "\n",
    "param_obj = set_parameters_for_inference(module_rootdir, config_gnn_obj, trained_proposal_weights_path)\n",
    "device = param_obj['device']\n",
    "grid = param_obj['grid']\n",
    "prop_extractor = param_obj['detector']\n",
    "\n",
    "detector_train = Model_Training(config_classifier_obj)\n",
    "detector_train = detector_train.to(device)\n",
    "\n",
    "params = [p for p in detector_train.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, momentum=0.9, lr=config_classifier_obj.learning_rate, weight_decay=config_classifier_obj.weight_decay)\n",
    "\n",
    "clustering_eps = config_classifier_obj.clustering_eps # 1.4\n",
    "valid_cluster_num_meas_thr = config_classifier_obj.valid_cluster_num_meas_thr  # 2\n",
    "meas_noise_cov = config_classifier_obj.meas_noise_cov\n",
    "clustering_obj = Simple_DBSCAN(clustering_eps)\n",
    "\n",
    "scene_metadata, radar_mount_data, radar_data_all_scenes, odometry_data_all_scenes \\\n",
    "    = get_sequence_data(dataset_rootdir, config_classifier_obj.dataset_path, sequence_name, config_classifier_obj.window_size)\n",
    "\n",
    "labels_to_id_dict = compute_new_labels_to_id_dict()\n",
    "old_to_new_label_id_map = compute_old_to_new_label_id_map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "frame number: 40\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x800 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_frame(idx):\n",
    "\n",
    "    print('-' * 100)\n",
    "    print(f'frame number: {idx}')\n",
    "\n",
    "    data_dict = extract_frame(\n",
    "        idx = idx, \n",
    "        windowed_data_list = scene_metadata,\n",
    "        radar_mount_data = radar_mount_data,\n",
    "        radar_data_all_scenes = radar_data_all_scenes,\n",
    "        odometry_data_all_scenes = odometry_data_all_scenes,\n",
    "        reject_outlier = False)\n",
    "    \n",
    "    gt_dict_node = compute_ground_truth_node(data_dict, labels_to_id_dict, old_to_new_label_id_map)\n",
    "    data_dict, gt_dict_node = grid.select_meas_within_the_grid(data_dict, gt_dict_node)\n",
    "    data_dict_dyn, node_labels_dict_dyn = select_moving_data(data_dict, gt_dict_node, labels_to_id_dict)\n",
    "    adj_dict_dyn = compute_adjacency_information(\n",
    "        data_dict_dyn, config_classifier_obj.ball_query_eps_square, config_classifier_obj.k_number_nearest_points)\n",
    "\n",
    "    other_features_dyn = np.stack((\n",
    "            data_dict_dyn['meas_px'], data_dict_dyn['meas_py'], \n",
    "            data_dict_dyn['meas_vx'], data_dict_dyn['meas_vy']), axis=-1)\n",
    "    edge_features_dyn = compute_edge_features(data_dict_dyn, adj_dict_dyn['adj_list'])\n",
    "    node_features_dyn = compute_node_features(\n",
    "            data_dict_dyn, adj_dict_dyn['degree'], \n",
    "            include_region_confidence = config_classifier_obj.include_region_confidence, \n",
    "            min_range = config_classifier_obj.grid_min_r, max_range = config_classifier_obj.grid_max_r, \n",
    "            min_azimuth = config_classifier_obj.grid_min_th, max_azimuth = config_classifier_obj.grid_max_th)\n",
    "    \n",
    "    graph_features = {}\n",
    "    graph_features['other_features_dyn'] = torch.from_numpy(other_features_dyn).to(device).to(torch.float32)\n",
    "    graph_features['edge_features_dyn'] = torch.from_numpy(edge_features_dyn).to(device).to(torch.float32)\n",
    "    graph_features['node_features_dyn'] = torch.from_numpy(node_features_dyn).to(device).to(torch.float32)\n",
    "    graph_features['edge_index_dyn'] = torch.from_numpy(adj_dict_dyn['adj_list'] ).to(device).to(torch.int64)\n",
    "    graph_features['adj_matrix_dyn'] = torch.from_numpy(adj_dict_dyn['adj_matrix'] ).to(device).to(torch.bool)\n",
    "\n",
    "    gt_labels_dyn = node_labels_dict_dyn['class_labels']\n",
    "    cluster_node_idx, cluster_labels = compute_node_idx_for_each_cluster(data_dict_dyn['meas_trackid'], gt_labels_dyn, device)\n",
    "\n",
    "    # cluster offsets predictions\n",
    "    _, node_offsets_predictions, _, _ = prop_extractor(\n",
    "            node_features = graph_features['node_features_dyn'],\n",
    "            edge_features = graph_features['edge_features_dyn'],\n",
    "            edge_index = graph_features['edge_index_dyn'],\n",
    "            adj_matrix = graph_features['adj_matrix_dyn'],\n",
    "            cluster_node_idx = cluster_node_idx )\n",
    "    \n",
    "    reg_deltas = unnormalize_gt_offsets(node_offsets_predictions, config_classifier_obj.reg_mu, config_classifier_obj.reg_sigma)\n",
    "    pred_cluster_centers_xy = graph_features['other_features_dyn'][:, :2] + reg_deltas\n",
    "\n",
    "    # extract proposals\n",
    "    pred_cluster_centers_xy = pred_cluster_centers_xy.detach().cpu().numpy()\n",
    "    proposal_list = extract_proposals(pred_cluster_centers_xy, clustering_obj)\n",
    "\n",
    "    # compute aggregate features and compute labels for each proposal\n",
    "    px = graph_features['other_features_dyn'][:, 0].detach().cpu().numpy()\n",
    "    py = graph_features['other_features_dyn'][:, 1].detach().cpu().numpy()\n",
    "    rcs = graph_features['node_features_dyn'][:, 1].detach().cpu().numpy()\n",
    "\n",
    "    object_features_list, object_class_list, \\\n",
    "    object_mu_list, object_sigma_list, object_num_meas_list \\\n",
    "        = extract_and_compute_features_and_labels(\n",
    "                proposal_list, px, py, rcs, \n",
    "                node_labels_dict_dyn['class_labels'], \n",
    "                meas_noise_cov)\n",
    "            \n",
    "    # remove bad quality proposals\n",
    "    # a proposal is considered bad quality of num_meas of each clustered proposal < self.num_meas_thr\n",
    "    object_features_list, object_class_list, \\\n",
    "    pred_mu_list, pred_sigma_list, object_num_meas_list \\\n",
    "        = remove_low_quality_proposals(\n",
    "            valid_cluster_num_meas_thr, object_features_list, object_class_list, \n",
    "            object_mu_list, object_sigma_list, object_num_meas_list)\n",
    "    \n",
    "    # ======================================================================================\n",
    "    # compare prediction vs ground-truth clusters\n",
    "    gt_mu_list, gt_sigma_list = compute_gt_clusters(px, py, data_dict_dyn['meas_trackid'], meas_noise_cov)\n",
    "    gt_cluster_boundary_points_list = []\n",
    "    for mu, cov in zip(gt_mu_list, gt_sigma_list):\n",
    "        boundary_points, _ = compute_cov_ellipse(mu, cov, chi_sq = 2, n_points=50)\n",
    "        gt_cluster_boundary_points_list.append(boundary_points)\n",
    "\n",
    "    pred_cluster_boundary_points_list = []\n",
    "    for mu, cov in zip(pred_mu_list, pred_sigma_list):\n",
    "        boundary_points, _ = compute_cov_ellipse(mu, cov, chi_sq = 2, n_points=50)\n",
    "        pred_cluster_boundary_points_list.append(boundary_points)\n",
    "    \n",
    "    compare_pred_gt_clusters(\n",
    "        px, py, \n",
    "        pred_mu_list, pred_cluster_boundary_points_list,\n",
    "        gt_mu_list, gt_cluster_boundary_points_list,\n",
    "        boundary_marker_size=2,\n",
    "        figsize=(16,8))\n",
    "    # ======================================================================================\n",
    "    \n",
    "    # create training data: node_features, graph_structure, cluster_sizes\n",
    "    object_features, object_class, object_num_meas, edge_idx = \\\n",
    "        compute_graph(object_features_list, object_class_list, object_num_meas_list, device)\n",
    "    \n",
    "    training_data = {}\n",
    "    training_data['object_features'] = [object_features]\n",
    "    training_data['object_class'] = [object_class]\n",
    "    training_data['object_num_meas'] = [object_num_meas]\n",
    "    training_data['edge_idx'] = [edge_idx]\n",
    "\n",
    "    return training_data\n",
    "\n",
    "\n",
    "batch_data = process_frame(idx=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 0][Class loss: 4.499345302581787]\n",
      "[Iter 1][Class loss: 4.339859008789062]\n",
      "[Iter 2][Class loss: 4.027580261230469]\n",
      "[Iter 3][Class loss: 3.337669849395752]\n",
      "[Iter 4][Class loss: 1.263587474822998]\n",
      "[Iter 5][Class loss: 8.526643753051758]\n",
      "[Iter 6][Class loss: 3.869756221771240]\n",
      "[Iter 7][Class loss: 4.299895286560059]\n",
      "[Iter 8][Class loss: 4.382486343383789]\n",
      "[Iter 9][Class loss: 4.413622856140137]\n",
      "[Iter 10][Class loss: 4.427082538604736]\n",
      "[Iter 11][Class loss: 4.428927421569824]\n",
      "[Iter 12][Class loss: 4.418644428253174]\n",
      "[Iter 13][Class loss: 4.392259597778320]\n",
      "[Iter 14][Class loss: 4.345764160156250]\n",
      "[Iter 15][Class loss: 4.264877796173096]\n",
      "[Iter 16][Class loss: 4.116000175476074]\n",
      "[Iter 17][Class loss: 3.804168224334717]\n",
      "[Iter 18][Class loss: 3.003735303878784]\n",
      "[Iter 19][Class loss: 1.130524754524231]\n",
      "[Iter 20][Class loss: 4.706548213958740]\n",
      "[Iter 21][Class loss: 3.815249204635620]\n",
      "[Iter 22][Class loss: 4.358558654785156]\n",
      "[Iter 23][Class loss: 4.427595138549805]\n",
      "[Iter 24][Class loss: 4.442425727844238]\n",
      "[Iter 25][Class loss: 4.447530746459961]\n",
      "[Iter 26][Class loss: 4.448372840881348]\n",
      "[Iter 27][Class loss: 4.447527885437012]\n",
      "[Iter 28][Class loss: 4.446070671081543]\n",
      "[Iter 29][Class loss: 4.443895816802979]\n",
      "[Iter 30][Class loss: 4.441287994384766]\n",
      "[Iter 31][Class loss: 4.438087463378906]\n",
      "[Iter 32][Class loss: 4.434525489807129]\n",
      "[Iter 33][Class loss: 4.430575370788574]\n",
      "[Iter 34][Class loss: 4.426307201385498]\n",
      "[Iter 35][Class loss: 4.421722412109375]\n",
      "[Iter 36][Class loss: 4.417028427124023]\n",
      "[Iter 37][Class loss: 4.412107467651367]\n",
      "[Iter 38][Class loss: 4.406968116760254]\n",
      "[Iter 39][Class loss: 4.401619911193848]\n",
      "[Iter 40][Class loss: 4.396084785461426]\n",
      "[Iter 41][Class loss: 4.390503406524658]\n",
      "[Iter 42][Class loss: 4.384852409362793]\n",
      "[Iter 43][Class loss: 4.379140853881836]\n",
      "[Iter 44][Class loss: 4.373312473297119]\n",
      "[Iter 45][Class loss: 4.367355346679688]\n",
      "[Iter 46][Class loss: 4.361204624176025]\n",
      "[Iter 47][Class loss: 4.354886054992676]\n",
      "[Iter 48][Class loss: 4.348398208618164]\n",
      "[Iter 49][Class loss: 4.341944217681885]\n",
      "[Iter 50][Class loss: 4.335452079772949]\n",
      "[Iter 51][Class loss: 4.328960895538330]\n",
      "[Iter 52][Class loss: 4.322574615478516]\n",
      "[Iter 53][Class loss: 4.316127777099609]\n",
      "[Iter 54][Class loss: 4.309648036956787]\n",
      "[Iter 55][Class loss: 4.303121089935303]\n",
      "[Iter 56][Class loss: 4.296436786651611]\n",
      "[Iter 57][Class loss: 4.289646625518799]\n",
      "[Iter 58][Class loss: 4.282705307006836]\n",
      "[Iter 59][Class loss: 4.275588035583496]\n",
      "[Iter 60][Class loss: 4.268308639526367]\n",
      "[Iter 61][Class loss: 4.260948657989502]\n",
      "[Iter 62][Class loss: 4.253382682800293]\n",
      "[Iter 63][Class loss: 4.245531558990479]\n",
      "[Iter 64][Class loss: 4.237331867218018]\n",
      "[Iter 65][Class loss: 4.228715419769287]\n",
      "[Iter 66][Class loss: 4.219643592834473]\n",
      "[Iter 67][Class loss: 4.210037231445312]\n",
      "[Iter 68][Class loss: 4.199848175048828]\n",
      "[Iter 69][Class loss: 4.188908100128174]\n",
      "[Iter 70][Class loss: 4.177059173583984]\n",
      "[Iter 71][Class loss: 4.164156913757324]\n",
      "[Iter 72][Class loss: 4.149863719940186]\n",
      "[Iter 73][Class loss: 4.133674621582031]\n",
      "[Iter 74][Class loss: 4.115071296691895]\n",
      "[Iter 75][Class loss: 4.093178749084473]\n",
      "[Iter 76][Class loss: 4.066908836364746]\n",
      "[Iter 77][Class loss: 4.034278392791748]\n",
      "[Iter 78][Class loss: 3.993016719818115]\n",
      "[Iter 79][Class loss: 3.938690900802612]\n",
      "[Iter 80][Class loss: 3.861904621124268]\n",
      "[Iter 81][Class loss: 3.748798370361328]\n",
      "[Iter 82][Class loss: 3.567711353302002]\n",
      "[Iter 83][Class loss: 3.235997915267944]\n",
      "[Iter 84][Class loss: 2.525692462921143]\n",
      "[Iter 85][Class loss: 1.392655372619629]\n",
      "[Iter 86][Class loss: 3.924199581146240]\n",
      "[Iter 87][Class loss: 1.553853273391724]\n",
      "[Iter 88][Class loss: 2.357624053955078]\n",
      "[Iter 89][Class loss: 2.429383516311646]\n",
      "[Iter 90][Class loss: 1.947000026702881]\n",
      "[Iter 91][Class loss: 0.993231654167175]\n",
      "[Iter 92][Class loss: 2.840204000473022]\n",
      "[Iter 93][Class loss: 1.254207253456116]\n",
      "[Iter 94][Class loss: 2.007837772369385]\n",
      "[Iter 95][Class loss: 2.009774684906006]\n",
      "[Iter 96][Class loss: 1.445935726165771]\n",
      "[Iter 97][Class loss: 1.023128747940063]\n",
      "[Iter 98][Class loss: 1.757552623748779]\n",
      "[Iter 99][Class loss: 1.340149164199829]\n",
      "[Iter 100][Class loss: 1.928303241729736]\n",
      "[Iter 101][Class loss: 1.893121242523193]\n",
      "[Iter 102][Class loss: 1.361001014709473]\n",
      "[Iter 103][Class loss: 0.963746845722198]\n",
      "[Iter 104][Class loss: 1.799430131912231]\n",
      "[Iter 105][Class loss: 0.999244391918182]\n",
      "[Iter 106][Class loss: 1.493892073631287]\n",
      "[Iter 107][Class loss: 1.494025945663452]\n",
      "[Iter 108][Class loss: 1.070850849151611]\n",
      "[Iter 109][Class loss: 1.086122512817383]\n",
      "[Iter 110][Class loss: 1.260613799095154]\n",
      "[Iter 111][Class loss: 0.930866360664368]\n",
      "[Iter 112][Class loss: 1.199680924415588]\n",
      "[Iter 113][Class loss: 1.151984453201294]\n",
      "[Iter 114][Class loss: 0.891848444938660]\n",
      "[Iter 115][Class loss: 1.118457317352295]\n",
      "[Iter 116][Class loss: 0.943237364292145]\n",
      "[Iter 117][Class loss: 0.935139775276184]\n",
      "[Iter 118][Class loss: 1.051520824432373]\n",
      "[Iter 119][Class loss: 0.939860165119171]\n",
      "[Iter 120][Class loss: 0.889132201671600]\n",
      "[Iter 121][Class loss: 1.011349439620972]\n",
      "[Iter 122][Class loss: 0.862397909164429]\n",
      "[Iter 123][Class loss: 0.946105837821960]\n",
      "[Iter 124][Class loss: 0.945012867450714]\n",
      "[Iter 125][Class loss: 0.861135840415955]\n",
      "[Iter 126][Class loss: 0.929159641265869]\n",
      "[Iter 127][Class loss: 0.890002787113190]\n",
      "[Iter 128][Class loss: 0.868233621120453]\n",
      "[Iter 129][Class loss: 0.912183165550232]\n",
      "[Iter 130][Class loss: 0.877190649509430]\n",
      "[Iter 131][Class loss: 0.860656857490540]\n",
      "[Iter 132][Class loss: 0.897761821746826]\n",
      "[Iter 133][Class loss: 0.855828225612640]\n",
      "[Iter 134][Class loss: 0.867888033390045]\n",
      "[Iter 135][Class loss: 0.879063665866852]\n",
      "[Iter 136][Class loss: 0.851314902305603]\n",
      "[Iter 137][Class loss: 0.860466301441193]\n",
      "[Iter 138][Class loss: 0.864507675170898]\n",
      "[Iter 139][Class loss: 0.844170093536377]\n",
      "[Iter 140][Class loss: 0.857101023197174]\n",
      "[Iter 141][Class loss: 0.853241264820099]\n",
      "[Iter 142][Class loss: 0.840100705623627]\n",
      "[Iter 143][Class loss: 0.850429654121399]\n",
      "[Iter 144][Class loss: 0.842570126056671]\n",
      "[Iter 145][Class loss: 0.836367070674896]\n",
      "[Iter 146][Class loss: 0.842297315597534]\n",
      "[Iter 147][Class loss: 0.834216058254242]\n",
      "[Iter 148][Class loss: 0.829835712909698]\n",
      "[Iter 149][Class loss: 0.833186388015747]\n",
      "[Iter 150][Class loss: 0.824722051620483]\n",
      "[Iter 151][Class loss: 0.823034048080444]\n",
      "[Iter 152][Class loss: 0.823042035102844]\n",
      "[Iter 153][Class loss: 0.815644264221191]\n",
      "[Iter 154][Class loss: 0.813613712787628]\n",
      "[Iter 155][Class loss: 0.811343312263489]\n",
      "[Iter 156][Class loss: 0.804172575473785]\n",
      "[Iter 157][Class loss: 0.801749348640442]\n",
      "[Iter 158][Class loss: 0.797231078147888]\n",
      "[Iter 159][Class loss: 0.790201783180237]\n",
      "[Iter 160][Class loss: 0.786562860012054]\n",
      "[Iter 161][Class loss: 0.780376732349396]\n",
      "[Iter 162][Class loss: 0.773047089576721]\n",
      "[Iter 163][Class loss: 0.767794132232666]\n",
      "[Iter 164][Class loss: 0.760279715061188]\n",
      "[Iter 165][Class loss: 0.752354621887207]\n",
      "[Iter 166][Class loss: 0.746014297008514]\n",
      "[Iter 167][Class loss: 0.737793624401093]\n",
      "[Iter 168][Class loss: 0.728989303112030]\n",
      "[Iter 169][Class loss: 0.720825314521790]\n",
      "[Iter 170][Class loss: 0.711439251899719]\n",
      "[Iter 171][Class loss: 0.701942324638367]\n",
      "[Iter 172][Class loss: 0.693300306797028]\n",
      "[Iter 173][Class loss: 0.683858096599579]\n",
      "[Iter 174][Class loss: 0.674587965011597]\n",
      "[Iter 175][Class loss: 0.666468560695648]\n",
      "[Iter 176][Class loss: 0.658360362052917]\n",
      "[Iter 177][Class loss: 0.650889694690704]\n",
      "[Iter 178][Class loss: 0.644513547420502]\n",
      "[Iter 179][Class loss: 0.638362646102905]\n",
      "[Iter 180][Class loss: 0.632519543170929]\n",
      "[Iter 181][Class loss: 0.627844750881195]\n",
      "[Iter 182][Class loss: 0.623669683933258]\n",
      "[Iter 183][Class loss: 0.619733572006226]\n",
      "[Iter 184][Class loss: 0.616356611251831]\n",
      "[Iter 185][Class loss: 0.613301217556000]\n",
      "[Iter 186][Class loss: 0.610357284545898]\n",
      "[Iter 187][Class loss: 0.607812404632568]\n",
      "[Iter 188][Class loss: 0.605564117431641]\n",
      "[Iter 189][Class loss: 0.603372216224670]\n",
      "[Iter 190][Class loss: 0.601413249969482]\n",
      "[Iter 191][Class loss: 0.599716305732727]\n",
      "[Iter 192][Class loss: 0.598071575164795]\n",
      "[Iter 193][Class loss: 0.596549034118652]\n",
      "[Iter 194][Class loss: 0.595195114612579]\n",
      "[Iter 195][Class loss: 0.593855857849121]\n",
      "[Iter 196][Class loss: 0.592576324939728]\n",
      "[Iter 197][Class loss: 0.591414749622345]\n",
      "[Iter 198][Class loss: 0.590246200561523]\n",
      "[Iter 199][Class loss: 0.589104950428009]\n",
      "[Iter 200][Class loss: 0.588046252727509]\n",
      "[Iter 201][Class loss: 0.586965441703796]\n",
      "[Iter 202][Class loss: 0.585898280143738]\n",
      "[Iter 203][Class loss: 0.584892511367798]\n",
      "[Iter 204][Class loss: 0.583886981010437]\n",
      "[Iter 205][Class loss: 0.582873940467834]\n",
      "[Iter 206][Class loss: 0.581909060478210]\n",
      "[Iter 207][Class loss: 0.580949306488037]\n",
      "[Iter 208][Class loss: 0.579977989196777]\n",
      "[Iter 209][Class loss: 0.579033076763153]\n",
      "[Iter 210][Class loss: 0.578085184097290]\n",
      "[Iter 211][Class loss: 0.577130436897278]\n",
      "[Iter 212][Class loss: 0.576186299324036]\n",
      "[Iter 213][Class loss: 0.575241327285767]\n",
      "[Iter 214][Class loss: 0.574293673038483]\n",
      "[Iter 215][Class loss: 0.573346614837646]\n",
      "[Iter 216][Class loss: 0.572384119033813]\n",
      "[Iter 217][Class loss: 0.571406245231628]\n",
      "[Iter 218][Class loss: 0.570425093173981]\n",
      "[Iter 219][Class loss: 0.569428265094757]\n",
      "[Iter 220][Class loss: 0.568412840366364]\n",
      "[Iter 221][Class loss: 0.567395448684692]\n",
      "[Iter 222][Class loss: 0.566372275352478]\n",
      "[Iter 223][Class loss: 0.565328717231750]\n",
      "[Iter 224][Class loss: 0.564280271530151]\n",
      "[Iter 225][Class loss: 0.563210248947144]\n",
      "[Iter 226][Class loss: 0.562126576900482]\n",
      "[Iter 227][Class loss: 0.561016559600830]\n",
      "[Iter 228][Class loss: 0.559883713722229]\n",
      "[Iter 229][Class loss: 0.558721184730530]\n",
      "[Iter 230][Class loss: 0.557539701461792]\n",
      "[Iter 231][Class loss: 0.556336879730225]\n",
      "[Iter 232][Class loss: 0.555108487606049]\n",
      "[Iter 233][Class loss: 0.553850769996643]\n",
      "[Iter 234][Class loss: 0.552568972110748]\n",
      "[Iter 235][Class loss: 0.551260769367218]\n",
      "[Iter 236][Class loss: 0.549929022789001]\n",
      "[Iter 237][Class loss: 0.548566937446594]\n",
      "[Iter 238][Class loss: 0.547168314456940]\n",
      "[Iter 239][Class loss: 0.545745372772217]\n",
      "[Iter 240][Class loss: 0.544287085533142]\n",
      "[Iter 241][Class loss: 0.542791306972504]\n",
      "[Iter 242][Class loss: 0.541255533695221]\n",
      "[Iter 243][Class loss: 0.539680957794189]\n",
      "[Iter 244][Class loss: 0.538063585758209]\n",
      "[Iter 245][Class loss: 0.536412358283997]\n",
      "[Iter 246][Class loss: 0.534729599952698]\n",
      "[Iter 247][Class loss: 0.532992720603943]\n",
      "[Iter 248][Class loss: 0.531195342540741]\n",
      "[Iter 249][Class loss: 0.529335558414459]\n",
      "[Iter 250][Class loss: 0.527420282363892]\n",
      "[Iter 251][Class loss: 0.525437355041504]\n",
      "[Iter 252][Class loss: 0.523378849029541]\n",
      "[Iter 253][Class loss: 0.521259069442749]\n",
      "[Iter 254][Class loss: 0.519060492515564]\n",
      "[Iter 255][Class loss: 0.516792893409729]\n",
      "[Iter 256][Class loss: 0.514444947242737]\n",
      "[Iter 257][Class loss: 0.512016773223877]\n",
      "[Iter 258][Class loss: 0.509494125843048]\n",
      "[Iter 259][Class loss: 0.506874859333038]\n",
      "[Iter 260][Class loss: 0.504149556159973]\n",
      "[Iter 261][Class loss: 0.501331508159637]\n",
      "[Iter 262][Class loss: 0.498418360948563]\n",
      "[Iter 263][Class loss: 0.495376646518707]\n",
      "[Iter 264][Class loss: 0.492215812206268]\n",
      "[Iter 265][Class loss: 0.488919615745544]\n",
      "[Iter 266][Class loss: 0.485463678836823]\n",
      "[Iter 267][Class loss: 0.481856226921082]\n",
      "[Iter 268][Class loss: 0.478062540292740]\n",
      "[Iter 269][Class loss: 0.474124789237976]\n",
      "[Iter 270][Class loss: 0.470019131898880]\n",
      "[Iter 271][Class loss: 0.465754806995392]\n",
      "[Iter 272][Class loss: 0.461290925741196]\n",
      "[Iter 273][Class loss: 0.456652581691742]\n",
      "[Iter 274][Class loss: 0.451738834381104]\n",
      "[Iter 275][Class loss: 0.446598261594772]\n",
      "[Iter 276][Class loss: 0.441205084323883]\n",
      "[Iter 277][Class loss: 0.435540199279785]\n",
      "[Iter 278][Class loss: 0.429538428783417]\n",
      "[Iter 279][Class loss: 0.423205375671387]\n",
      "[Iter 280][Class loss: 0.416513204574585]\n",
      "[Iter 281][Class loss: 0.409435212612152]\n",
      "[Iter 282][Class loss: 0.401941478252411]\n",
      "[Iter 283][Class loss: 0.394010603427887]\n",
      "[Iter 284][Class loss: 0.385590761899948]\n",
      "[Iter 285][Class loss: 0.376651942729950]\n",
      "[Iter 286][Class loss: 0.367182254791260]\n",
      "[Iter 287][Class loss: 0.357148945331573]\n",
      "[Iter 288][Class loss: 0.346497893333435]\n",
      "[Iter 289][Class loss: 0.335217028856277]\n",
      "[Iter 290][Class loss: 0.323322534561157]\n",
      "[Iter 291][Class loss: 0.310865581035614]\n",
      "[Iter 292][Class loss: 0.297942042350769]\n",
      "[Iter 293][Class loss: 0.284556388854980]\n",
      "[Iter 294][Class loss: 0.270715534687042]\n",
      "[Iter 295][Class loss: 0.256620407104492]\n",
      "[Iter 296][Class loss: 0.242520794272423]\n",
      "[Iter 297][Class loss: 0.228675961494446]\n",
      "[Iter 298][Class loss: 0.215393006801605]\n",
      "[Iter 299][Class loss: 0.202984899282455]\n",
      "[Iter 300][Class loss: 0.191812664270401]\n",
      "[Iter 301][Class loss: 0.182064771652222]\n",
      "[Iter 302][Class loss: 0.173851549625397]\n",
      "[Iter 303][Class loss: 0.167199462652206]\n",
      "[Iter 304][Class loss: 0.161974906921387]\n",
      "[Iter 305][Class loss: 0.157998099923134]\n",
      "[Iter 306][Class loss: 0.154920130968094]\n",
      "[Iter 307][Class loss: 0.152649074792862]\n",
      "[Iter 308][Class loss: 0.150915846228600]\n",
      "[Iter 309][Class loss: 0.149623736739159]\n",
      "[Iter 310][Class loss: 0.148552373051643]\n",
      "[Iter 311][Class loss: 0.147729575634003]\n",
      "[Iter 312][Class loss: 0.147093787789345]\n",
      "[Iter 313][Class loss: 0.146562874317169]\n",
      "[Iter 314][Class loss: 0.146148011088371]\n",
      "[Iter 315][Class loss: 0.145810782909393]\n",
      "[Iter 316][Class loss: 0.145563542842865]\n",
      "[Iter 317][Class loss: 0.145378232002258]\n",
      "[Iter 318][Class loss: 0.145215466618538]\n",
      "[Iter 319][Class loss: 0.145084455609322]\n",
      "[Iter 320][Class loss: 0.144975036382675]\n",
      "[Iter 321][Class loss: 0.144876718521118]\n",
      "[Iter 322][Class loss: 0.144792646169662]\n",
      "[Iter 323][Class loss: 0.144719615578651]\n",
      "[Iter 324][Class loss: 0.144653797149658]\n",
      "[Iter 325][Class loss: 0.144589468836784]\n",
      "[Iter 326][Class loss: 0.144518509507179]\n",
      "[Iter 327][Class loss: 0.144452676177025]\n",
      "[Iter 328][Class loss: 0.144398003816605]\n",
      "[Iter 329][Class loss: 0.144351124763489]\n",
      "[Iter 330][Class loss: 0.144306421279907]\n",
      "[Iter 331][Class loss: 0.144263818860054]\n",
      "[Iter 332][Class loss: 0.144225880503654]\n",
      "[Iter 333][Class loss: 0.144187226891518]\n",
      "[Iter 334][Class loss: 0.144160524010658]\n",
      "[Iter 335][Class loss: 0.144128173589706]\n",
      "[Iter 336][Class loss: 0.144095838069916]\n",
      "[Iter 337][Class loss: 0.144075199961662]\n",
      "[Iter 338][Class loss: 0.144057944417000]\n",
      "[Iter 339][Class loss: 0.144041076302528]\n",
      "[Iter 340][Class loss: 0.144027441740036]\n",
      "[Iter 341][Class loss: 0.144010633230209]\n",
      "[Iter 342][Class loss: 0.143996670842171]\n",
      "[Iter 343][Class loss: 0.143981456756592]\n",
      "[Iter 344][Class loss: 0.143966332077980]\n",
      "[Iter 345][Class loss: 0.143952324986458]\n",
      "[Iter 346][Class loss: 0.143937677145004]\n",
      "[Iter 347][Class loss: 0.143923833966255]\n",
      "[Iter 348][Class loss: 0.143909275531769]\n",
      "[Iter 349][Class loss: 0.143895104527473]\n",
      "[Iter 350][Class loss: 0.143881157040596]\n",
      "[Iter 351][Class loss: 0.143866717815399]\n",
      "[Iter 352][Class loss: 0.143852740526199]\n",
      "[Iter 353][Class loss: 0.143839776515961]\n",
      "[Iter 354][Class loss: 0.143824607133865]\n",
      "[Iter 355][Class loss: 0.143811717629433]\n",
      "[Iter 356][Class loss: 0.143803015351295]\n",
      "[Iter 357][Class loss: 0.143793657422066]\n",
      "[Iter 358][Class loss: 0.143783733248711]\n",
      "[Iter 359][Class loss: 0.143773630261421]\n",
      "[Iter 360][Class loss: 0.143763303756714]\n",
      "[Iter 361][Class loss: 0.143752872943878]\n",
      "[Iter 362][Class loss: 0.143742248415947]\n",
      "[Iter 363][Class loss: 0.143732845783234]\n",
      "[Iter 364][Class loss: 0.143724784255028]\n",
      "[Iter 365][Class loss: 0.143715679645538]\n",
      "[Iter 366][Class loss: 0.143706470727921]\n",
      "[Iter 367][Class loss: 0.143696710467339]\n",
      "[Iter 368][Class loss: 0.143687024712563]\n",
      "[Iter 369][Class loss: 0.143680706620216]\n",
      "[Iter 370][Class loss: 0.143671706318855]\n",
      "[Iter 371][Class loss: 0.143664196133614]\n",
      "[Iter 372][Class loss: 0.143655300140381]\n",
      "[Iter 373][Class loss: 0.143645405769348]\n",
      "[Iter 374][Class loss: 0.143636435270309]\n",
      "[Iter 375][Class loss: 0.143628016114235]\n",
      "[Iter 376][Class loss: 0.143620923161507]\n",
      "[Iter 377][Class loss: 0.143612563610077]\n",
      "[Iter 378][Class loss: 0.143603384494781]\n",
      "[Iter 379][Class loss: 0.143593952059746]\n",
      "[Iter 380][Class loss: 0.143583431839943]\n",
      "[Iter 381][Class loss: 0.143574923276901]\n",
      "[Iter 382][Class loss: 0.143565550446510]\n",
      "[Iter 383][Class loss: 0.143555238842964]\n",
      "[Iter 384][Class loss: 0.143545582890511]\n",
      "[Iter 385][Class loss: 0.143536075949669]\n",
      "[Iter 386][Class loss: 0.143525600433350]\n",
      "[Iter 387][Class loss: 0.143514737486839]\n",
      "[Iter 388][Class loss: 0.143503725528717]\n",
      "[Iter 389][Class loss: 0.143493741750717]\n",
      "[Iter 390][Class loss: 0.143487960100174]\n",
      "[Iter 391][Class loss: 0.143474265933037]\n",
      "[Iter 392][Class loss: 0.143463164567947]\n",
      "[Iter 393][Class loss: 0.143453940749168]\n",
      "[Iter 394][Class loss: 0.143443182110786]\n",
      "[Iter 395][Class loss: 0.143433108925819]\n",
      "[Iter 396][Class loss: 0.143423765897751]\n",
      "[Iter 397][Class loss: 0.143413618206978]\n",
      "[Iter 398][Class loss: 0.143404364585876]\n",
      "[Iter 399][Class loss: 0.143394351005554]\n",
      "[Iter 400][Class loss: 0.143383175134659]\n",
      "[Iter 401][Class loss: 0.143371418118477]\n",
      "[Iter 402][Class loss: 0.143358349800110]\n",
      "[Iter 403][Class loss: 0.143354445695877]\n",
      "[Iter 404][Class loss: 0.143343389034271]\n",
      "[Iter 405][Class loss: 0.143328920006752]\n",
      "[Iter 406][Class loss: 0.143317043781281]\n",
      "[Iter 407][Class loss: 0.143301621079445]\n",
      "[Iter 408][Class loss: 0.143286690115929]\n",
      "[Iter 409][Class loss: 0.143278211355209]\n",
      "[Iter 410][Class loss: 0.143261551856995]\n",
      "[Iter 411][Class loss: 0.143248721957207]\n",
      "[Iter 412][Class loss: 0.143238753080368]\n",
      "[Iter 413][Class loss: 0.143228203058243]\n",
      "[Iter 414][Class loss: 0.143216311931610]\n",
      "[Iter 415][Class loss: 0.143201842904091]\n",
      "[Iter 416][Class loss: 0.143190667033195]\n",
      "[Iter 417][Class loss: 0.143178358674049]\n",
      "[Iter 418][Class loss: 0.143166005611420]\n",
      "[Iter 419][Class loss: 0.143153443932533]\n",
      "[Iter 420][Class loss: 0.143141627311707]\n",
      "[Iter 421][Class loss: 0.143129974603653]\n",
      "[Iter 422][Class loss: 0.143117919564247]\n",
      "[Iter 423][Class loss: 0.143105983734131]\n",
      "[Iter 424][Class loss: 0.143091946840286]\n",
      "[Iter 425][Class loss: 0.143082767724991]\n",
      "[Iter 426][Class loss: 0.143067359924316]\n",
      "[Iter 427][Class loss: 0.143055647611618]\n",
      "[Iter 428][Class loss: 0.143042340874672]\n",
      "[Iter 429][Class loss: 0.143027946352959]\n",
      "[Iter 430][Class loss: 0.143014416098595]\n",
      "[Iter 431][Class loss: 0.142999917268753]\n",
      "[Iter 432][Class loss: 0.142986655235291]\n",
      "[Iter 433][Class loss: 0.142971947789192]\n",
      "[Iter 434][Class loss: 0.142957523465157]\n",
      "[Iter 435][Class loss: 0.142944455146790]\n",
      "[Iter 436][Class loss: 0.142928972840309]\n",
      "[Iter 437][Class loss: 0.142915368080139]\n",
      "[Iter 438][Class loss: 0.142897978425026]\n",
      "[Iter 439][Class loss: 0.142884388566017]\n",
      "[Iter 440][Class loss: 0.142867535352707]\n",
      "[Iter 441][Class loss: 0.142853543162346]\n",
      "[Iter 442][Class loss: 0.142838776111603]\n",
      "[Iter 443][Class loss: 0.142821818590164]\n",
      "[Iter 444][Class loss: 0.142805159091949]\n",
      "[Iter 445][Class loss: 0.142789795994759]\n",
      "[Iter 446][Class loss: 0.142773374915123]\n",
      "[Iter 447][Class loss: 0.142754852771759]\n",
      "[Iter 448][Class loss: 0.142737820744514]\n",
      "[Iter 449][Class loss: 0.142720460891724]\n",
      "[Iter 450][Class loss: 0.142703518271446]\n",
      "[Iter 451][Class loss: 0.142684474587440]\n",
      "[Iter 452][Class loss: 0.142667785286903]\n",
      "[Iter 453][Class loss: 0.142649888992310]\n",
      "[Iter 454][Class loss: 0.142629817128181]\n",
      "[Iter 455][Class loss: 0.142612040042877]\n",
      "[Iter 456][Class loss: 0.142593339085579]\n",
      "[Iter 457][Class loss: 0.142572104930878]\n",
      "[Iter 458][Class loss: 0.142554357647896]\n",
      "[Iter 459][Class loss: 0.142536759376526]\n",
      "[Iter 460][Class loss: 0.142513364553452]\n",
      "[Iter 461][Class loss: 0.142584457993507]\n",
      "[Iter 462][Class loss: 0.142601773142815]\n",
      "[Iter 463][Class loss: 0.142582610249519]\n",
      "[Iter 464][Class loss: 0.142520472407341]\n",
      "[Iter 465][Class loss: 0.142410919070244]\n",
      "[Iter 466][Class loss: 0.142396628856659]\n",
      "[Iter 467][Class loss: 0.142373800277710]\n",
      "[Iter 468][Class loss: 0.142360299825668]\n",
      "[Iter 469][Class loss: 0.142338141798973]\n",
      "[Iter 470][Class loss: 0.142297893762589]\n",
      "[Iter 471][Class loss: 0.142282411456108]\n",
      "[Iter 472][Class loss: 0.142250865697861]\n",
      "[Iter 473][Class loss: 0.142220944166183]\n",
      "[Iter 474][Class loss: 0.142194867134094]\n",
      "[Iter 475][Class loss: 0.142167776823044]\n",
      "[Iter 476][Class loss: 0.142139792442322]\n",
      "[Iter 477][Class loss: 0.142100512981415]\n",
      "[Iter 478][Class loss: 0.142080113291740]\n",
      "[Iter 479][Class loss: 0.142050608992577]\n",
      "[Iter 480][Class loss: 0.142020598053932]\n",
      "[Iter 481][Class loss: 0.141989037394524]\n",
      "[Iter 482][Class loss: 0.141964659094810]\n",
      "[Iter 483][Class loss: 0.141935467720032]\n",
      "[Iter 484][Class loss: 0.141909152269363]\n",
      "[Iter 485][Class loss: 0.141884386539459]\n",
      "[Iter 486][Class loss: 0.141855075955391]\n",
      "[Iter 487][Class loss: 0.141829729080200]\n",
      "[Iter 488][Class loss: 0.141802251338959]\n",
      "[Iter 489][Class loss: 0.141775265336037]\n",
      "[Iter 490][Class loss: 0.141751945018768]\n",
      "[Iter 491][Class loss: 0.141722992062569]\n",
      "[Iter 492][Class loss: 0.141696631908417]\n",
      "[Iter 493][Class loss: 0.141672551631927]\n",
      "[Iter 494][Class loss: 0.141640037298203]\n",
      "[Iter 495][Class loss: 0.141610905528069]\n",
      "[Iter 496][Class loss: 0.141576796770096]\n",
      "[Iter 497][Class loss: 0.141540482640266]\n",
      "[Iter 498][Class loss: 0.141513168811798]\n",
      "[Iter 499][Class loss: 0.141484990715981]\n",
      "[Iter 500][Class loss: 0.141449600458145]\n",
      "[Iter 501][Class loss: 0.141420155763626]\n",
      "[Iter 502][Class loss: 0.141400784254074]\n",
      "[Iter 503][Class loss: 0.141370698809624]\n",
      "[Iter 504][Class loss: 0.141323894262314]\n",
      "[Iter 505][Class loss: 0.141295522451401]\n",
      "[Iter 506][Class loss: 0.141267031431198]\n",
      "[Iter 507][Class loss: 0.141224309802055]\n",
      "[Iter 508][Class loss: 0.141179889440536]\n",
      "[Iter 509][Class loss: 0.141132444143295]\n",
      "[Iter 510][Class loss: 0.141103714704514]\n",
      "[Iter 511][Class loss: 0.141058877110481]\n",
      "[Iter 512][Class loss: 0.141021385788918]\n",
      "[Iter 513][Class loss: 0.140981480479240]\n",
      "[Iter 514][Class loss: 0.140945777297020]\n",
      "[Iter 515][Class loss: 0.140904679894447]\n",
      "[Iter 516][Class loss: 0.140869811177254]\n",
      "[Iter 517][Class loss: 0.140833884477615]\n",
      "[Iter 518][Class loss: 0.140784367918968]\n",
      "[Iter 519][Class loss: 0.140758275985718]\n",
      "[Iter 520][Class loss: 0.140719026327133]\n",
      "[Iter 521][Class loss: 0.140663743019104]\n",
      "[Iter 522][Class loss: 0.140628620982170]\n",
      "[Iter 523][Class loss: 0.140594914555550]\n",
      "[Iter 524][Class loss: 0.140547230839729]\n",
      "[Iter 525][Class loss: 0.140483751893044]\n",
      "[Iter 526][Class loss: 0.140445992350578]\n",
      "[Iter 527][Class loss: 0.140399724245071]\n",
      "[Iter 528][Class loss: 0.140338122844696]\n",
      "[Iter 529][Class loss: 0.140294983983040]\n",
      "[Iter 530][Class loss: 0.140245005488396]\n",
      "[Iter 531][Class loss: 0.140202239155769]\n",
      "[Iter 532][Class loss: 0.140148952603340]\n",
      "[Iter 533][Class loss: 0.140100166201591]\n",
      "[Iter 534][Class loss: 0.140051156282425]\n",
      "[Iter 535][Class loss: 0.139993727207184]\n",
      "[Iter 536][Class loss: 0.139941498637199]\n",
      "[Iter 537][Class loss: 0.139897018671036]\n",
      "[Iter 538][Class loss: 0.139834806323051]\n",
      "[Iter 539][Class loss: 0.139777600765228]\n",
      "[Iter 540][Class loss: 0.139725938439369]\n",
      "[Iter 541][Class loss: 0.139669001102448]\n",
      "[Iter 542][Class loss: 0.139619201421738]\n",
      "[Iter 543][Class loss: 0.139565408229828]\n",
      "[Iter 544][Class loss: 0.139490410685539]\n",
      "[Iter 545][Class loss: 0.139458522200584]\n",
      "[Iter 546][Class loss: 0.139393016695976]\n",
      "[Iter 547][Class loss: 0.139316231012344]\n",
      "[Iter 548][Class loss: 0.139265343546867]\n",
      "[Iter 549][Class loss: 0.139199331402779]\n",
      "[Iter 550][Class loss: 0.139129504561424]\n",
      "[Iter 551][Class loss: 0.139026284217834]\n",
      "[Iter 552][Class loss: 0.138912290334702]\n",
      "[Iter 553][Class loss: 0.138819947838783]\n",
      "[Iter 554][Class loss: 0.138705089688301]\n",
      "[Iter 555][Class loss: 0.138575881719589]\n",
      "[Iter 556][Class loss: 0.138450682163239]\n",
      "[Iter 557][Class loss: 0.138335391879082]\n",
      "[Iter 558][Class loss: 0.138315126299858]\n",
      "[Iter 559][Class loss: 0.138295501470566]\n",
      "[Iter 560][Class loss: 0.138186380267143]\n",
      "[Iter 561][Class loss: 0.138087406754494]\n",
      "[Iter 562][Class loss: 0.137938737869263]\n",
      "[Iter 563][Class loss: 0.137767806649208]\n",
      "[Iter 564][Class loss: 0.137656360864639]\n",
      "[Iter 565][Class loss: 0.137530550360680]\n",
      "[Iter 566][Class loss: 0.137391522526741]\n",
      "[Iter 567][Class loss: 0.137186065316200]\n",
      "[Iter 568][Class loss: 0.136998310685158]\n",
      "[Iter 569][Class loss: 0.136845231056213]\n",
      "[Iter 570][Class loss: 0.136761695146561]\n",
      "[Iter 571][Class loss: 0.136614605784416]\n",
      "[Iter 572][Class loss: 0.136510923504829]\n",
      "[Iter 573][Class loss: 0.136420831084251]\n",
      "[Iter 574][Class loss: 0.136323824524879]\n",
      "[Iter 575][Class loss: 0.136163204908371]\n",
      "[Iter 576][Class loss: 0.136014163494110]\n",
      "[Iter 577][Class loss: 0.135912060737610]\n",
      "[Iter 578][Class loss: 0.135750114917755]\n",
      "[Iter 579][Class loss: 0.135580927133560]\n",
      "[Iter 580][Class loss: 0.135479256510735]\n",
      "[Iter 581][Class loss: 0.135379761457443]\n",
      "[Iter 582][Class loss: 0.135227158665657]\n",
      "[Iter 583][Class loss: 0.135087296366692]\n",
      "[Iter 584][Class loss: 0.134952947497368]\n",
      "[Iter 585][Class loss: 0.134850785136223]\n",
      "[Iter 586][Class loss: 0.134729668498039]\n",
      "[Iter 587][Class loss: 0.134599894285202]\n",
      "[Iter 588][Class loss: 0.134444504976273]\n",
      "[Iter 589][Class loss: 0.134359240531921]\n",
      "[Iter 590][Class loss: 0.134180471301079]\n",
      "[Iter 591][Class loss: 0.134077608585358]\n",
      "[Iter 592][Class loss: 0.133963733911514]\n",
      "[Iter 593][Class loss: 0.133805319666862]\n",
      "[Iter 594][Class loss: 0.133622810244560]\n",
      "[Iter 595][Class loss: 0.133584722876549]\n",
      "[Iter 596][Class loss: 0.133455574512482]\n",
      "[Iter 597][Class loss: 0.133222445845604]\n",
      "[Iter 598][Class loss: 0.133080303668976]\n",
      "[Iter 599][Class loss: 0.132971376180649]\n",
      "[Iter 600][Class loss: 0.132820427417755]\n",
      "[Iter 601][Class loss: 0.132613569498062]\n",
      "[Iter 602][Class loss: 0.132534697651863]\n",
      "[Iter 603][Class loss: 0.132403448224068]\n",
      "[Iter 604][Class loss: 0.132275491952896]\n",
      "[Iter 605][Class loss: 0.132098048925400]\n",
      "[Iter 606][Class loss: 0.131992369890213]\n",
      "[Iter 607][Class loss: 0.131813362240791]\n",
      "[Iter 608][Class loss: 0.131636291742325]\n",
      "[Iter 609][Class loss: 0.131475403904915]\n",
      "[Iter 610][Class loss: 0.131386727094650]\n",
      "[Iter 611][Class loss: 0.131204277276993]\n",
      "[Iter 612][Class loss: 0.131010144948959]\n",
      "[Iter 613][Class loss: 0.130857259035110]\n",
      "[Iter 614][Class loss: 0.130724743008614]\n",
      "[Iter 615][Class loss: 0.130566596984863]\n",
      "[Iter 616][Class loss: 0.130379185080528]\n",
      "[Iter 617][Class loss: 0.130346044898033]\n",
      "[Iter 618][Class loss: 0.130194172263145]\n",
      "[Iter 619][Class loss: 0.129936963319778]\n",
      "[Iter 620][Class loss: 0.129808798432350]\n",
      "[Iter 621][Class loss: 0.129715904593468]\n",
      "[Iter 622][Class loss: 0.129473969340324]\n",
      "[Iter 623][Class loss: 0.129296064376831]\n",
      "[Iter 624][Class loss: 0.129214376211166]\n",
      "[Iter 625][Class loss: 0.129010766744614]\n",
      "[Iter 626][Class loss: 0.128846421837807]\n",
      "[Iter 627][Class loss: 0.128618255257607]\n",
      "[Iter 628][Class loss: 0.128549009561539]\n",
      "[Iter 629][Class loss: 0.128456965088844]\n",
      "[Iter 630][Class loss: 0.128245040774345]\n",
      "[Iter 631][Class loss: 0.128011718392372]\n",
      "[Iter 632][Class loss: 0.127939492464066]\n",
      "[Iter 633][Class loss: 0.127803966403008]\n",
      "[Iter 634][Class loss: 0.127548262476921]\n",
      "[Iter 635][Class loss: 0.127477020025253]\n",
      "[Iter 636][Class loss: 0.127372771501541]\n",
      "[Iter 637][Class loss: 0.127247452735901]\n",
      "[Iter 638][Class loss: 0.127051264047623]\n",
      "[Iter 639][Class loss: 0.127084821462631]\n",
      "[Iter 640][Class loss: 0.127140149474144]\n",
      "[Iter 641][Class loss: 0.126800537109375]\n",
      "[Iter 642][Class loss: 0.126429364085197]\n",
      "[Iter 643][Class loss: 0.126515924930573]\n",
      "[Iter 644][Class loss: 0.126491189002991]\n",
      "[Iter 645][Class loss: 0.126107171177864]\n",
      "[Iter 646][Class loss: 0.125922724604607]\n",
      "[Iter 647][Class loss: 0.125950455665588]\n",
      "[Iter 648][Class loss: 0.125909850001335]\n",
      "[Iter 649][Class loss: 0.125608891248703]\n",
      "[Iter 650][Class loss: 0.125392585992813]\n",
      "[Iter 651][Class loss: 0.125351905822754]\n",
      "[Iter 652][Class loss: 0.125308364629745]\n",
      "[Iter 653][Class loss: 0.125112116336823]\n",
      "[Iter 654][Class loss: 0.124841257929802]\n",
      "[Iter 655][Class loss: 0.124741047620773]\n",
      "[Iter 656][Class loss: 0.124668560922146]\n",
      "[Iter 657][Class loss: 0.124531790614128]\n",
      "[Iter 658][Class loss: 0.124406002461910]\n",
      "[Iter 659][Class loss: 0.124263241887093]\n",
      "[Iter 660][Class loss: 0.124133095145226]\n",
      "[Iter 661][Class loss: 0.124084562063217]\n",
      "[Iter 662][Class loss: 0.123997911810875]\n",
      "[Iter 663][Class loss: 0.123825900256634]\n",
      "[Iter 664][Class loss: 0.123719945549965]\n",
      "[Iter 665][Class loss: 0.123656511306763]\n",
      "[Iter 666][Class loss: 0.123551480472088]\n",
      "[Iter 667][Class loss: 0.123436614871025]\n",
      "[Iter 668][Class loss: 0.123362936079502]\n",
      "[Iter 669][Class loss: 0.123261108994484]\n",
      "[Iter 670][Class loss: 0.123134121298790]\n",
      "[Iter 671][Class loss: 0.123064897954464]\n",
      "[Iter 672][Class loss: 0.122986815869808]\n",
      "[Iter 673][Class loss: 0.122881956398487]\n",
      "[Iter 674][Class loss: 0.122779287397861]\n",
      "[Iter 675][Class loss: 0.122691988945007]\n",
      "[Iter 676][Class loss: 0.122607827186584]\n",
      "[Iter 677][Class loss: 0.122524291276932]\n",
      "[Iter 678][Class loss: 0.122437700629234]\n",
      "[Iter 679][Class loss: 0.122360512614250]\n",
      "[Iter 680][Class loss: 0.122298754751682]\n",
      "[Iter 681][Class loss: 0.122508458793163]\n",
      "[Iter 682][Class loss: 0.122444890439510]\n",
      "[Iter 683][Class loss: 0.122344560921192]\n",
      "[Iter 684][Class loss: 0.122280433773994]\n",
      "[Iter 685][Class loss: 0.124527618288994]\n",
      "[Iter 686][Class loss: 0.129225507378578]\n",
      "[Iter 687][Class loss: 0.135366335511208]\n",
      "[Iter 688][Class loss: 0.140812873840332]\n",
      "[Iter 689][Class loss: 0.143067613244057]\n",
      "[Iter 690][Class loss: 0.127337113022804]\n",
      "[Iter 691][Class loss: 0.122546918690205]\n",
      "[Iter 692][Class loss: 0.128898844122887]\n",
      "[Iter 693][Class loss: 0.128717303276062]\n",
      "[Iter 694][Class loss: 0.123828560113907]\n",
      "[Iter 695][Class loss: 0.122878551483154]\n",
      "[Iter 696][Class loss: 0.126496076583862]\n",
      "[Iter 697][Class loss: 0.126128420233727]\n",
      "[Iter 698][Class loss: 0.122380480170250]\n",
      "[Iter 699][Class loss: 0.123963288962841]\n",
      "[Iter 700][Class loss: 0.125500112771988]\n",
      "[Iter 701][Class loss: 0.122625678777695]\n",
      "[Iter 702][Class loss: 0.122668474912643]\n",
      "[Iter 703][Class loss: 0.124465316534042]\n",
      "[Iter 704][Class loss: 0.122390396893024]\n",
      "[Iter 705][Class loss: 0.121919743716717]\n",
      "[Iter 706][Class loss: 0.123370029032230]\n",
      "[Iter 707][Class loss: 0.122139818966389]\n",
      "[Iter 708][Class loss: 0.121340155601501]\n",
      "[Iter 709][Class loss: 0.122273758053780]\n",
      "[Iter 710][Class loss: 0.121535025537014]\n",
      "[Iter 711][Class loss: 0.121074482798576]\n",
      "[Iter 712][Class loss: 0.121653743088245]\n",
      "[Iter 713][Class loss: 0.121176093816757]\n",
      "[Iter 714][Class loss: 0.120948016643524]\n",
      "[Iter 715][Class loss: 0.121310077607632]\n",
      "[Iter 716][Class loss: 0.120947740972042]\n",
      "[Iter 717][Class loss: 0.120709106326103]\n",
      "[Iter 718][Class loss: 0.120802566409111]\n",
      "[Iter 719][Class loss: 0.120669230818748]\n",
      "[Iter 720][Class loss: 0.120548844337463]\n",
      "[Iter 721][Class loss: 0.120570838451385]\n",
      "[Iter 722][Class loss: 0.120439149439335]\n",
      "[Iter 723][Class loss: 0.120301946997643]\n",
      "[Iter 724][Class loss: 0.120305843651295]\n",
      "[Iter 725][Class loss: 0.120222270488739]\n",
      "[Iter 726][Class loss: 0.120110332965851]\n",
      "[Iter 727][Class loss: 0.120103351771832]\n",
      "[Iter 728][Class loss: 0.120033785700798]\n",
      "[Iter 729][Class loss: 0.119891375303268]\n",
      "[Iter 730][Class loss: 0.119857750833035]\n",
      "[Iter 731][Class loss: 0.119826935231686]\n",
      "[Iter 732][Class loss: 0.119755089282990]\n",
      "[Iter 733][Class loss: 0.119685925543308]\n",
      "[Iter 734][Class loss: 0.119655989110470]\n",
      "[Iter 735][Class loss: 0.119604691863060]\n",
      "[Iter 736][Class loss: 0.119549244642258]\n",
      "[Iter 737][Class loss: 0.119465753436089]\n",
      "[Iter 738][Class loss: 0.119736567139626]\n",
      "[Iter 739][Class loss: 0.119832456111908]\n",
      "[Iter 740][Class loss: 0.119516707956791]\n",
      "[Iter 741][Class loss: 0.119236454367638]\n",
      "[Iter 742][Class loss: 0.119387209415436]\n",
      "[Iter 743][Class loss: 0.119535773992538]\n",
      "[Iter 744][Class loss: 0.119309358298779]\n",
      "[Iter 745][Class loss: 0.119074597954750]\n",
      "[Iter 746][Class loss: 0.119106136262417]\n",
      "[Iter 747][Class loss: 0.119216337800026]\n",
      "[Iter 748][Class loss: 0.119133323431015]\n",
      "[Iter 749][Class loss: 0.118917822837830]\n",
      "[Iter 750][Class loss: 0.118891395628452]\n",
      "[Iter 751][Class loss: 0.118940897285938]\n",
      "[Iter 752][Class loss: 0.118938237428665]\n",
      "[Iter 753][Class loss: 0.118807122111320]\n",
      "[Iter 754][Class loss: 0.118497207760811]\n",
      "[Iter 755][Class loss: 0.118791639804840]\n",
      "[Iter 756][Class loss: 0.119050346314907]\n",
      "[Iter 757][Class loss: 0.118890352547169]\n",
      "[Iter 758][Class loss: 0.118536472320557]\n",
      "[Iter 759][Class loss: 0.118142090737820]\n",
      "[Iter 760][Class loss: 0.118019737303257]\n",
      "[Iter 761][Class loss: 0.118222080171108]\n",
      "[Iter 762][Class loss: 0.118441149592400]\n",
      "[Iter 763][Class loss: 0.118460297584534]\n",
      "[Iter 764][Class loss: 0.118321694433689]\n",
      "[Iter 765][Class loss: 0.117963142693043]\n",
      "[Iter 766][Class loss: 0.117714263498783]\n",
      "[Iter 767][Class loss: 0.117657713592052]\n",
      "[Iter 768][Class loss: 0.117661260068417]\n",
      "[Iter 769][Class loss: 0.117595061659813]\n",
      "[Iter 770][Class loss: 0.117402367293835]\n",
      "[Iter 771][Class loss: 0.117294855415821]\n",
      "[Iter 772][Class loss: 0.117247290909290]\n",
      "[Iter 773][Class loss: 0.117245726287365]\n",
      "[Iter 774][Class loss: 0.117173433303833]\n",
      "[Iter 775][Class loss: 0.117216087877750]\n",
      "[Iter 776][Class loss: 0.116942316293716]\n",
      "[Iter 777][Class loss: 0.116830877959728]\n",
      "[Iter 778][Class loss: 0.116776511073112]\n",
      "[Iter 779][Class loss: 0.116740845143795]\n",
      "[Iter 780][Class loss: 0.116681374609470]\n",
      "[Iter 781][Class loss: 0.116613626480103]\n",
      "[Iter 782][Class loss: 0.116508230566978]\n",
      "[Iter 783][Class loss: 0.116396754980087]\n",
      "[Iter 784][Class loss: 0.116345599293709]\n",
      "[Iter 785][Class loss: 0.116261504590511]\n",
      "[Iter 786][Class loss: 0.116372354328632]\n",
      "[Iter 787][Class loss: 0.116504751145840]\n",
      "[Iter 788][Class loss: 0.116948843002319]\n",
      "[Iter 789][Class loss: 0.117785602807999]\n",
      "[Iter 790][Class loss: 0.119180917739868]\n",
      "[Iter 791][Class loss: 0.120189949870110]\n",
      "[Iter 792][Class loss: 0.121192671358585]\n",
      "[Iter 793][Class loss: 0.120381087064743]\n",
      "[Iter 794][Class loss: 0.118987701833248]\n",
      "[Iter 795][Class loss: 0.116462476551533]\n",
      "[Iter 796][Class loss: 0.115069344639778]\n",
      "[Iter 797][Class loss: 0.115207500755787]\n",
      "[Iter 798][Class loss: 0.116165041923523]\n",
      "[Iter 799][Class loss: 0.117206171154976]\n",
      "[Iter 800][Class loss: 0.118170090019703]\n",
      "[Iter 801][Class loss: 0.119401149451733]\n",
      "[Iter 802][Class loss: 0.118063084781170]\n",
      "[Iter 803][Class loss: 0.116230562329292]\n",
      "[Iter 804][Class loss: 0.114361114799976]\n",
      "[Iter 805][Class loss: 0.114074252545834]\n",
      "[Iter 806][Class loss: 0.114991769194603]\n",
      "[Iter 807][Class loss: 0.116453044116497]\n",
      "[Iter 808][Class loss: 0.118575796484947]\n",
      "[Iter 809][Class loss: 0.120811626315117]\n",
      "[Iter 810][Class loss: 0.122970104217529]\n",
      "[Iter 811][Class loss: 0.119172386825085]\n",
      "[Iter 812][Class loss: 0.115270063281059]\n",
      "[Iter 813][Class loss: 0.113210529088974]\n",
      "[Iter 814][Class loss: 0.114386469125748]\n",
      "[Iter 815][Class loss: 0.117500543594360]\n",
      "[Iter 816][Class loss: 0.119218565523624]\n",
      "[Iter 817][Class loss: 0.119960226118565]\n",
      "[Iter 818][Class loss: 0.118060596287251]\n",
      "[Iter 819][Class loss: 0.115059450268745]\n",
      "[Iter 820][Class loss: 0.112428337335587]\n",
      "[Iter 821][Class loss: 0.112809963524342]\n",
      "[Iter 822][Class loss: 0.114404872059822]\n",
      "[Iter 823][Class loss: 0.113868169486523]\n",
      "[Iter 824][Class loss: 0.112343080341816]\n",
      "[Iter 825][Class loss: 0.111053116619587]\n",
      "[Iter 826][Class loss: 0.111263781785965]\n",
      "[Iter 827][Class loss: 0.112287260591984]\n",
      "[Iter 828][Class loss: 0.112677328288555]\n",
      "[Iter 829][Class loss: 0.112386740744114]\n",
      "[Iter 830][Class loss: 0.111245639622211]\n",
      "[Iter 831][Class loss: 0.110019505023956]\n",
      "[Iter 832][Class loss: 0.109256960451603]\n",
      "[Iter 833][Class loss: 0.109212271869183]\n",
      "[Iter 834][Class loss: 0.109495297074318]\n",
      "[Iter 835][Class loss: 0.109416142106056]\n",
      "[Iter 836][Class loss: 0.108756065368652]\n",
      "[Iter 837][Class loss: 0.107808053493500]\n",
      "[Iter 838][Class loss: 0.107335343956947]\n",
      "[Iter 839][Class loss: 0.107242539525032]\n",
      "[Iter 840][Class loss: 0.107268065214157]\n",
      "[Iter 841][Class loss: 0.107674859464169]\n",
      "[Iter 842][Class loss: 0.109867937862873]\n",
      "[Iter 843][Class loss: 0.113242253661156]\n",
      "[Iter 844][Class loss: 0.113040000200272]\n",
      "[Iter 845][Class loss: 0.112108021974564]\n",
      "[Iter 846][Class loss: 0.109332941472530]\n",
      "[Iter 847][Class loss: 0.107815891504288]\n",
      "[Iter 848][Class loss: 0.107921041548252]\n",
      "[Iter 849][Class loss: 0.109149232506752]\n",
      "[Iter 850][Class loss: 0.110089108347893]\n",
      "[Iter 851][Class loss: 0.109954871237278]\n",
      "[Iter 852][Class loss: 0.105726726353168]\n",
      "[Iter 853][Class loss: 0.104175060987473]\n",
      "[Iter 854][Class loss: 0.102396361529827]\n",
      "[Iter 855][Class loss: 0.101678445935249]\n",
      "[Iter 856][Class loss: 0.102522253990173]\n",
      "[Iter 857][Class loss: 0.106423817574978]\n",
      "[Iter 858][Class loss: 0.117073364555836]\n",
      "[Iter 859][Class loss: 0.137303933501244]\n",
      "[Iter 860][Class loss: 0.123221628367901]\n",
      "[Iter 861][Class loss: 0.110575899481773]\n",
      "[Iter 862][Class loss: 0.098086059093475]\n",
      "[Iter 863][Class loss: 0.103878431022167]\n",
      "[Iter 864][Class loss: 0.113080888986588]\n",
      "[Iter 865][Class loss: 0.100790180265903]\n",
      "[Iter 866][Class loss: 0.091820247471333]\n",
      "[Iter 867][Class loss: 0.093111708760262]\n",
      "[Iter 868][Class loss: 0.100713469088078]\n",
      "[Iter 869][Class loss: 0.103350624442101]\n",
      "[Iter 870][Class loss: 0.089134164154530]\n",
      "[Iter 871][Class loss: 0.087799988687038]\n",
      "[Iter 872][Class loss: 0.096927173435688]\n",
      "[Iter 873][Class loss: 0.085948191583157]\n",
      "[Iter 874][Class loss: 0.077714726328850]\n",
      "[Iter 875][Class loss: 0.085464477539062]\n",
      "[Iter 876][Class loss: 0.107039690017700]\n",
      "[Iter 877][Class loss: 0.115886092185974]\n",
      "[Iter 878][Class loss: 0.074220687150955]\n",
      "[Iter 879][Class loss: 0.105686880648136]\n",
      "[Iter 880][Class loss: 0.142490878701210]\n",
      "[Iter 881][Class loss: 0.073828972876072]\n",
      "[Iter 882][Class loss: 0.079955212771893]\n",
      "[Iter 883][Class loss: 0.131345018744469]\n",
      "[Iter 884][Class loss: 0.076276443898678]\n",
      "[Iter 885][Class loss: 0.118705645203590]\n",
      "[Iter 886][Class loss: 0.054256271570921]\n",
      "[Iter 887][Class loss: 0.059597119688988]\n",
      "[Iter 888][Class loss: 0.077651701867580]\n",
      "[Iter 889][Class loss: 0.045732483267784]\n",
      "[Iter 890][Class loss: 0.035797271877527]\n",
      "[Iter 891][Class loss: 0.047796096652746]\n",
      "[Iter 892][Class loss: 0.039766419678926]\n",
      "[Iter 893][Class loss: 0.025541616603732]\n",
      "[Iter 894][Class loss: 0.026553766801953]\n",
      "[Iter 895][Class loss: 0.026560951024294]\n",
      "[Iter 896][Class loss: 0.018401704728603]\n",
      "[Iter 897][Class loss: 0.020361915230751]\n",
      "[Iter 898][Class loss: 0.011666619218886]\n",
      "[Iter 899][Class loss: 0.015156677924097]\n",
      "[Iter 900][Class loss: 0.010181337594986]\n",
      "[Iter 901][Class loss: 0.007991114631295]\n",
      "[Iter 902][Class loss: 0.008753549307585]\n",
      "[Iter 903][Class loss: 0.007608653511852]\n",
      "[Iter 904][Class loss: 0.005929911043495]\n",
      "[Iter 905][Class loss: 0.005552460439503]\n",
      "[Iter 906][Class loss: 0.005162581801414]\n",
      "[Iter 907][Class loss: 0.004045231733471]\n",
      "[Iter 908][Class loss: 0.003318700939417]\n",
      "[Iter 909][Class loss: 0.003101806389168]\n",
      "[Iter 910][Class loss: 0.002919648308307]\n",
      "[Iter 911][Class loss: 0.002494432730600]\n",
      "[Iter 912][Class loss: 0.001975812250748]\n",
      "[Iter 913][Class loss: 0.001684000249952]\n",
      "[Iter 914][Class loss: 0.001543542719446]\n",
      "[Iter 915][Class loss: 0.001481636310928]\n",
      "[Iter 916][Class loss: 0.001385399024002]\n",
      "[Iter 917][Class loss: 0.001242475584149]\n",
      "[Iter 918][Class loss: 0.001092348247766]\n",
      "[Iter 919][Class loss: 0.000973870279267]\n",
      "[Iter 920][Class loss: 0.000899943523109]\n",
      "[Iter 921][Class loss: 0.000858085986692]\n",
      "[Iter 922][Class loss: 0.000821237917989]\n",
      "[Iter 923][Class loss: 0.000774864049163]\n",
      "[Iter 924][Class loss: 0.000720858923160]\n",
      "[Iter 925][Class loss: 0.000664596969727]\n",
      "[Iter 926][Class loss: 0.000614810734987]\n",
      "[Iter 927][Class loss: 0.000580675434321]\n",
      "[Iter 928][Class loss: 0.000560628483072]\n",
      "[Iter 929][Class loss: 0.000545308052097]\n",
      "[Iter 930][Class loss: 0.000523180759046]\n",
      "[Iter 931][Class loss: 0.000494011212140]\n",
      "[Iter 932][Class loss: 0.000467068864964]\n",
      "[Iter 933][Class loss: 0.000445644749561]\n",
      "[Iter 934][Class loss: 0.000429382314906]\n",
      "[Iter 935][Class loss: 0.000416923197918]\n",
      "[Iter 936][Class loss: 0.000406443665270]\n",
      "[Iter 937][Class loss: 0.000397384515963]\n",
      "[Iter 938][Class loss: 0.000388439861126]\n",
      "[Iter 939][Class loss: 0.000379077158868]\n",
      "[Iter 940][Class loss: 0.000369309098460]\n",
      "[Iter 941][Class loss: 0.000359522469807]\n",
      "[Iter 942][Class loss: 0.000350638350938]\n",
      "[Iter 943][Class loss: 0.000342893152265]\n",
      "[Iter 944][Class loss: 0.000336396187777]\n",
      "[Iter 945][Class loss: 0.000330337526975]\n",
      "[Iter 946][Class loss: 0.000324433378410]\n",
      "[Iter 947][Class loss: 0.000318530073855]\n",
      "[Iter 948][Class loss: 0.000312232121360]\n",
      "[Iter 949][Class loss: 0.000305757916067]\n",
      "[Iter 950][Class loss: 0.000299512961647]\n",
      "[Iter 951][Class loss: 0.000293726770906]\n",
      "[Iter 952][Class loss: 0.000288657203782]\n",
      "[Iter 953][Class loss: 0.000284004898276]\n",
      "[Iter 954][Class loss: 0.000279415166005]\n",
      "[Iter 955][Class loss: 0.000275228638202]\n",
      "[Iter 956][Class loss: 0.000271154218353]\n",
      "[Iter 957][Class loss: 0.000267321476713]\n",
      "[Iter 958][Class loss: 0.000263518246356]\n",
      "[Iter 959][Class loss: 0.000259834690951]\n",
      "[Iter 960][Class loss: 0.000256389117567]\n",
      "[Iter 961][Class loss: 0.000253560952842]\n",
      "[Iter 962][Class loss: 0.000250644516200]\n",
      "[Iter 963][Class loss: 0.000247594696702]\n",
      "[Iter 964][Class loss: 0.000244512950303]\n",
      "[Iter 965][Class loss: 0.000241560381255]\n",
      "[Iter 966][Class loss: 0.000238650711253]\n",
      "[Iter 967][Class loss: 0.000235952902585]\n",
      "[Iter 968][Class loss: 0.000233422062593]\n",
      "[Iter 969][Class loss: 0.000230953388382]\n",
      "[Iter 970][Class loss: 0.000228495831834]\n",
      "[Iter 971][Class loss: 0.000226208590902]\n",
      "[Iter 972][Class loss: 0.000223946815822]\n",
      "[Iter 973][Class loss: 0.000221895839786]\n",
      "[Iter 974][Class loss: 0.000219855370233]\n",
      "[Iter 975][Class loss: 0.000217847205931]\n",
      "[Iter 976][Class loss: 0.000215941981878]\n",
      "[Iter 977][Class loss: 0.000214003928704]\n",
      "[Iter 978][Class loss: 0.000211985097849]\n",
      "[Iter 979][Class loss: 0.000209943915252]\n",
      "[Iter 980][Class loss: 0.000207969103940]\n",
      "[Iter 981][Class loss: 0.000206122465897]\n",
      "[Iter 982][Class loss: 0.000204382842639]\n",
      "[Iter 983][Class loss: 0.000202649258426]\n",
      "[Iter 984][Class loss: 0.000200976821361]\n",
      "[Iter 985][Class loss: 0.000199379646801]\n",
      "[Iter 986][Class loss: 0.000197756293346]\n",
      "[Iter 987][Class loss: 0.000196082401089]\n",
      "[Iter 988][Class loss: 0.000194555323105]\n",
      "[Iter 989][Class loss: 0.000192980369320]\n",
      "[Iter 990][Class loss: 0.000191400788026]\n",
      "[Iter 991][Class loss: 0.000189998871065]\n",
      "[Iter 992][Class loss: 0.000188568752492]\n",
      "[Iter 993][Class loss: 0.000187167810509]\n",
      "[Iter 994][Class loss: 0.000185820186744]\n",
      "[Iter 995][Class loss: 0.000184518576134]\n",
      "[Iter 996][Class loss: 0.000183167285286]\n",
      "[Iter 997][Class loss: 0.000181821786100]\n",
      "[Iter 998][Class loss: 0.000180493632797]\n",
      "[Iter 999][Class loss: 0.000179209309863]\n",
      "[Iter 1000][Class loss: 0.000177953828825]\n",
      "[Iter 1001][Class loss: 0.000176779431058]\n",
      "[Iter 1002][Class loss: 0.000175602617674]\n",
      "[Iter 1003][Class loss: 0.000174401706317]\n",
      "[Iter 1004][Class loss: 0.000173286171048]\n",
      "[Iter 1005][Class loss: 0.000172122207005]\n",
      "[Iter 1006][Class loss: 0.000170960891410]\n",
      "[Iter 1007][Class loss: 0.000169820996234]\n",
      "[Iter 1008][Class loss: 0.000168677826878]\n",
      "[Iter 1009][Class loss: 0.000167556601809]\n",
      "[Iter 1010][Class loss: 0.000166425917996]\n",
      "[Iter 1011][Class loss: 0.000165379431564]\n",
      "[Iter 1012][Class loss: 0.000164391152794]\n",
      "[Iter 1013][Class loss: 0.000163329706993]\n",
      "[Iter 1014][Class loss: 0.000162308904692]\n",
      "[Iter 1015][Class loss: 0.000161289717653]\n",
      "[Iter 1016][Class loss: 0.000160229275934]\n",
      "[Iter 1017][Class loss: 0.000159252202138]\n",
      "[Iter 1018][Class loss: 0.000158258902957]\n",
      "[Iter 1019][Class loss: 0.000157252725330]\n",
      "[Iter 1020][Class loss: 0.000156298221555]\n",
      "[Iter 1021][Class loss: 0.000155318673933]\n",
      "[Iter 1022][Class loss: 0.000154375346028]\n",
      "[Iter 1023][Class loss: 0.000153396264068]\n",
      "[Iter 1024][Class loss: 0.000152430584421]\n",
      "[Iter 1025][Class loss: 0.000151506043039]\n",
      "[Iter 1026][Class loss: 0.000150573454448]\n",
      "[Iter 1027][Class loss: 0.000149677973241]\n",
      "[Iter 1028][Class loss: 0.000148762846948]\n",
      "[Iter 1029][Class loss: 0.000147896585986]\n",
      "[Iter 1030][Class loss: 0.000147043901961]\n",
      "[Iter 1031][Class loss: 0.000146172838868]\n",
      "[Iter 1032][Class loss: 0.000145294936374]\n",
      "[Iter 1033][Class loss: 0.000144425517647]\n",
      "[Iter 1034][Class loss: 0.000143590688822]\n",
      "[Iter 1035][Class loss: 0.000142790129757]\n",
      "[Iter 1036][Class loss: 0.000141948126839]\n",
      "[Iter 1037][Class loss: 0.000141156473546]\n",
      "[Iter 1038][Class loss: 0.000140349395224]\n",
      "[Iter 1039][Class loss: 0.000139553681947]\n",
      "[Iter 1040][Class loss: 0.000138766743476]\n",
      "[Iter 1041][Class loss: 0.000137986382470]\n",
      "[Iter 1042][Class loss: 0.000137231982080]\n",
      "[Iter 1043][Class loss: 0.000136438306072]\n",
      "[Iter 1044][Class loss: 0.000135706461151]\n",
      "[Iter 1045][Class loss: 0.000134960675496]\n",
      "[Iter 1046][Class loss: 0.000134257672471]\n",
      "[Iter 1047][Class loss: 0.000133539797389]\n",
      "[Iter 1048][Class loss: 0.000132816247060]\n",
      "[Iter 1049][Class loss: 0.000132092362037]\n",
      "[Iter 1050][Class loss: 0.000131397013320]\n",
      "[Iter 1051][Class loss: 0.000130716594867]\n",
      "[Iter 1052][Class loss: 0.000130044325488]\n",
      "[Iter 1053][Class loss: 0.000129367617774]\n",
      "[Iter 1054][Class loss: 0.000128702289658]\n",
      "[Iter 1055][Class loss: 0.000128047395265]\n",
      "[Iter 1056][Class loss: 0.000127399631310]\n",
      "[Iter 1057][Class loss: 0.000126757629914]\n",
      "[Iter 1058][Class loss: 0.000126114871819]\n",
      "[Iter 1059][Class loss: 0.000125471997308]\n",
      "[Iter 1060][Class loss: 0.000124861719087]\n",
      "[Iter 1061][Class loss: 0.000124248908833]\n",
      "[Iter 1062][Class loss: 0.000123625737615]\n",
      "[Iter 1063][Class loss: 0.000122985700727]\n",
      "[Iter 1064][Class loss: 0.000122397192172]\n",
      "[Iter 1065][Class loss: 0.000121799232147]\n",
      "[Iter 1066][Class loss: 0.000121220080473]\n",
      "[Iter 1067][Class loss: 0.000120624819829]\n",
      "[Iter 1068][Class loss: 0.000120038588648]\n",
      "[Iter 1069][Class loss: 0.000119457894471]\n",
      "[Iter 1070][Class loss: 0.000118906820717]\n",
      "[Iter 1071][Class loss: 0.000118351978017]\n",
      "[Iter 1072][Class loss: 0.000117784962640]\n",
      "[Iter 1073][Class loss: 0.000117219140520]\n",
      "[Iter 1074][Class loss: 0.000116693467135]\n",
      "[Iter 1075][Class loss: 0.000116152339615]\n",
      "[Iter 1076][Class loss: 0.000115636081318]\n",
      "[Iter 1077][Class loss: 0.000115091053885]\n",
      "[Iter 1078][Class loss: 0.000114589958685]\n",
      "[Iter 1079][Class loss: 0.000114049922558]\n",
      "[Iter 1080][Class loss: 0.000113515445264]\n",
      "[Iter 1081][Class loss: 0.000112995803647]\n",
      "[Iter 1082][Class loss: 0.000112509202154]\n",
      "[Iter 1083][Class loss: 0.000111996778287]\n",
      "[Iter 1084][Class loss: 0.000111492918222]\n",
      "[Iter 1085][Class loss: 0.000111007619125]\n",
      "[Iter 1086][Class loss: 0.000110497807327]\n",
      "[Iter 1087][Class loss: 0.000110019238491]\n",
      "[Iter 1088][Class loss: 0.000109525404696]\n",
      "[Iter 1089][Class loss: 0.000109081462142]\n",
      "[Iter 1090][Class loss: 0.000108583510155]\n",
      "[Iter 1091][Class loss: 0.000108124986582]\n",
      "[Iter 1092][Class loss: 0.000107653177110]\n",
      "[Iter 1093][Class loss: 0.000107190404378]\n",
      "[Iter 1094][Class loss: 0.000106739622424]\n",
      "[Iter 1095][Class loss: 0.000106287283415]\n",
      "[Iter 1096][Class loss: 0.000105836355942]\n",
      "[Iter 1097][Class loss: 0.000105377912405]\n",
      "[Iter 1098][Class loss: 0.000104940576421]\n",
      "[Iter 1099][Class loss: 0.000104510771052]\n",
      "[Iter 1100][Class loss: 0.000104078200820]\n",
      "[Iter 1101][Class loss: 0.000103652084363]\n",
      "[Iter 1102][Class loss: 0.000103223646875]\n",
      "[Iter 1103][Class loss: 0.000102801110188]\n",
      "[Iter 1104][Class loss: 0.000102394413261]\n",
      "[Iter 1105][Class loss: 0.000101968238596]\n",
      "[Iter 1106][Class loss: 0.000101556463051]\n",
      "[Iter 1107][Class loss: 0.000101135330624]\n",
      "[Iter 1108][Class loss: 0.000100782854133]\n",
      "[Iter 1109][Class loss: 0.000100352030131]\n",
      "[Iter 1110][Class loss: 0.000099946875707]\n",
      "[Iter 1111][Class loss: 0.000099560398667]\n",
      "[Iter 1112][Class loss: 0.000099170982139]\n",
      "[Iter 1113][Class loss: 0.000098773743957]\n",
      "[Iter 1114][Class loss: 0.000098410499049]\n",
      "[Iter 1115][Class loss: 0.000098014075775]\n",
      "[Iter 1116][Class loss: 0.000097653042758]\n",
      "[Iter 1117][Class loss: 0.000097280018963]\n",
      "[Iter 1118][Class loss: 0.000096877331089]\n",
      "[Iter 1119][Class loss: 0.000096508279967]\n",
      "[Iter 1120][Class loss: 0.000096136216598]\n",
      "[Iter 1121][Class loss: 0.000095750132459]\n",
      "[Iter 1122][Class loss: 0.000095404553576]\n",
      "[Iter 1123][Class loss: 0.000095049676020]\n",
      "[Iter 1124][Class loss: 0.000094681272458]\n",
      "[Iter 1125][Class loss: 0.000094332965091]\n",
      "[Iter 1126][Class loss: 0.000093971670140]\n",
      "[Iter 1127][Class loss: 0.000093625509180]\n",
      "[Iter 1128][Class loss: 0.000093281851150]\n",
      "[Iter 1129][Class loss: 0.000092940405011]\n",
      "[Iter 1130][Class loss: 0.000092594287707]\n",
      "[Iter 1131][Class loss: 0.000092265974672]\n",
      "[Iter 1132][Class loss: 0.000091960537247]\n",
      "[Iter 1133][Class loss: 0.000091646346846]\n",
      "[Iter 1134][Class loss: 0.000091311987489]\n",
      "[Iter 1135][Class loss: 0.000091010675533]\n",
      "[Iter 1136][Class loss: 0.000090706998890]\n",
      "[Iter 1137][Class loss: 0.000090396672022]\n",
      "[Iter 1138][Class loss: 0.000090094050393]\n",
      "[Iter 1139][Class loss: 0.000089788525656]\n",
      "[Iter 1140][Class loss: 0.000089473811386]\n",
      "[Iter 1141][Class loss: 0.000089181179646]\n",
      "[Iter 1142][Class loss: 0.000088872497145]\n",
      "[Iter 1143][Class loss: 0.000088567081548]\n",
      "[Iter 1144][Class loss: 0.000088275730377]\n",
      "[Iter 1145][Class loss: 0.000087969507149]\n",
      "[Iter 1146][Class loss: 0.000087672662630]\n",
      "[Iter 1147][Class loss: 0.000087389817054]\n",
      "[Iter 1148][Class loss: 0.000087100750534]\n",
      "[Iter 1149][Class loss: 0.000086802960141]\n",
      "[Iter 1150][Class loss: 0.000086508778622]\n",
      "[Iter 1151][Class loss: 0.000086231593741]\n",
      "[Iter 1152][Class loss: 0.000085963365564]\n",
      "[Iter 1153][Class loss: 0.000085680760094]\n",
      "[Iter 1154][Class loss: 0.000085394880443]\n",
      "[Iter 1155][Class loss: 0.000085135980044]\n",
      "[Iter 1156][Class loss: 0.000084865343524]\n",
      "[Iter 1157][Class loss: 0.000084610102931]\n",
      "[Iter 1158][Class loss: 0.000084317463916]\n",
      "[Iter 1159][Class loss: 0.000084051876911]\n",
      "[Iter 1160][Class loss: 0.000083814586105]\n",
      "[Iter 1161][Class loss: 0.000083535545855]\n",
      "[Iter 1162][Class loss: 0.000083280334366]\n",
      "[Iter 1163][Class loss: 0.000083031307440]\n",
      "[Iter 1164][Class loss: 0.000082767292042]\n",
      "[Iter 1165][Class loss: 0.000082507162006]\n",
      "[Iter 1166][Class loss: 0.000082256752648]\n",
      "[Iter 1167][Class loss: 0.000082030666817]\n",
      "[Iter 1168][Class loss: 0.000081744808995]\n",
      "[Iter 1169][Class loss: 0.000081504913396]\n",
      "[Iter 1170][Class loss: 0.000081265367044]\n",
      "[Iter 1171][Class loss: 0.000081004065578]\n",
      "[Iter 1172][Class loss: 0.000080763173173]\n",
      "[Iter 1173][Class loss: 0.000080527599494]\n",
      "[Iter 1174][Class loss: 0.000080283774878]\n",
      "[Iter 1175][Class loss: 0.000080053890997]\n",
      "[Iter 1176][Class loss: 0.000079808218288]\n",
      "[Iter 1177][Class loss: 0.000079577286670]\n",
      "[Iter 1178][Class loss: 0.000079332254245]\n",
      "[Iter 1179][Class loss: 0.000079099743743]\n",
      "[Iter 1180][Class loss: 0.000078872421000]\n",
      "[Iter 1181][Class loss: 0.000078643715824]\n",
      "[Iter 1182][Class loss: 0.000078422126535]\n",
      "[Iter 1183][Class loss: 0.000078185679740]\n",
      "[Iter 1184][Class loss: 0.000077957418398]\n",
      "[Iter 1185][Class loss: 0.000077748351032]\n",
      "[Iter 1186][Class loss: 0.000077513665019]\n",
      "[Iter 1187][Class loss: 0.000077303622675]\n",
      "[Iter 1188][Class loss: 0.000077084623626]\n",
      "[Iter 1189][Class loss: 0.000076853299106]\n",
      "[Iter 1190][Class loss: 0.000076628901297]\n",
      "[Iter 1191][Class loss: 0.000076411139162]\n",
      "[Iter 1192][Class loss: 0.000076191339758]\n",
      "[Iter 1193][Class loss: 0.000075985648436]\n",
      "[Iter 1194][Class loss: 0.000075781848864]\n",
      "[Iter 1195][Class loss: 0.000075561445556]\n",
      "[Iter 1196][Class loss: 0.000075345182267]\n",
      "[Iter 1197][Class loss: 0.000075135903899]\n",
      "[Iter 1198][Class loss: 0.000074932053394]\n",
      "[Iter 1199][Class loss: 0.000074729556218]\n",
      "[Iter 1200][Class loss: 0.000074521987699]\n",
      "[Iter 1201][Class loss: 0.000074309173215]\n",
      "[Iter 1202][Class loss: 0.000074131727160]\n",
      "[Iter 1203][Class loss: 0.000073904288001]\n",
      "[Iter 1204][Class loss: 0.000073717616033]\n",
      "[Iter 1205][Class loss: 0.000073524264735]\n",
      "[Iter 1206][Class loss: 0.000073317365604]\n",
      "[Iter 1207][Class loss: 0.000073134055128]\n",
      "[Iter 1208][Class loss: 0.000072920447565]\n",
      "[Iter 1209][Class loss: 0.000072720838943]\n",
      "[Iter 1210][Class loss: 0.000072527429438]\n",
      "[Iter 1211][Class loss: 0.000072329101386]\n",
      "[Iter 1212][Class loss: 0.000072140559496]\n",
      "[Iter 1213][Class loss: 0.000071935966844]\n",
      "[Iter 1214][Class loss: 0.000071755741374]\n",
      "[Iter 1215][Class loss: 0.000071566340921]\n",
      "[Iter 1216][Class loss: 0.000071376663982]\n",
      "[Iter 1217][Class loss: 0.000071182788815]\n",
      "[Iter 1218][Class loss: 0.000071004193160]\n",
      "[Iter 1219][Class loss: 0.000070813402999]\n",
      "[Iter 1220][Class loss: 0.000070622809289]\n",
      "[Iter 1221][Class loss: 0.000070446680184]\n",
      "[Iter 1222][Class loss: 0.000070263486123]\n",
      "[Iter 1223][Class loss: 0.000070080575824]\n",
      "[Iter 1224][Class loss: 0.000069896035711]\n",
      "[Iter 1225][Class loss: 0.000069723319029]\n",
      "[Iter 1226][Class loss: 0.000069545363658]\n",
      "[Iter 1227][Class loss: 0.000069366535172]\n",
      "[Iter 1228][Class loss: 0.000069186222390]\n",
      "[Iter 1229][Class loss: 0.000069024645200]\n",
      "[Iter 1230][Class loss: 0.000068836685386]\n",
      "[Iter 1231][Class loss: 0.000068667344749]\n",
      "[Iter 1232][Class loss: 0.000068492656283]\n",
      "[Iter 1233][Class loss: 0.000068320361606]\n",
      "[Iter 1234][Class loss: 0.000068140914664]\n",
      "[Iter 1235][Class loss: 0.000067971523094]\n",
      "[Iter 1236][Class loss: 0.000067803615821]\n",
      "[Iter 1237][Class loss: 0.000067639848567]\n",
      "[Iter 1238][Class loss: 0.000067470784415]\n",
      "[Iter 1239][Class loss: 0.000067296277848]\n",
      "[Iter 1240][Class loss: 0.000067137269070]\n",
      "[Iter 1241][Class loss: 0.000066984706791]\n",
      "[Iter 1242][Class loss: 0.000066826898546]\n",
      "[Iter 1243][Class loss: 0.000066652646638]\n",
      "[Iter 1244][Class loss: 0.000066492757469]\n",
      "[Iter 1245][Class loss: 0.000066316759330]\n",
      "[Iter 1246][Class loss: 0.000066172062361]\n",
      "[Iter 1247][Class loss: 0.000066016844357]\n",
      "[Iter 1248][Class loss: 0.000065850843384]\n",
      "[Iter 1249][Class loss: 0.000065688640461]\n",
      "[Iter 1250][Class loss: 0.000065534026362]\n",
      "[Iter 1251][Class loss: 0.000065374420956]\n",
      "[Iter 1252][Class loss: 0.000065217449446]\n",
      "[Iter 1253][Class loss: 0.000065066873503]\n",
      "[Iter 1254][Class loss: 0.000064902233134]\n",
      "[Iter 1255][Class loss: 0.000064763065893]\n",
      "[Iter 1256][Class loss: 0.000064596322773]\n",
      "[Iter 1257][Class loss: 0.000064441643190]\n",
      "[Iter 1258][Class loss: 0.000064291263698]\n",
      "[Iter 1259][Class loss: 0.000064138512244]\n",
      "[Iter 1260][Class loss: 0.000063993051299]\n",
      "[Iter 1261][Class loss: 0.000063845553086]\n",
      "[Iter 1262][Class loss: 0.000063689687522]\n",
      "[Iter 1263][Class loss: 0.000063553961809]\n",
      "[Iter 1264][Class loss: 0.000063392704760]\n",
      "[Iter 1265][Class loss: 0.000063253741246]\n",
      "[Iter 1266][Class loss: 0.000063101448177]\n",
      "[Iter 1267][Class loss: 0.000062955819885]\n",
      "[Iter 1268][Class loss: 0.000062814040575]\n",
      "[Iter 1269][Class loss: 0.000062669263571]\n",
      "[Iter 1270][Class loss: 0.000062528291892]\n",
      "[Iter 1271][Class loss: 0.000062384009652]\n",
      "[Iter 1272][Class loss: 0.000062241982960]\n",
      "[Iter 1273][Class loss: 0.000062100181822]\n",
      "[Iter 1274][Class loss: 0.000061960425228]\n",
      "[Iter 1275][Class loss: 0.000061824262957]\n",
      "[Iter 1276][Class loss: 0.000061682469095]\n",
      "[Iter 1277][Class loss: 0.000061545171775]\n",
      "[Iter 1278][Class loss: 0.000061407758039]\n",
      "[Iter 1279][Class loss: 0.000061276106862]\n",
      "[Iter 1280][Class loss: 0.000061140643083]\n",
      "[Iter 1281][Class loss: 0.000061003069277]\n",
      "[Iter 1282][Class loss: 0.000060868023866]\n",
      "[Iter 1283][Class loss: 0.000060729238612]\n",
      "[Iter 1284][Class loss: 0.000060593374656]\n",
      "[Iter 1285][Class loss: 0.000060464415583]\n",
      "[Iter 1286][Class loss: 0.000060332084104]\n",
      "[Iter 1287][Class loss: 0.000060197526182]\n",
      "[Iter 1288][Class loss: 0.000060069043684]\n",
      "[Iter 1289][Class loss: 0.000059927100665]\n",
      "[Iter 1290][Class loss: 0.000059798523580]\n",
      "[Iter 1291][Class loss: 0.000059673777287]\n",
      "[Iter 1292][Class loss: 0.000059545833210]\n",
      "[Iter 1293][Class loss: 0.000059410496760]\n",
      "[Iter 1294][Class loss: 0.000059290927311]\n",
      "[Iter 1295][Class loss: 0.000059157180658]\n",
      "[Iter 1296][Class loss: 0.000059034275182]\n",
      "[Iter 1297][Class loss: 0.000058901598095]\n",
      "[Iter 1298][Class loss: 0.000058774789068]\n",
      "[Iter 1299][Class loss: 0.000058656005422]\n",
      "[Iter 1300][Class loss: 0.000058534948039]\n",
      "[Iter 1301][Class loss: 0.000058402791183]\n",
      "[Iter 1302][Class loss: 0.000058281078964]\n",
      "[Iter 1303][Class loss: 0.000058152618294]\n",
      "[Iter 1304][Class loss: 0.000058034929680]\n",
      "[Iter 1305][Class loss: 0.000057914690842]\n",
      "[Iter 1306][Class loss: 0.000057792407461]\n",
      "[Iter 1307][Class loss: 0.000057673492847]\n",
      "[Iter 1308][Class loss: 0.000057546476455]\n",
      "[Iter 1309][Class loss: 0.000057428318541]\n",
      "[Iter 1310][Class loss: 0.000057307570387]\n",
      "[Iter 1311][Class loss: 0.000057188917708]\n",
      "[Iter 1312][Class loss: 0.000057075940276]\n",
      "[Iter 1313][Class loss: 0.000056956923800]\n",
      "[Iter 1314][Class loss: 0.000056841879996]\n",
      "[Iter 1315][Class loss: 0.000056730321376]\n",
      "[Iter 1316][Class loss: 0.000056606822909]\n",
      "[Iter 1317][Class loss: 0.000056487529946]\n",
      "[Iter 1318][Class loss: 0.000056376211433]\n",
      "[Iter 1319][Class loss: 0.000056267504988]\n",
      "[Iter 1320][Class loss: 0.000056152679463]\n",
      "[Iter 1321][Class loss: 0.000056048284023]\n",
      "[Iter 1322][Class loss: 0.000055928579968]\n",
      "[Iter 1323][Class loss: 0.000055819153204]\n",
      "[Iter 1324][Class loss: 0.000055706248531]\n",
      "[Iter 1325][Class loss: 0.000055594577134]\n",
      "[Iter 1326][Class loss: 0.000055481417803]\n",
      "[Iter 1327][Class loss: 0.000055371521739]\n",
      "[Iter 1328][Class loss: 0.000055263044487]\n",
      "[Iter 1329][Class loss: 0.000055154079746]\n",
      "[Iter 1330][Class loss: 0.000055052398238]\n",
      "[Iter 1331][Class loss: 0.000054941527196]\n",
      "[Iter 1332][Class loss: 0.000054838375945]\n",
      "[Iter 1333][Class loss: 0.000054723452195]\n",
      "[Iter 1334][Class loss: 0.000054626721976]\n",
      "[Iter 1335][Class loss: 0.000054513300711]\n",
      "[Iter 1336][Class loss: 0.000054412161262]\n",
      "[Iter 1337][Class loss: 0.000054300882766]\n",
      "[Iter 1338][Class loss: 0.000054200412706]\n",
      "[Iter 1339][Class loss: 0.000054098938563]\n",
      "[Iter 1340][Class loss: 0.000053989540902]\n",
      "[Iter 1341][Class loss: 0.000053888059483]\n",
      "[Iter 1342][Class loss: 0.000053781663155]\n",
      "[Iter 1343][Class loss: 0.000053675808886]\n",
      "[Iter 1344][Class loss: 0.000053570816817]\n",
      "[Iter 1345][Class loss: 0.000053471070714]\n",
      "[Iter 1346][Class loss: 0.000053369029047]\n",
      "[Iter 1347][Class loss: 0.000053268682677]\n",
      "[Iter 1348][Class loss: 0.000053174400819]\n",
      "[Iter 1349][Class loss: 0.000053065130487]\n",
      "[Iter 1350][Class loss: 0.000052978917665]\n",
      "[Iter 1351][Class loss: 0.000052868937928]\n",
      "[Iter 1352][Class loss: 0.000052776002121]\n",
      "[Iter 1353][Class loss: 0.000052675517509]\n",
      "[Iter 1354][Class loss: 0.000052571551350]\n",
      "[Iter 1355][Class loss: 0.000052472762036]\n",
      "[Iter 1356][Class loss: 0.000052373368817]\n",
      "[Iter 1357][Class loss: 0.000052280989621]\n",
      "[Iter 1358][Class loss: 0.000052182160289]\n",
      "[Iter 1359][Class loss: 0.000052083458286]\n",
      "[Iter 1360][Class loss: 0.000051988467021]\n",
      "[Iter 1361][Class loss: 0.000051894436183]\n",
      "[Iter 1362][Class loss: 0.000051796901971]\n",
      "[Iter 1363][Class loss: 0.000051698407333]\n",
      "[Iter 1364][Class loss: 0.000051604300097]\n",
      "[Iter 1365][Class loss: 0.000051513052313]\n",
      "[Iter 1366][Class loss: 0.000051416176575]\n",
      "[Iter 1367][Class loss: 0.000051324863307]\n",
      "[Iter 1368][Class loss: 0.000051226012147]\n",
      "[Iter 1369][Class loss: 0.000051135561080]\n",
      "[Iter 1370][Class loss: 0.000051042479754]\n",
      "[Iter 1371][Class loss: 0.000050956525229]\n",
      "[Iter 1372][Class loss: 0.000050864338846]\n",
      "[Iter 1373][Class loss: 0.000050777005526]\n",
      "[Iter 1374][Class loss: 0.000050679736887]\n",
      "[Iter 1375][Class loss: 0.000050586237194]\n",
      "[Iter 1376][Class loss: 0.000050496499171]\n",
      "[Iter 1377][Class loss: 0.000050405928050]\n",
      "[Iter 1378][Class loss: 0.000050317103160]\n",
      "[Iter 1379][Class loss: 0.000050225302402]\n",
      "[Iter 1380][Class loss: 0.000050141374231]\n",
      "[Iter 1381][Class loss: 0.000050050988648]\n",
      "[Iter 1382][Class loss: 0.000049963928177]\n",
      "[Iter 1383][Class loss: 0.000049871618103]\n",
      "[Iter 1384][Class loss: 0.000049788770411]\n",
      "[Iter 1385][Class loss: 0.000049696711358]\n",
      "[Iter 1386][Class loss: 0.000049613470765]\n",
      "[Iter 1387][Class loss: 0.000049530106480]\n",
      "[Iter 1388][Class loss: 0.000049443351600]\n",
      "[Iter 1389][Class loss: 0.000049349371693]\n",
      "[Iter 1390][Class loss: 0.000049264202971]\n",
      "[Iter 1391][Class loss: 0.000049177855544]\n",
      "[Iter 1392][Class loss: 0.000049093214329]\n",
      "[Iter 1393][Class loss: 0.000049013226089]\n",
      "[Iter 1394][Class loss: 0.000048927915486]\n",
      "[Iter 1395][Class loss: 0.000048840411182]\n",
      "[Iter 1396][Class loss: 0.000048753925512]\n",
      "[Iter 1397][Class loss: 0.000048675356084]\n",
      "[Iter 1398][Class loss: 0.000048593297834]\n",
      "[Iter 1399][Class loss: 0.000048511734349]\n",
      "[Iter 1400][Class loss: 0.000048423753469]\n",
      "[Iter 1401][Class loss: 0.000048346944823]\n",
      "[Iter 1402][Class loss: 0.000048264006182]\n",
      "[Iter 1403][Class loss: 0.000048181973398]\n",
      "[Iter 1404][Class loss: 0.000048102090659]\n",
      "[Iter 1405][Class loss: 0.000048020658141]\n",
      "[Iter 1406][Class loss: 0.000047935744078]\n",
      "[Iter 1407][Class loss: 0.000047856192396]\n",
      "[Iter 1408][Class loss: 0.000047773472033]\n",
      "[Iter 1409][Class loss: 0.000047691693908]\n",
      "[Iter 1410][Class loss: 0.000047614317737]\n",
      "[Iter 1411][Class loss: 0.000047532961617]\n",
      "[Iter 1412][Class loss: 0.000047457371693]\n",
      "[Iter 1413][Class loss: 0.000047377609008]\n",
      "[Iter 1414][Class loss: 0.000047302823077]\n",
      "[Iter 1415][Class loss: 0.000047220604756]\n",
      "[Iter 1416][Class loss: 0.000047141744290]\n",
      "[Iter 1417][Class loss: 0.000047061410442]\n",
      "[Iter 1418][Class loss: 0.000046988399845]\n",
      "[Iter 1419][Class loss: 0.000046913206461]\n",
      "[Iter 1420][Class loss: 0.000046834629757]\n",
      "[Iter 1421][Class loss: 0.000046760873374]\n",
      "[Iter 1422][Class loss: 0.000046680015657]\n",
      "[Iter 1423][Class loss: 0.000046609660785]\n",
      "[Iter 1424][Class loss: 0.000046536308218]\n",
      "[Iter 1425][Class loss: 0.000046459419536]\n",
      "[Iter 1426][Class loss: 0.000046384011512]\n",
      "[Iter 1427][Class loss: 0.000046307523007]\n",
      "[Iter 1428][Class loss: 0.000046231118176]\n",
      "[Iter 1429][Class loss: 0.000046156543249]\n",
      "[Iter 1430][Class loss: 0.000046083179768]\n",
      "[Iter 1431][Class loss: 0.000046009030484]\n",
      "[Iter 1432][Class loss: 0.000045935317758]\n",
      "[Iter 1433][Class loss: 0.000045861408580]\n",
      "[Iter 1434][Class loss: 0.000045785494876]\n",
      "[Iter 1435][Class loss: 0.000045712200517]\n",
      "[Iter 1436][Class loss: 0.000045642220357]\n",
      "[Iter 1437][Class loss: 0.000045571785449]\n",
      "[Iter 1438][Class loss: 0.000045497617975]\n",
      "[Iter 1439][Class loss: 0.000045426368160]\n",
      "[Iter 1440][Class loss: 0.000045351502195]\n",
      "[Iter 1441][Class loss: 0.000045280332415]\n",
      "[Iter 1442][Class loss: 0.000045207802032]\n",
      "[Iter 1443][Class loss: 0.000045137705456]\n",
      "[Iter 1444][Class loss: 0.000045066124585]\n",
      "[Iter 1445][Class loss: 0.000044997450459]\n",
      "[Iter 1446][Class loss: 0.000044927030103]\n",
      "[Iter 1447][Class loss: 0.000044858450565]\n",
      "[Iter 1448][Class loss: 0.000044782074838]\n",
      "[Iter 1449][Class loss: 0.000044725507905]\n",
      "[Iter 1450][Class loss: 0.000044645112212]\n",
      "[Iter 1451][Class loss: 0.000044583379349]\n",
      "[Iter 1452][Class loss: 0.000044517542847]\n",
      "[Iter 1453][Class loss: 0.000044441800128]\n",
      "[Iter 1454][Class loss: 0.000044373518904]\n",
      "[Iter 1455][Class loss: 0.000044307751523]\n",
      "[Iter 1456][Class loss: 0.000044237589464]\n",
      "[Iter 1457][Class loss: 0.000044174001232]\n",
      "[Iter 1458][Class loss: 0.000044099295337]\n",
      "[Iter 1459][Class loss: 0.000044040774810]\n",
      "[Iter 1460][Class loss: 0.000043966036174]\n",
      "[Iter 1461][Class loss: 0.000043901261961]\n",
      "[Iter 1462][Class loss: 0.000043834133976]\n",
      "[Iter 1463][Class loss: 0.000043764168367]\n",
      "[Iter 1464][Class loss: 0.000043706051656]\n",
      "[Iter 1465][Class loss: 0.000043637581257]\n",
      "[Iter 1466][Class loss: 0.000043570642447]\n",
      "[Iter 1467][Class loss: 0.000043506053771]\n",
      "[Iter 1468][Class loss: 0.000043442028982]\n",
      "[Iter 1469][Class loss: 0.000043373322114]\n",
      "[Iter 1470][Class loss: 0.000043306681619]\n",
      "[Iter 1471][Class loss: 0.000043242693209]\n",
      "[Iter 1472][Class loss: 0.000043175354222]\n",
      "[Iter 1473][Class loss: 0.000043112053390]\n",
      "[Iter 1474][Class loss: 0.000043050640670]\n",
      "[Iter 1475][Class loss: 0.000042986481276]\n",
      "[Iter 1476][Class loss: 0.000042921983550]\n",
      "[Iter 1477][Class loss: 0.000042859224777]\n",
      "[Iter 1478][Class loss: 0.000042797830247]\n",
      "[Iter 1479][Class loss: 0.000042733896407]\n",
      "[Iter 1480][Class loss: 0.000042668452807]\n",
      "[Iter 1481][Class loss: 0.000042606025090]\n",
      "[Iter 1482][Class loss: 0.000042540872528]\n",
      "[Iter 1483][Class loss: 0.000042478703108]\n",
      "[Iter 1484][Class loss: 0.000042418192606]\n",
      "[Iter 1485][Class loss: 0.000042355688493]\n",
      "[Iter 1486][Class loss: 0.000042294024752]\n",
      "[Iter 1487][Class loss: 0.000042240615585]\n",
      "[Iter 1488][Class loss: 0.000042169962398]\n",
      "[Iter 1489][Class loss: 0.000042113624659]\n",
      "[Iter 1490][Class loss: 0.000042054805817]\n",
      "[Iter 1491][Class loss: 0.000041988456360]\n",
      "[Iter 1492][Class loss: 0.000041927480197]\n",
      "[Iter 1493][Class loss: 0.000041866849642]\n",
      "[Iter 1494][Class loss: 0.000041806568333]\n",
      "[Iter 1495][Class loss: 0.000041746570787]\n",
      "[Iter 1496][Class loss: 0.000041685700126]\n",
      "[Iter 1497][Class loss: 0.000041627354221]\n",
      "[Iter 1498][Class loss: 0.000041569830501]\n",
      "[Iter 1499][Class loss: 0.000041507810238]\n",
      "[Iter 1500][Class loss: 0.000041448933189]\n",
      "[Iter 1501][Class loss: 0.000041389666876]\n",
      "[Iter 1502][Class loss: 0.000041331521061]\n",
      "[Iter 1503][Class loss: 0.000041273029638]\n",
      "[Iter 1504][Class loss: 0.000041216546379]\n",
      "[Iter 1505][Class loss: 0.000041155260988]\n",
      "[Iter 1506][Class loss: 0.000041100363887]\n",
      "[Iter 1507][Class loss: 0.000041044062527]\n",
      "[Iter 1508][Class loss: 0.000040987055399]\n",
      "[Iter 1509][Class loss: 0.000040926355723]\n",
      "[Iter 1510][Class loss: 0.000040871367673]\n",
      "[Iter 1511][Class loss: 0.000040811395593]\n",
      "[Iter 1512][Class loss: 0.000040754086513]\n",
      "[Iter 1513][Class loss: 0.000040699524106]\n",
      "[Iter 1514][Class loss: 0.000040645569243]\n",
      "[Iter 1515][Class loss: 0.000040587427065]\n",
      "[Iter 1516][Class loss: 0.000040532395360]\n",
      "[Iter 1517][Class loss: 0.000040473547415]\n",
      "[Iter 1518][Class loss: 0.000040418824938]\n",
      "[Iter 1519][Class loss: 0.000040360959247]\n",
      "[Iter 1520][Class loss: 0.000040307393647]\n",
      "[Iter 1521][Class loss: 0.000040250281018]\n",
      "[Iter 1522][Class loss: 0.000040196282498]\n",
      "[Iter 1523][Class loss: 0.000040137951146]\n",
      "[Iter 1524][Class loss: 0.000040087506932]\n",
      "[Iter 1525][Class loss: 0.000040031402023]\n",
      "[Iter 1526][Class loss: 0.000039978272980]\n",
      "[Iter 1527][Class loss: 0.000039919461415]\n",
      "[Iter 1528][Class loss: 0.000039870457840]\n",
      "[Iter 1529][Class loss: 0.000039816161006]\n",
      "[Iter 1530][Class loss: 0.000039763453969]\n",
      "[Iter 1531][Class loss: 0.000039710859710]\n",
      "[Iter 1532][Class loss: 0.000039655147702]\n",
      "[Iter 1533][Class loss: 0.000039600876335]\n",
      "[Iter 1534][Class loss: 0.000039542745071]\n",
      "[Iter 1535][Class loss: 0.000039495054807]\n",
      "[Iter 1536][Class loss: 0.000039442842535]\n",
      "[Iter 1537][Class loss: 0.000039386344724]\n",
      "[Iter 1538][Class loss: 0.000039341888623]\n",
      "[Iter 1539][Class loss: 0.000039285798266]\n",
      "[Iter 1540][Class loss: 0.000039232832933]\n",
      "[Iter 1541][Class loss: 0.000039183309127]\n",
      "[Iter 1542][Class loss: 0.000039126556658]\n",
      "[Iter 1543][Class loss: 0.000039076898247]\n",
      "[Iter 1544][Class loss: 0.000039021306293]\n",
      "[Iter 1545][Class loss: 0.000038967755245]\n",
      "[Iter 1546][Class loss: 0.000038917685742]\n",
      "[Iter 1547][Class loss: 0.000038865444367]\n",
      "[Iter 1548][Class loss: 0.000038814861909]\n",
      "[Iter 1549][Class loss: 0.000038763133489]\n",
      "[Iter 1550][Class loss: 0.000038712245441]\n",
      "[Iter 1551][Class loss: 0.000038660320570]\n",
      "[Iter 1552][Class loss: 0.000038612164644]\n",
      "[Iter 1553][Class loss: 0.000038563252019]\n",
      "[Iter 1554][Class loss: 0.000038513207983]\n",
      "[Iter 1555][Class loss: 0.000038458103518]\n",
      "[Iter 1556][Class loss: 0.000038407459215]\n",
      "[Iter 1557][Class loss: 0.000038359699829]\n",
      "[Iter 1558][Class loss: 0.000038309684896]\n",
      "[Iter 1559][Class loss: 0.000038263078750]\n",
      "[Iter 1560][Class loss: 0.000038213867811]\n",
      "[Iter 1561][Class loss: 0.000038164471334]\n",
      "[Iter 1562][Class loss: 0.000038113026676]\n",
      "[Iter 1563][Class loss: 0.000038069083530]\n",
      "[Iter 1564][Class loss: 0.000038016311009]\n",
      "[Iter 1565][Class loss: 0.000037970814446]\n",
      "[Iter 1566][Class loss: 0.000037920530303]\n",
      "[Iter 1567][Class loss: 0.000037869576772]\n",
      "[Iter 1568][Class loss: 0.000037820715079]\n",
      "[Iter 1569][Class loss: 0.000037771791540]\n",
      "[Iter 1570][Class loss: 0.000037722085835]\n",
      "[Iter 1571][Class loss: 0.000037674122723]\n",
      "[Iter 1572][Class loss: 0.000037626901758]\n",
      "[Iter 1573][Class loss: 0.000037582263758]\n",
      "[Iter 1574][Class loss: 0.000037532132410]\n",
      "[Iter 1575][Class loss: 0.000037484809582]\n",
      "[Iter 1576][Class loss: 0.000037435260310]\n",
      "[Iter 1577][Class loss: 0.000037390978832]\n",
      "[Iter 1578][Class loss: 0.000037343888835]\n",
      "[Iter 1579][Class loss: 0.000037299447285]\n",
      "[Iter 1580][Class loss: 0.000037248973968]\n",
      "[Iter 1581][Class loss: 0.000037203353713]\n",
      "[Iter 1582][Class loss: 0.000037157304178]\n",
      "[Iter 1583][Class loss: 0.000037111210986]\n",
      "[Iter 1584][Class loss: 0.000037063517084]\n",
      "[Iter 1585][Class loss: 0.000037014866393]\n",
      "[Iter 1586][Class loss: 0.000036969060602]\n",
      "[Iter 1587][Class loss: 0.000036925335735]\n",
      "[Iter 1588][Class loss: 0.000036879169784]\n",
      "[Iter 1589][Class loss: 0.000036831428588]\n",
      "[Iter 1590][Class loss: 0.000036785699194]\n",
      "[Iter 1591][Class loss: 0.000036740428186]\n",
      "[Iter 1592][Class loss: 0.000036696019379]\n",
      "[Iter 1593][Class loss: 0.000036653109419]\n",
      "[Iter 1594][Class loss: 0.000036605379137]\n",
      "[Iter 1595][Class loss: 0.000036559402361]\n",
      "[Iter 1596][Class loss: 0.000036515499232]\n",
      "[Iter 1597][Class loss: 0.000036470068153]\n",
      "[Iter 1598][Class loss: 0.000036425350117]\n",
      "[Iter 1599][Class loss: 0.000036380093661]\n",
      "[Iter 1600][Class loss: 0.000036333949538]\n",
      "[Iter 1601][Class loss: 0.000036291727156]\n",
      "[Iter 1602][Class loss: 0.000036248115066]\n",
      "[Iter 1603][Class loss: 0.000036207209632]\n",
      "[Iter 1604][Class loss: 0.000036162244214]\n",
      "[Iter 1605][Class loss: 0.000036118559365]\n",
      "[Iter 1606][Class loss: 0.000036073783122]\n",
      "[Iter 1607][Class loss: 0.000036031065974]\n",
      "[Iter 1608][Class loss: 0.000035987963201]\n",
      "[Iter 1609][Class loss: 0.000035943383409]\n",
      "[Iter 1610][Class loss: 0.000035897955968]\n",
      "[Iter 1611][Class loss: 0.000035856319300]\n",
      "[Iter 1612][Class loss: 0.000035815850424]\n",
      "[Iter 1613][Class loss: 0.000035772973206]\n",
      "[Iter 1614][Class loss: 0.000035730714444]\n",
      "[Iter 1615][Class loss: 0.000035686272895]\n",
      "[Iter 1616][Class loss: 0.000035644854506]\n",
      "[Iter 1617][Class loss: 0.000035600489355]\n",
      "[Iter 1618][Class loss: 0.000035561799450]\n",
      "[Iter 1619][Class loss: 0.000035517456126]\n",
      "[Iter 1620][Class loss: 0.000035476099583]\n",
      "[Iter 1621][Class loss: 0.000035433145968]\n",
      "[Iter 1622][Class loss: 0.000035391756683]\n",
      "[Iter 1623][Class loss: 0.000035348439269]\n",
      "[Iter 1624][Class loss: 0.000035307948565]\n",
      "[Iter 1625][Class loss: 0.000035267839849]\n",
      "[Iter 1626][Class loss: 0.000035224224121]\n",
      "[Iter 1627][Class loss: 0.000035183173168]\n",
      "[Iter 1628][Class loss: 0.000035139844840]\n",
      "[Iter 1629][Class loss: 0.000035098146327]\n",
      "[Iter 1630][Class loss: 0.000035056975321]\n",
      "[Iter 1631][Class loss: 0.000035017023038]\n",
      "[Iter 1632][Class loss: 0.000034976172174]\n",
      "[Iter 1633][Class loss: 0.000034934088035]\n",
      "[Iter 1634][Class loss: 0.000034895980207]\n",
      "[Iter 1635][Class loss: 0.000034856053389]\n",
      "[Iter 1636][Class loss: 0.000034815566323]\n",
      "[Iter 1637][Class loss: 0.000034775082895]\n",
      "[Iter 1638][Class loss: 0.000034736272937]\n",
      "[Iter 1639][Class loss: 0.000034695753129]\n",
      "[Iter 1640][Class loss: 0.000034655713534]\n",
      "[Iter 1641][Class loss: 0.000034615899494]\n",
      "[Iter 1642][Class loss: 0.000034576041799]\n",
      "[Iter 1643][Class loss: 0.000034537119063]\n",
      "[Iter 1644][Class loss: 0.000034498349123]\n",
      "[Iter 1645][Class loss: 0.000034460896131]\n",
      "[Iter 1646][Class loss: 0.000034418881114]\n",
      "[Iter 1647][Class loss: 0.000034380816942]\n",
      "[Iter 1648][Class loss: 0.000034341719584]\n",
      "[Iter 1649][Class loss: 0.000034303346183]\n",
      "[Iter 1650][Class loss: 0.000034262000554]\n",
      "[Iter 1651][Class loss: 0.000034225005948]\n",
      "[Iter 1652][Class loss: 0.000034184664401]\n",
      "[Iter 1653][Class loss: 0.000034147007682]\n",
      "[Iter 1654][Class loss: 0.000034108510590]\n",
      "[Iter 1655][Class loss: 0.000034070864785]\n",
      "[Iter 1656][Class loss: 0.000034031683754]\n",
      "[Iter 1657][Class loss: 0.000033994976548]\n",
      "[Iter 1658][Class loss: 0.000033956814150]\n",
      "[Iter 1659][Class loss: 0.000033917051041]\n",
      "[Iter 1660][Class loss: 0.000033880092815]\n",
      "[Iter 1661][Class loss: 0.000033841657569]\n",
      "[Iter 1662][Class loss: 0.000033805001294]\n",
      "[Iter 1663][Class loss: 0.000033767071727]\n",
      "[Iter 1664][Class loss: 0.000033729465940]\n",
      "[Iter 1665][Class loss: 0.000033693249861]\n",
      "[Iter 1666][Class loss: 0.000033657044696]\n",
      "[Iter 1667][Class loss: 0.000033617478039]\n",
      "[Iter 1668][Class loss: 0.000033583914046]\n",
      "[Iter 1669][Class loss: 0.000033541262383]\n",
      "[Iter 1670][Class loss: 0.000033507607441]\n",
      "[Iter 1671][Class loss: 0.000033471755160]\n",
      "[Iter 1672][Class loss: 0.000033433832868]\n",
      "[Iter 1673][Class loss: 0.000033396674553]\n",
      "[Iter 1674][Class loss: 0.000033358614019]\n",
      "[Iter 1675][Class loss: 0.000033320924558]\n",
      "[Iter 1676][Class loss: 0.000033283886296]\n",
      "[Iter 1677][Class loss: 0.000033249387343]\n",
      "[Iter 1678][Class loss: 0.000033212658309]\n",
      "[Iter 1679][Class loss: 0.000033175216231]\n",
      "[Iter 1680][Class loss: 0.000033138941944]\n",
      "[Iter 1681][Class loss: 0.000033103395253]\n",
      "[Iter 1682][Class loss: 0.000033068015910]\n",
      "[Iter 1683][Class loss: 0.000033031428757]\n",
      "[Iter 1684][Class loss: 0.000032995165384]\n",
      "[Iter 1685][Class loss: 0.000032961506804]\n",
      "[Iter 1686][Class loss: 0.000032923799154]\n",
      "[Iter 1687][Class loss: 0.000032891170122]\n",
      "[Iter 1688][Class loss: 0.000032854357414]\n",
      "[Iter 1689][Class loss: 0.000032821495552]\n",
      "[Iter 1690][Class loss: 0.000032783573261]\n",
      "[Iter 1691][Class loss: 0.000032748539525]\n",
      "[Iter 1692][Class loss: 0.000032713058317]\n",
      "[Iter 1693][Class loss: 0.000032678210118]\n",
      "[Iter 1694][Class loss: 0.000032643474697]\n",
      "[Iter 1695][Class loss: 0.000032608084439]\n",
      "[Iter 1696][Class loss: 0.000032573316275]\n",
      "[Iter 1697][Class loss: 0.000032536918297]\n",
      "[Iter 1698][Class loss: 0.000032504416595]\n",
      "[Iter 1699][Class loss: 0.000032470510632]\n",
      "[Iter 1700][Class loss: 0.000032434349123]\n",
      "[Iter 1701][Class loss: 0.000032402080251]\n",
      "[Iter 1702][Class loss: 0.000032368945540]\n",
      "[Iter 1703][Class loss: 0.000032335545257]\n",
      "[Iter 1704][Class loss: 0.000032299456507]\n",
      "[Iter 1705][Class loss: 0.000032268038922]\n",
      "[Iter 1706][Class loss: 0.000032231986552]\n",
      "[Iter 1707][Class loss: 0.000032199739508]\n",
      "[Iter 1708][Class loss: 0.000032164964068]\n",
      "[Iter 1709][Class loss: 0.000032128751627]\n",
      "[Iter 1710][Class loss: 0.000032095595088]\n",
      "[Iter 1711][Class loss: 0.000032062162063]\n",
      "[Iter 1712][Class loss: 0.000032028689020]\n",
      "[Iter 1713][Class loss: 0.000031994928577]\n",
      "[Iter 1714][Class loss: 0.000031960167689]\n",
      "[Iter 1715][Class loss: 0.000031928106182]\n",
      "[Iter 1716][Class loss: 0.000031894873246]\n",
      "[Iter 1717][Class loss: 0.000031860901800]\n",
      "[Iter 1718][Class loss: 0.000031827614293]\n",
      "[Iter 1719][Class loss: 0.000031795927498]\n",
      "[Iter 1720][Class loss: 0.000031762749131]\n",
      "[Iter 1721][Class loss: 0.000031732066418]\n",
      "[Iter 1722][Class loss: 0.000031698888051]\n",
      "[Iter 1723][Class loss: 0.000031666560972]\n",
      "[Iter 1724][Class loss: 0.000031634379411]\n",
      "[Iter 1725][Class loss: 0.000031600447983]\n",
      "[Iter 1726][Class loss: 0.000031568117265]\n",
      "[Iter 1727][Class loss: 0.000031536648748]\n",
      "[Iter 1728][Class loss: 0.000031503561331]\n",
      "[Iter 1729][Class loss: 0.000031470786780]\n",
      "[Iter 1730][Class loss: 0.000031438328733]\n",
      "[Iter 1731][Class loss: 0.000031407260394]\n",
      "[Iter 1732][Class loss: 0.000031377014238]\n",
      "[Iter 1733][Class loss: 0.000031343286537]\n",
      "[Iter 1734][Class loss: 0.000031312538340]\n",
      "[Iter 1735][Class loss: 0.000031280647818]\n",
      "[Iter 1736][Class loss: 0.000031250227039]\n",
      "[Iter 1737][Class loss: 0.000031218216463]\n",
      "[Iter 1738][Class loss: 0.000031185430998]\n",
      "[Iter 1739][Class loss: 0.000031154282624]\n",
      "[Iter 1740][Class loss: 0.000031124767702]\n",
      "[Iter 1741][Class loss: 0.000031093972211]\n",
      "[Iter 1742][Class loss: 0.000031062918424]\n",
      "[Iter 1743][Class loss: 0.000031033596315]\n",
      "[Iter 1744][Class loss: 0.000031000061426]\n",
      "[Iter 1745][Class loss: 0.000030971110391]\n",
      "[Iter 1746][Class loss: 0.000030940056604]\n",
      "[Iter 1747][Class loss: 0.000030908991903]\n",
      "[Iter 1748][Class loss: 0.000030876486562]\n",
      "[Iter 1749][Class loss: 0.000030847204471]\n",
      "[Iter 1750][Class loss: 0.000030816983781]\n",
      "[Iter 1751][Class loss: 0.000030785144190]\n",
      "[Iter 1752][Class loss: 0.000030754650652]\n",
      "[Iter 1753][Class loss: 0.000030724891985]\n",
      "[Iter 1754][Class loss: 0.000030694649467]\n",
      "[Iter 1755][Class loss: 0.000030664115911]\n",
      "[Iter 1756][Class loss: 0.000030635903386]\n",
      "[Iter 1757][Class loss: 0.000030603845516]\n",
      "[Iter 1758][Class loss: 0.000030576069548]\n",
      "[Iter 1759][Class loss: 0.000030544171750]\n",
      "[Iter 1760][Class loss: 0.000030514638638]\n",
      "[Iter 1761][Class loss: 0.000030485080060]\n",
      "[Iter 1762][Class loss: 0.000030454857551]\n",
      "[Iter 1763][Class loss: 0.000030427272577]\n",
      "[Iter 1764][Class loss: 0.000030397855880]\n",
      "[Iter 1765][Class loss: 0.000030367109503]\n",
      "[Iter 1766][Class loss: 0.000030342511309]\n",
      "[Iter 1767][Class loss: 0.000030308459827]\n",
      "[Iter 1768][Class loss: 0.000030280913052]\n",
      "[Iter 1769][Class loss: 0.000030252684155]\n",
      "[Iter 1770][Class loss: 0.000030219975088]\n",
      "[Iter 1771][Class loss: 0.000030192646591]\n",
      "[Iter 1772][Class loss: 0.000030162862458]\n",
      "[Iter 1773][Class loss: 0.000030134991903]\n",
      "[Iter 1774][Class loss: 0.000030106770282]\n",
      "[Iter 1775][Class loss: 0.000030075947507]\n",
      "[Iter 1776][Class loss: 0.000030050632631]\n",
      "[Iter 1777][Class loss: 0.000030018269172]\n",
      "[Iter 1778][Class loss: 0.000029989669201]\n",
      "[Iter 1779][Class loss: 0.000029960227039]\n",
      "[Iter 1780][Class loss: 0.000029933726182]\n",
      "[Iter 1781][Class loss: 0.000029906392228]\n",
      "[Iter 1782][Class loss: 0.000029877674024]\n",
      "[Iter 1783][Class loss: 0.000029846041798]\n",
      "[Iter 1784][Class loss: 0.000029817978429]\n",
      "[Iter 1785][Class loss: 0.000029790629924]\n",
      "[Iter 1786][Class loss: 0.000029762768463]\n",
      "[Iter 1787][Class loss: 0.000029734392228]\n",
      "[Iter 1788][Class loss: 0.000029706277928]\n",
      "[Iter 1789][Class loss: 0.000029677097700]\n",
      "[Iter 1790][Class loss: 0.000029649630960]\n",
      "[Iter 1791][Class loss: 0.000029623875889]\n",
      "[Iter 1792][Class loss: 0.000029594821171]\n",
      "[Iter 1793][Class loss: 0.000029570175684]\n",
      "[Iter 1794][Class loss: 0.000029538507079]\n",
      "[Iter 1795][Class loss: 0.000029513237678]\n",
      "[Iter 1796][Class loss: 0.000029485670893]\n",
      "[Iter 1797][Class loss: 0.000029454999094]\n",
      "[Iter 1798][Class loss: 0.000029430731956]\n",
      "[Iter 1799][Class loss: 0.000029401024221]\n",
      "[Iter 1800][Class loss: 0.000029373652069]\n",
      "[Iter 1801][Class loss: 0.000029345414077]\n",
      "[Iter 1802][Class loss: 0.000029319982787]\n",
      "[Iter 1803][Class loss: 0.000029292954423]\n",
      "[Iter 1804][Class loss: 0.000029265200283]\n",
      "[Iter 1805][Class loss: 0.000029237113267]\n",
      "[Iter 1806][Class loss: 0.000029210712455]\n",
      "[Iter 1807][Class loss: 0.000029184200685]\n",
      "[Iter 1808][Class loss: 0.000029156868550]\n",
      "[Iter 1809][Class loss: 0.000029130125768]\n",
      "[Iter 1810][Class loss: 0.000029102800909]\n",
      "[Iter 1811][Class loss: 0.000029075788916]\n",
      "[Iter 1812][Class loss: 0.000029049353543]\n",
      "[Iter 1813][Class loss: 0.000029022667150]\n",
      "[Iter 1814][Class loss: 0.000028995364119]\n",
      "[Iter 1815][Class loss: 0.000028968868719]\n",
      "[Iter 1816][Class loss: 0.000028944756195]\n",
      "[Iter 1817][Class loss: 0.000028918886528]\n",
      "[Iter 1818][Class loss: 0.000028891816328]\n",
      "[Iter 1819][Class loss: 0.000028864904380]\n",
      "[Iter 1820][Class loss: 0.000028839047445]\n",
      "[Iter 1821][Class loss: 0.000028811306038]\n",
      "[Iter 1822][Class loss: 0.000028786977055]\n",
      "[Iter 1823][Class loss: 0.000028763097362]\n",
      "[Iter 1824][Class loss: 0.000028734977604]\n",
      "[Iter 1825][Class loss: 0.000028711858249]\n",
      "[Iter 1826][Class loss: 0.000028682281481]\n",
      "[Iter 1827][Class loss: 0.000028657996154]\n",
      "[Iter 1828][Class loss: 0.000028633001421]\n",
      "[Iter 1829][Class loss: 0.000028605074476]\n",
      "[Iter 1830][Class loss: 0.000028580347134]\n",
      "[Iter 1831][Class loss: 0.000028554155506]\n",
      "[Iter 1832][Class loss: 0.000028527805625]\n",
      "[Iter 1833][Class loss: 0.000028502430723]\n",
      "[Iter 1834][Class loss: 0.000028475524232]\n",
      "[Iter 1835][Class loss: 0.000028451700928]\n",
      "[Iter 1836][Class loss: 0.000028423928597]\n",
      "[Iter 1837][Class loss: 0.000028399766961]\n",
      "[Iter 1838][Class loss: 0.000028373320674]\n",
      "[Iter 1839][Class loss: 0.000028349866625]\n",
      "[Iter 1840][Class loss: 0.000028322232538]\n",
      "[Iter 1841][Class loss: 0.000028299269616]\n",
      "[Iter 1842][Class loss: 0.000028272312193]\n",
      "[Iter 1843][Class loss: 0.000028247231967]\n",
      "[Iter 1844][Class loss: 0.000028222855690]\n",
      "[Iter 1845][Class loss: 0.000028197551728]\n",
      "[Iter 1846][Class loss: 0.000028172235034]\n",
      "[Iter 1847][Class loss: 0.000028147973353]\n",
      "[Iter 1848][Class loss: 0.000028122456570]\n",
      "[Iter 1849][Class loss: 0.000028098173061]\n",
      "[Iter 1850][Class loss: 0.000028072816349]\n",
      "[Iter 1851][Class loss: 0.000028048332751]\n",
      "[Iter 1852][Class loss: 0.000028024658604]\n",
      "[Iter 1853][Class loss: 0.000028001353712]\n",
      "[Iter 1854][Class loss: 0.000027975958801]\n",
      "[Iter 1855][Class loss: 0.000027952923119]\n",
      "[Iter 1856][Class loss: 0.000027926171242]\n",
      "[Iter 1857][Class loss: 0.000027904839953]\n",
      "[Iter 1858][Class loss: 0.000027879395930]\n",
      "[Iter 1859][Class loss: 0.000027855581720]\n",
      "[Iter 1860][Class loss: 0.000027829657483]\n",
      "[Iter 1861][Class loss: 0.000027807283914]\n",
      "[Iter 1862][Class loss: 0.000027782520192]\n",
      "[Iter 1863][Class loss: 0.000027758687793]\n",
      "[Iter 1864][Class loss: 0.000027736135962]\n",
      "[Iter 1865][Class loss: 0.000027711303119]\n",
      "[Iter 1866][Class loss: 0.000027687618058]\n",
      "[Iter 1867][Class loss: 0.000027664616937]\n",
      "[Iter 1868][Class loss: 0.000027640564440]\n",
      "[Iter 1869][Class loss: 0.000027617525120]\n",
      "[Iter 1870][Class loss: 0.000027594440326]\n",
      "[Iter 1871][Class loss: 0.000027569220038]\n",
      "[Iter 1872][Class loss: 0.000027548359867]\n",
      "[Iter 1873][Class loss: 0.000027521533411]\n",
      "[Iter 1874][Class loss: 0.000027498963391]\n",
      "[Iter 1875][Class loss: 0.000027474445233]\n",
      "[Iter 1876][Class loss: 0.000027452613722]\n",
      "[Iter 1877][Class loss: 0.000027431151466]\n",
      "[Iter 1878][Class loss: 0.000027405822038]\n",
      "[Iter 1879][Class loss: 0.000027383308407]\n",
      "[Iter 1880][Class loss: 0.000027357998988]\n",
      "[Iter 1881][Class loss: 0.000027334857805]\n",
      "[Iter 1882][Class loss: 0.000027312420571]\n",
      "[Iter 1883][Class loss: 0.000027289172067]\n",
      "[Iter 1884][Class loss: 0.000027266200050]\n",
      "[Iter 1885][Class loss: 0.000027243317163]\n",
      "[Iter 1886][Class loss: 0.000027221638447]\n",
      "[Iter 1887][Class loss: 0.000027197165764]\n",
      "[Iter 1888][Class loss: 0.000027174839488]\n",
      "[Iter 1889][Class loss: 0.000027152233088]\n",
      "[Iter 1890][Class loss: 0.000027128937290]\n",
      "[Iter 1891][Class loss: 0.000027106545531]\n",
      "[Iter 1892][Class loss: 0.000027083953682]\n",
      "[Iter 1893][Class loss: 0.000027061254514]\n",
      "[Iter 1894][Class loss: 0.000027038560802]\n",
      "[Iter 1895][Class loss: 0.000027017007596]\n",
      "[Iter 1896][Class loss: 0.000026994095606]\n",
      "[Iter 1897][Class loss: 0.000026970879844]\n",
      "[Iter 1898][Class loss: 0.000026948226150]\n",
      "[Iter 1899][Class loss: 0.000026927316867]\n",
      "[Iter 1900][Class loss: 0.000026903613616]\n",
      "[Iter 1901][Class loss: 0.000026881811209]\n",
      "[Iter 1902][Class loss: 0.000026859826903]\n",
      "[Iter 1903][Class loss: 0.000026838468330]\n",
      "[Iter 1904][Class loss: 0.000026814772355]\n",
      "[Iter 1905][Class loss: 0.000026793273719]\n",
      "[Iter 1906][Class loss: 0.000026771584089]\n",
      "[Iter 1907][Class loss: 0.000026749923563]\n",
      "[Iter 1908][Class loss: 0.000026727160730]\n",
      "[Iter 1909][Class loss: 0.000026705134587]\n",
      "[Iter 1910][Class loss: 0.000026682637326]\n",
      "[Iter 1911][Class loss: 0.000026661087759]\n",
      "[Iter 1912][Class loss: 0.000026639339922]\n",
      "[Iter 1913][Class loss: 0.000026617632102]\n",
      "[Iter 1914][Class loss: 0.000026594665542]\n",
      "[Iter 1915][Class loss: 0.000026574829462]\n",
      "[Iter 1916][Class loss: 0.000026552765121]\n",
      "[Iter 1917][Class loss: 0.000026530775358]\n",
      "[Iter 1918][Class loss: 0.000026510188036]\n",
      "[Iter 1919][Class loss: 0.000026489535230]\n",
      "[Iter 1920][Class loss: 0.000026468856959]\n",
      "[Iter 1921][Class loss: 0.000026445342883]\n",
      "[Iter 1922][Class loss: 0.000026424910175]\n",
      "[Iter 1923][Class loss: 0.000026401525247]\n",
      "[Iter 1924][Class loss: 0.000026381614589]\n",
      "[Iter 1925][Class loss: 0.000026360406991]\n",
      "[Iter 1926][Class loss: 0.000026337211239]\n",
      "[Iter 1927][Class loss: 0.000026317295124]\n",
      "[Iter 1928][Class loss: 0.000026294843337]\n",
      "[Iter 1929][Class loss: 0.000026274130505]\n",
      "[Iter 1930][Class loss: 0.000026252524549]\n",
      "[Iter 1931][Class loss: 0.000026232164601]\n",
      "[Iter 1932][Class loss: 0.000026210684155]\n",
      "[Iter 1933][Class loss: 0.000026190849894]\n",
      "[Iter 1934][Class loss: 0.000026169202101]\n",
      "[Iter 1935][Class loss: 0.000026148791221]\n",
      "[Iter 1936][Class loss: 0.000026126144803]\n",
      "[Iter 1937][Class loss: 0.000026109375540]\n",
      "[Iter 1938][Class loss: 0.000026084780984]\n",
      "[Iter 1939][Class loss: 0.000026065274142]\n",
      "[Iter 1940][Class loss: 0.000026044102924]\n",
      "[Iter 1941][Class loss: 0.000026022447855]\n",
      "[Iter 1942][Class loss: 0.000026003082894]\n",
      "[Iter 1943][Class loss: 0.000025981487852]\n",
      "[Iter 1944][Class loss: 0.000025961509891]\n",
      "[Iter 1945][Class loss: 0.000025940531486]\n",
      "[Iter 1946][Class loss: 0.000025919594918]\n",
      "[Iter 1947][Class loss: 0.000025900595574]\n",
      "[Iter 1948][Class loss: 0.000025879617169]\n",
      "[Iter 1949][Class loss: 0.000025859355446]\n",
      "[Iter 1950][Class loss: 0.000025840556191]\n",
      "[Iter 1951][Class loss: 0.000025820954761]\n",
      "[Iter 1952][Class loss: 0.000025798653951]\n",
      "[Iter 1953][Class loss: 0.000025778650524]\n",
      "[Iter 1954][Class loss: 0.000025758203265]\n",
      "[Iter 1955][Class loss: 0.000025737203032]\n",
      "[Iter 1956][Class loss: 0.000025716461096]\n",
      "[Iter 1957][Class loss: 0.000025696805096]\n",
      "[Iter 1958][Class loss: 0.000025678029488]\n",
      "[Iter 1959][Class loss: 0.000025658639061]\n",
      "[Iter 1960][Class loss: 0.000025637742510]\n",
      "[Iter 1961][Class loss: 0.000025618628570]\n",
      "[Iter 1962][Class loss: 0.000025596467822]\n",
      "[Iter 1963][Class loss: 0.000025578179702]\n",
      "[Iter 1964][Class loss: 0.000025557785193]\n",
      "[Iter 1965][Class loss: 0.000025539171475]\n",
      "[Iter 1966][Class loss: 0.000025516797905]\n",
      "[Iter 1967][Class loss: 0.000025499746698]\n",
      "[Iter 1968][Class loss: 0.000025480574550]\n",
      "[Iter 1969][Class loss: 0.000025459221433]\n",
      "[Iter 1970][Class loss: 0.000025438055673]\n",
      "[Iter 1971][Class loss: 0.000025423090847]\n",
      "[Iter 1972][Class loss: 0.000025400291634]\n",
      "[Iter 1973][Class loss: 0.000025381943487]\n",
      "[Iter 1974][Class loss: 0.000025363049645]\n",
      "[Iter 1975][Class loss: 0.000025341931178]\n",
      "[Iter 1976][Class loss: 0.000025323621230]\n",
      "[Iter 1977][Class loss: 0.000025303064831]\n",
      "[Iter 1978][Class loss: 0.000025283718060]\n",
      "[Iter 1979][Class loss: 0.000025263983844]\n",
      "[Iter 1980][Class loss: 0.000025243543860]\n",
      "[Iter 1981][Class loss: 0.000025228157028]\n",
      "[Iter 1982][Class loss: 0.000025205299607]\n",
      "[Iter 1983][Class loss: 0.000025187247957]\n",
      "[Iter 1984][Class loss: 0.000025167937565]\n",
      "[Iter 1985][Class loss: 0.000025147150154]\n",
      "[Iter 1986][Class loss: 0.000025129376809]\n",
      "[Iter 1987][Class loss: 0.000025108891350]\n",
      "[Iter 1988][Class loss: 0.000025091698262]\n",
      "[Iter 1989][Class loss: 0.000025072358767]\n",
      "[Iter 1990][Class loss: 0.000025053406716]\n",
      "[Iter 1991][Class loss: 0.000025032950361]\n",
      "[Iter 1992][Class loss: 0.000025014540370]\n",
      "[Iter 1993][Class loss: 0.000024995726562]\n",
      "[Iter 1994][Class loss: 0.000024976670829]\n",
      "[Iter 1995][Class loss: 0.000024957571441]\n",
      "[Iter 1996][Class loss: 0.000024938321076]\n",
      "[Iter 1997][Class loss: 0.000024919227144]\n",
      "[Iter 1998][Class loss: 0.000024901088182]\n",
      "[Iter 1999][Class loss: 0.000024881806894]\n"
     ]
    }
   ],
   "source": [
    "max_iters = 2000\n",
    "iter_start_offset = 0\n",
    "\n",
    "for iter_train_outer in range(iter_start_offset, max_iters):\n",
    "    detector_train.train()\n",
    "    loss = detector_train(\n",
    "        node_features = batch_data['object_features'],\n",
    "        edge_index = batch_data['edge_idx'],\n",
    "        object_size = batch_data['object_num_meas'],\n",
    "        groundtruths = batch_data['object_class'] )\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_str = f\"[Iter {iter_train_outer}][Class loss: {loss.item():.15f}]\"\n",
    "    print(loss_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted class scores: tensor([1.0000, 0.9980, 1.0000, 1.0000, 0.9982, 0.9988, 0.9987, 0.9987],\n",
      "       device='cuda:0')\n",
      "predicted class       : tensor([0, 0, 0, 0, 4, 6, 6, 6], device='cuda:0')\n",
      "ground-truth classses : tensor([0, 0, 0, 0, 4, 6, 6, 6], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "@ torch.no_grad()\n",
    "def inference(classifier):\n",
    "    pred_class_logits = classifier(\n",
    "        node_features = batch_data['object_features'][0],\n",
    "        edge_index = batch_data['edge_idx'][0],\n",
    "        object_size = batch_data['object_num_meas'][0])\n",
    "    cls_prob = F.softmax(pred_class_logits, dim=-1)\n",
    "    cls_score, cls_idx = torch.max(cls_prob, dim=-1)\n",
    "    return cls_score, cls_idx\n",
    "\n",
    "classifier = detector_train.pred.eval()\n",
    "cls_score, cls_idx = inference(classifier)\n",
    "\n",
    "print(f\"predicted class scores: {cls_score}\")\n",
    "print(f\"predicted class       : {cls_idx}\")\n",
    "print(f\"ground-truth classses : {batch_data['object_class'][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
